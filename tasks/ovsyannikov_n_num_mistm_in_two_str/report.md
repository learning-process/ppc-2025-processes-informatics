# Количество несовпадений символов двух строк

- Студент: Овсянников Никита Владимирович, группа 3823Б1ФИ2
- Технология: SEQ, MPI
- Вариант: 27

## 1. Introduction

Цель данной работы — научиться применять технологию MPI для распараллеливания задач. В качестве примера выбрана задача сравнения двух строк. Хотя сама по себе задача простая, на ней удобно разбирать, как делить данные между процессами и собирать общий результат.

## 2. Problem Statement

У нас есть две строки одинаковой длины. Нужно пройтись по ним и посчитать, сколько раз символы на одной и той же позиции не совпадают.

**Входные данные:** Две строки (`std::string`).
**Выходные данные:** Целое число — количество несовпадений.

## 3. Baseline Algorithm (Sequential)

В последовательной версии мы просто бежим циклом от начала до конца строки.

```cpp
int mismatch_count = 0;
size_t n = seq_one.size();
for (size_t i = 0; i < n; ++i) {
    if (seq_one[i] != seq_two[i]) {
        mismatch_count++;
    }
}
```
Сложность линейная - O(N).

## 4. Parallelization Scheme
Процесс с рангом 0 определяет размер данных. Исходные строки делятся на части по количеству процессов. Ранг 0 рассылает каждому процессу его куски двух строк (используется Scatterv). Каждый процесс независимо сравнивает символы в полученных частях. Локальные счетчики несовпадений суммируются в общий результат (Allreduce).

## 5. Implementation Details
- common: Определяет общие типы данных.
- seq: Последовательная реализация (обычный цикл).
- mpi: Параллельная реализация. Использует векторы для упаковки данных перед отправкой.
- tests: Тесты на корректность (разные случаи несовпадений) и на скорость.

## 6. Experimental Setup
- Аппаратное обеспечение: 13th Gen Intel Core i5-13500H (12 ядер: 4P + 8E, 2.60 GHz)
- ОЗУ: 16 ГБ (4266 MT/s)
- Операционная система: Windows 11
- Компилятор: MSVC
- Тип сборки: Release

## 7. Results and Discussion

### 7.1 Correctness
Корректность алгоритма подтверждена функциональными тестами. Результаты MPI-версии идентичны результатам последовательной версии на всех тестовых наборах данных (пустые строки, полное совпадение, частичное совпадение).

### 7.2 Performance

Тестирование проводилось на строках длиной 100 000 000 символов.
За базовое время ($T_{seq}$) взято время работы последовательной реализации: **0.1734 s**.

| Mode | Processes | Time (s) | Speedup | Efficiency |
|:----:|:---------:|:--------:|:-------:|:----------:|
| seq  | 1         | 0.1734   | 1.00    | 100%       |
| mpi  | 2         | 0.3410   | 0.51    | 25.5%      |
| mpi  | 4         | 0.3304   | 0.52    | 13.0%      |
| mpi  | 8         | 0.2650   | 0.65    | 8.1%       |
| mpi  | 12        | 0.3064   | 0.56    | 4.7%       |
| mpi  | 24        | 0.8192   | 0.21    | 0.9%       |

## 8. Conclusions

Анализ результатов экспериментов позволяет сделать следующие выводы:

1.  **Отрицательное ускорение (Slowdown):**
    Параллельная версия (MPI) во всех случаях работает медленнее последовательной (Speedup < 1). Это объясняется природой задачи:
    *   **Низкая вычислительная сложность:** Операция сравнения двух символов (`char == char`) выполняется процессором за один такт и экстремально быстра.
    *   **Доминирование накладных расходов:** Время, затрачиваемое на передачу данных через `MPI_Scatterv` и инициализацию процессов, значительно превышает время полезных вычислений. Даже оптимизация с прямым доступом к памяти (без лишних буферов) не позволяет перекрыть латентность межпроцессного взаимодействия.

2.  **Динамика масштабируемости:**
    *   На 2-4 процессах накладные расходы высоки, ускорение составляет около 0.5.
    *   Наилучший результат достигнут на **8 процессах** (0.2650 s), где накладные расходы на коммуникацию лучше всего балансируются с уменьшением объема работы на каждом ядре. Однако даже этот результат уступает последовательной версии.

3.  **Эффект переподписки (Oversubscription):**
    При запуске на **24 процессах** наблюдается резкое падение производительности (время выросло до 0.8192 s). Это связано с тем, что количество MPI-процессов превышает количество физических потоков процессора (Intel Core i5-13500H имеет 12 ядер/16 потоков). Планировщик ОС вынужден постоянно переключать контекст (Context Switching) между процессами, что убивает производительность и кэш-локальность.

**Итог:** Технология MPI плохо подходит для задач с тривиальными вычислениями и большим объемом передаваемых данных на системах с общей памятью. Для таких задач эффективнее использовать многопоточность (OpenMP/TBB) или векторизацию (SIMD), так как они не требуют дорогостоящей пересылки данных между адресными пространствами.

## 9. References
1. Лекции и практики курса "Параллельное программирование".