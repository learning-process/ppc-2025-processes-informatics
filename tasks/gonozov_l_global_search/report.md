# Метод простой итерации
- Студент: Гонозов Леонид Андреевич, 3823Б1ФИ3
- Технология: SEQ, MPI 
- Вариант: 11

---

## 1. Введение
Методы решения многоэкстремальных задач широко встречаются в приложениях, например, при проектировании радиотетехнических устройств, и зачастую для решенния многоэкстремальных задач недостаточно нахождения локального мнимимума. Он может как не удовлетворять заданным требованиям и не показывать точную картину для поставленной задачи, так и быть разделённым существенным выигрышем по отношению к глобальному минимуму. На этой почве и возникает потребность в алгоритмах глобальной оптимизации. Одним из которых как раз и является алгоритм глобального поиска или алгоритм Стронгина. Он позволит найти глобальный минимум на отрезке (в случае одномерного варианта - отрезок вещественной оси) для заданной функции.

Цель работы — реализовать параллельную версию одномерного алгоритма глобального поиска с использованием технологии MPI, оценить корректность работы и исследовать ускорение относительно последовательного варианта.

---

## 2. Постановка задачи
**Определение**: Дана система линейных уравнений, записанная в виде матрицы коэффициентов. Она должна обладать условием строгого диагонального преобладания для того, чтобы была возможность применения метода простых итераций. В ходе алгоритма будет произведено количество итераций достаточное для достижения определённой точности: разницы между предыдущими и текущими значениями найденных неизвестных, она должна быть достаточно мала. 

**Входные данные**: число неизвестных, матрица размера число неизвестных на число неизвестных и вектор значений уравнений.
**Выходные данные:** вектор, состоящий из найденных приближений для каждой из неизвестных.

**Ограничения**:
- входная матрица коэффициентов при неизвестных должна быть с диагональным преобладанием
- система линенйых уравнений должна содержать хотя бы одно уравнение;
- начальное приближение должно быть взято так, чтобы была гарантирована возможность сходимости из него
- различное количество процессов не должно приводить к непредвиденным последствиям.

---

## 3. Описание базового алгоритма 
Последовательная версия начинается с подсчёта первых приближений, где каждое приближение получается из деления коэффициента b конкретной строки на диагональный элемент этой строки:
```cpp
  for (int i = 0; i < number_unknowns; i++) {
    previous_approximations[i] = b[i] / matrix[i * number_unknowns + i];
  }
```
В основном цикле, ограниченном заранее заданным максимальным количеством операций существует цикл, проходящий по каждой неизвестной и получающий её новое приближение. Из коэффициента b конкретной строки вычитается сумма всех остальных элементов строки за исключением диагонального элемента умноженных на предыдущие приближения. Эта разница делится на диагональный элемент этой строки:
```cpp
    current_approximations[i] = (b[i] - sum) / matrix[i * number_unknowns + i];
```
После построения новых приближений происходит проверка на достижение требуемой точности: разница между текущим и прошлым приближением по каждой из неизвестных должны быть меньше заданного эпсилон. В случае достижения требуемой точности алгоритм успешно завершается, иначе подсчёты продолжаются.
---

## 4.  Схема распараллеливания
- **Декомпозиция данных**:
    - Матрица коэффициентов перед неизвестными представленная массивом и вектор значений b разбиваются на блоки по количеству процессов, то есть каждый процесс получают информацию для работы с определённым числом неизвестных. 
	- Каждый процесс получает определённое количество строк матрицы и значений коэффициентов b через `MPI_Scatterv`.
- **Вычисления на процессе**:
    Каждый процесс вычисляет приближения для неизвестных, которые соответствуют полученным строкам матрицы и значений коэффициентов b
    Также каждый процесс подсчитывает достигло ли каждое из посчитанных им приближений требуемой точности.
- **Коммуникация**:
	После локальных подсчётов приближений выполняется `MPI_Allgatherv`, для того чтобы каждый процесс получил сведения о построенных приближениях на текущей итерации и использовал эту информацию для подсчёта новых приближений на следующей итерации. 
    С помощью `MPI_Bcast` каждый процесс получает информацию о том, как именно будут собираться в `MPI_Allgatherv` приближения со всех процессов в один вектор.  
    Чтобы понять по всем ли приближениям достигнута требуемая точность, со всех процессов собирается количество приближений, достигших требуемой точности с помощью `MPI_Allreduce` с операцией суммирования.  

- **Топология и обмен**:
	Используется коммуникатор `MPI_COMM_WORLD` без явной топологии (все процессы равноправны).  
	Тип обмена — коллективные операции (`MPI_Scatterv`, `MPI_Bcast`, `MPI_Allgatherv`, `MPI_Allreduce`).

 
---
## 5.  Детали реализации
- **Ключевые файлы:**
	- `tasks/gonozov_l__simple_iteration_method/seq/include/ops_seq.hpp` и `tasks/gonozov_l__simple_iteration_method/seq/src/ops_seq.cpp` - последовательная реализация.
	- `tasks/gonozov_l__simple_iteration_method/mpi/include/ops_mpi.hpp` и `tasks/gonozov_l__simple_iteration_method/mpi/src/ops_mpi.cpp` - параллельная реализация.
	- `tasks/gonozov_l__simple_iteration_method/common/include/common.hpp` - общие определения типов и интерфейсов.
- **Особенности**:
- **Разбиение данных на части по числу процессов**
	В том случае, если существует остаток от деления числа неизвестных на число процессов, процессы, чей ранг меньше, чем это остаток, получают в распоряжение на один элемент больше. Это иллюстрирует следующий псевдокод:
```cpp
  	int remainder = число_неизвестных % ко-во_процессов;
  	int local_size = число_неизвестных / ко-во_процессов + (procRank < remainder ? 1 : 0);
```
- **Передача процессам данных, требуемых для вычислений**
    Для этой цели два раза происходит `MPI_Scatterv`, один раз каждый процесс получает *local_size* элементов вектора b (так как он уникален для каждого процесса, они хранятся в *sendcounts*), расположенных в векторе, поступившим на вход, в котором смещение характеризуется величиной *displs* (массив, в котором хранится смещение для каждого отдельно взятого процесса)
    Второй раз `MPI_Scatterv` используется для распространения *local_size* строк с числом элементов *local_size* * *число_неизвестных*
- **Вычисления на каждом отдельно взятом процессе**
    Вычисление первых приближений, где my_first_row - смещение, характеризующее положение диагонального элемента для взятой строки:
	```cpp
  	  for (int i = 0; i < local_size; i++) {
        int global_row = my_first_row + i; 
        local_previous[i] = local_b[i] / local_matrix[i * number_unknowns + global_row]; 
        // первое приближение для неизвестной = коэффициент b cтроки / диагональный элемент этой же самой строки
    }
	```
    Затем происходит череда итераций метода простых итераций. Процесс происходит в виже суммирования всех элементов строки, за исключением диагонального, умноженных на результаты прошлых приближений. Затем получившаяся сумма участвует в подсчёте нового приближения:
    ```cpp
      local_current[i] = (local_b[i] - sum) / local_matrix[i * number_unknowns + global_row]; // деление всё так же на диагональный элемент
    ```
    В конце итераций происходит проверка сходимости для новых приблжений:
    ```cpp
    int local_converged = 0;
    for (int i = 0; i < local_size; i++) {
      int global_row = my_first_row + i;
      double diff = abs(current_approximations[global_row] - previous_approximations[global_row]);
      double norm = abs(current_approximations[global_row]);
      if (diff < 0.00001 * (norm + 1e-10)) { // 1e-10 - компенсирование возможной нулевой длины norm
        local_converged++;
      }
    }
     ```
- **Формирование пересылки новых приближений всем процессам**
    `MPI_Allgatherv` - операция, без которой было бы невозможно вычисление новых приближений на каждой итерации, так как для вычисления нового приближения, требуется чтобы на каждом процессе имелась полная информация о всех полученных приближениях прошлой итерации, что и обеспечивает данная функция.
    `MPI_Bcast` помогает узнать процессам какое количество данных и куда они должны переслать, эта информация используется в `MPI_Allgatherv`
    За счёт этой пересылки, в момент достижения требуемой точности все процессы уже знают о том, что является ответом алгоритма и дополнительная пересылка уже не требуется

---

## 6. Экспериментальная установка
- **Аппаратное обеспечение**: 
	12th Gen Intel(R) Core(TM) i7-12650H (2.30 GHz) (6 производительных ядер и 4 энергоэффективных)  
	ОЗУ — 16,0 ГБ
- **Операционная система:** Windows 11 Pro
- **Инструменты:**
	- Компилятор: g++ (Ubuntu 13.3.0-6ubuntu2~24.04) 13.3.0 
	- CMake 4.1.1
	- Режим сборки: Release.
	- Был использован Docker-container
- **Данные:**
    Матрица размерности N = 300, где все элементы равны 1, а диагональные N? и вектор значений b, где каждое значение равно N * 2 - 1

---

## 7.  Результаты и обсуждение

### 7.1 Корректность
Корректность проверялась сравнением результатов полученных в ходе алгоритма и заранее заготовленных значений. При этом на вход подавались вектора различной длины заполненных единицами.

### 7.2 Производительность

Тест 1. 100 уравнений (100 строк матрицы и 100 значений векотра b).

| Mode | Count | Time,  s | Speedup | Efficiency |
| ---- | ----- | -------- | ------- | ---------- |
| seq  | 1     | 0.005265 | 1.00    | N/A        |
| mpi  | 2     | 0.004062 | 1,296   | 64,8 %     |
| mpi  | 4     | 0.004178 | 1,26    | 31,5 %     |
| mpi  | 8     | 0.008705 | 0,6048  | 7,56 %     |

Тест 2. 300 уравнений (300 строк матрицы и 300 значений векотра b).

| Mode | Count | Time,  s | Speedup | Efficiency |
| ---- | ----- | -------- | ------- | ---------- |
| seq  | 1     | 0.131547 | 1.00    | N/A        |
| mpi  | 2     | 0.071180 | 1,848   | 92,4 %     |
| mpi  | 4     | 0.048718 | 2,7     | 67,5 %     |
| mpi  | 8     | 0.060515 | 2,17    | 27,1 %     |

Тест 3. 600 уравнений (600 строк матрицы и 600 значений векотра b).

| Mode | Count | Time,  s | Speedup | Efficiency |
| ---- | ----- | -------- | ------- | ---------- |
| seq  | 1     | 1.0936   | 1.00    | N/A        |
| mpi  | 2     | 0.584227 | 1,872   | 93,59 %    |
| mpi  | 4     | 0.35287  | 3,099   | 77,4 %     |
| mpi  | 8     | 0.365688 | 2,99    | 37,3 %     |

Тест 4. 700 уравнений (700 строк матрицы и 700 значений векотра b).

| Mode | Count | Time,  s | Speedup | Efficiency |
| ---- | ----- | -------- | ------- | ---------- |
| seq  | 1     | 1.75425  | 1.00    | N/A        |
| mpi  | 2     | 0.936568 | 1,873   | 93,65 %    |
| mpi  | 4     | 0.5472   | 3,206   | 80,15 %    |
| mpi  | 8     | 0.5421   | 3,236   | 40,45 %    |

- При большем количестве процессов при одних и тех же входных данных падает эффективность, это обусловлено увелечением числа накладных раскодов (расходов на коммуникацию между процессами)
- При увеличении числа входных данных отмечается достаточно небольшой рост эффективности на 2 потоках, но существенный рост эффективности на 4 потоках, следует предполагать, что при ещё большем увеличении размера входных данных такой рост будет продолжаться, пока не начнётся обратный процесс уменьшения эффективности
- Наибольшая эффективность достигается на 2 потоках

---

## 8. Заключение
Реализованный в ходе работы параллельный вариант алгоритма метода простой итерации успешно решает поставленную задачу и демонстрирует значительное ускорение относительно последовательной версии. Метод хорошо поддаётся распараллеливанию благодаря большому количеству локальных вычислений, и чем больше будет входных данных, тем больше будет локальных вычислений и тем лучше себя покажет параллельный вариант алгоритма.

---

## 9. Источники
- Лекции и практики курса "Параллельное программирование", https://cppreference.com/
