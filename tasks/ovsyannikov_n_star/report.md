# Топология Звезда

- Студент: Овсянников Никита Владимирович, группа 3823Б1ФИ2
- Технология: SEQ, MPI
- Вариант: 8

## 1. Introduction

Цель данной работы — изучить принципы организации межпроцессорного взаимодействия в специфических логических топологиях. В качестве примера выбрана топология «Звезда» (Star Topology). Эта задача позволяет разобрать механизмы маршрутизации сообщений, когда периферийные узлы не могут общаться напрямую, и любые данные должны проходить через центральный процесс.

## 2. Problem Statement

Необходимо реализовать алгоритм пересылки данных между любыми двумя процессами в системе, имитируя топологию «Звезда». В этой структуре процесс с рангом 0 является центром, а остальные процессы — «лучами». Если отправитель и получатель не являются центром, сообщение должно быть сначала отправлено в процесс 0, а затем переслано конечному получателю.

**Входные данные:** Вектор из трех чисел (`std::vector<int>`): `{отправитель, получатель, значение}`.
**Выходные данные:** Целое число — значение, полученное процессом-получателем.

## 3. Baseline Algorithm (Sequential)

В последовательной версии топология не имеет значения, так как существует только один процесс. Задача сводится к простому возврату значения, которое было передано на вход.

```cpp
int value = input_data[2];
GetOutput() = value;
```

Сложность константная — O(1).
## 4. Parallelization Scheme

Параллельная реализация построена на строгом соблюдении топологических связей: периферийные узлы («лучи») лишены прямой связи друг с другом. Любое взаимодействие осуществляется через центральный узел (процесс 0).
1. Если отправитель (`src`) или получатель (`dst`) являются центром, выполняется прямая передача данных.
2. Если оба участника являются «лучами», отправитель посылает сообщение процессу 0, который выступает в роли посредника, пересылая полученные данные конечному адресату.
3. Для обеспечения корректности работы тестового фреймворка, в конце алгоритма используется `MPI_Bcast`, который рассылает результат от процесса-получателя всем остальным процессам. Это необходимо для того, чтобы проверка результата (`CheckTestOutputData`) прошла успешно на каждом из запущенных узлов.

## 5. Implementation Details

- common: Определяет структуру входного вектора `{отправитель, получатель, значение}` и тип выходного значения.
- seq: Последовательная реализация, которая имитирует результат работы системы, возвращая переданное значение.
- mpi: Реализует маршрутизацию с использованием точечных обменов `MPI_Send` и `MPI_Recv`. Узел 0 содержит логику пересылки (буферизации) для транзитных сообщений.
- tests: Включают функциональные тесты для всех возможных комбинаций маршрутов и тесты производительности.

## 6. Experimental Setup

- Аппаратное обеспечение: 13th Gen Intel Core i5-13500H (12 ядер: 4P + 8E, 2.60 GHz)
- ОЗУ: 16 ГБ (4266 MT/s)
- Операционная система: Windows 11
- Компилятор: MSVC
- Тип сборки: Release

## 7. Results and Discussion

### 7.1 Correctness
Корректность алгоритма подтверждена успешным прохождением функциональных тестов (`ppc_func_tests`). Проверены сценарии передачи данных между «лучами», а также сценарии, где центр является либо отправителем, либо получателем.

### 7.2 Performance

Тестирование проводилось на пересылке значения `int = 1000`. За базовое время ($T_{seq}$) взято среднее время `task_run` последовательной реализации: **0.0000008 s**.

| Mode | Processes | Time (s) | Speedup | Efficiency |
|:----:|:---------:|:--------:|:-------:|:----------:|
| seq  | 1         | 0.0000008| 1.00    | 100%       |
| mpi  | 2         | 0.0001162| 0.0068  | 0.34%      |
| mpi  | 4         | 0.0002673| 0.0029  | 0.07%      |
| mpi  | 8         | 0.0001726| 0.0046  | 0.05%      |
| mpi  | 12        | 0.0002999| 0.0026  | 0.02%      |
| mpi  | 24        | 0.0039081| 0.0002  | 0.0008%    |
| mpi  | 48        | 0.0018785| 0.0004  | 0.0008%    |

## 8. Conclusions

Анализ результатов экспериментов позволяет сделать следующие выводы:

1. **Доминирование накладных расходов:** Параллельная версия во всех случаях работает значительно медленнее последовательной. Время на выполнение системных вызовов MPI и передачу данных по шине на несколько порядков превышает время полезной работы (копирование 4 байт).
2. **Латентность топологии:** Структура «Звезда» вносит дополнительную задержку при взаимодействии периферийных узлов, так как данные проходят через промежуточный узел (двойной путь сообщения).
3. **Эффект переподписки:** При запуске на 24 и 48 процессах наблюдается существенный рост времени (до 0.0039 s), что вызвано конкуренцией процессов за физические ядра процессора и накладными расходами на переключение контекста планировщиком ОС.
4. **Важность конфигурации:** При запуске на одном процессе (`-n 1`) MPI-версия выдает ошибку, так как логика маршрутизации между узлами 0 и 1 требует наличия как минимум двух активных рангов в коммуникаторе.

## 9. References

1. Лекции и практики курса "Параллельное программирование".
2. Документация Microsoft MPI (MS-MPI).