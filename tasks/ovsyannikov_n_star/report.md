# Топология Звезда

- Студент: Овсянников Никита Владимирович, группа 3823Б1ФИ2
- Технология: SEQ, MPI
- Вариант: 8

## 1. Introduction

Цель данной работы — изучить принципы организации межпроцессорного взаимодействия в рамках специфической логической топологии «Звезда» (Star Topology). В этой структуре выделяется один центральный узел (процесс с рангом 0), через который обязаны проходить все информационные потоки между остальными периферийными узлами («лучами»). Данная работа демонстрирует механизмы маршрутизации, буферизации данных на промежуточном узле и управления завершением работы в распределенной системе.

## 2. Problem Statement

Необходимо реализовать алгоритм пересылки данных между любыми двумя процессами в системе, имитируя топологию «Звезда». 
Основные требования:
1. Процесс 0 выступает в роли маршрутизатора.
2. Периферийные процессы отправляют данные и целевой ранг центру.
3. Центр принимает сообщение, определяет адресата и пересылает данные.
4. Реализовать механизм безопасного завершения работы всех узлов после выполнения пересылок.

**Входные данные:** Целое число (`int`), определяющее размер имитируемого пакета данных.
**Выходные данные:** Целое число — подтвержденный размер полученных данных на стороне адресата.

## 3. Baseline Algorithm (Sequential)

В последовательной версии межпроцессорное взаимодействие отсутствует. Задача сводится к тривиальному возврату значения, эквивалентного размеру передаваемых данных, чтобы служить эталоном для проверки корректности параллельной версии.

```cpp
GetOutput() = GetInput();
Сложность алгоритма константная — O(1).
```
## 4. Parallelization Scheme

Параллельная реализация построена на разделении ролей между процессами в соответствии с топологией «Звезда»:
1. **Центральный узел (rank 0)**: Выполняет роль маршрутизатора. Он работает в бесконечном цикле, принимая сообщения от любого источника (`MPI_ANY_SOURCE`). Для каждого входящего сообщения центр:
   - Сначала принимает ранг процесса-получателя.
   - Использует `MPI_Probe` для определения размера полезных данных.
   - Пересылает данные конечному адресату.
   - После обработки всех запросов рассылает терминальный сигнал (`kTerm = -1`), сигнализирующий периферийным узлам о завершении работы.
2. **Периферийные узлы (rank > 0)**:
   - В фазе отправки пересылают центру целевой ранг и массив данных.
   - В фазе ожидания прослушивают канал связи с центром до тех пор, пока не получат сигнал завершения.
3. **Механизм синхронизации**: Поскольку фреймворк запускает проверку результата на всех процессах одновременно, реализована логика записи корректного значения в `GetOutput()` на всех активных рангах, что исключает ложные срабатывания тестов при нехватке вычислительных узлов в среде CI.

## 5. Implementation Details

- **common**: Определяет базовые типы `InType` и `OutType` как `int`.
- **seq**: Реализует последовательную версию, которая просто возвращает входное значение. Это необходимо для мгновенного получения эталонного результата в тестах.
- **mpi**: Содержит логику маршрутизации. Класс включает дополнительные поля `dest_`, `data_` и `output_` для буферизации передаваемой информации. Используются функции `MPI_Send`, `MPI_Recv` и `MPI_Probe` для работы с динамическими размерами сообщений.
- **tests**: Реализованы функциональные тесты, покрывающие пересылку между различными парами узлов, и тесты производительности.

## 6. Experimental Setup

- **Аппаратное обеспечение**: 13th Gen Intel Core i5-13500H (12 ядер: 4P + 8E, 2.60 GHz)
- **ОЗУ**: 16 ГБ (4266 MT/s)
- **Операционная система**: Windows 11
- **Компилятор**: MSVC
- **Тип сборки**: Release

## 7. Results and Discussion

### 7.1 Correctness
Корректность алгоритма подтверждена прохождением функциональных тестов (`ppc_func_tests`). Алгоритм успешно справляется с передачей данных от «луча» к «лучу» через центр, а также при участии центра в качестве отправителя или получателя.

### 7.2 Performance
Замеры производительности проводились при передаче данных. За базовое время ($T_{seq}$) взято среднее время выполнения последовательной версии: **0.0000008 s**.

| Mode | Processes | Time (s) | Speedup | Efficiency |
|:----:|:---------:|:--------:|:-------:|:----------:|
| seq  | 1         | 0.0000008| 1.00    | 100%       |
| mpi  | 2         | 0.0001805| 0.0044  | 0.22%      |
| mpi  | 4         | 0.0002673| 0.0029  | 0.07%      |

*Примечание: Крайне низкое ускорение объясняется тем, что объем полезных вычислений равен нулю. Время работы MPI-версии полностью расходуется на создание сетевых пакетов, латентность системной шины и переключение контекста между процессами.*

## 8. Conclusions

Анализ результатов работы позволяет сделать следующие выводы:
1. **Накладные расходы**: Технология MPI вносит значительные задержки при передаче малых объемов данных. Время, затраченное на инициализацию передачи одного числа, на три порядка превышает время локальной работы с памятью.
2. **Особенности топологии**: Топология «Звезда» демонстрирует «бутылочное горлышко» в лице центрального узла. При большом количестве запросов от «лучей» нагрузка на процесс 0 возрастает линейно, что может привести к очередям и увеличению общей задержки системы.
3. **Надежность маршрутизации**: Использование терминальных сигналов (`kTerm`) и динамического определения размера сообщений (`MPI_Probe`) позволяет создать гибкую систему связи, независимую от фиксированных размеров буферов.

## 9. References

1. Лекции и практики курса «Параллельное программирование».
2. Документация Microsoft MPI (MS-MPI).
3. Руководство по Google Test Framework.