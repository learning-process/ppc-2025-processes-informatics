# Отчет по лабораторной работе №2
## "Передача от всех одному (reduce)"

**Студент:** Шехирев Владислав Эдуардович
**Группа:** 3823Б1ФИ2
**Вариант:** 2

### 1. Введение
**Мотивация:** Коллективные операции являются фундаментом для большинства параллельных алгоритмов. Операция `reduce` (свёртка) позволяет агрегировать данные, распределенные по множеству процессов, в единый результат на одном (корневом) процессе. Это используется повсеместно: от подсчета глобальной ошибки в научных вычислениях до суммирования градиентов в машинном обучении.

**Проблема:** Эффективность операции `reduce` критически зависит от соотношения времени локальных вычислений ко времени коммуникации. Если объем вычислений на каждом узле мал, накладные расходы на сетевое взаимодействие могут превысить выигрыш от распараллеливания.

**Ожидаемый результат:** На малых объемах данных ожидается, что с увеличением числа процессов накладные расходы будут расти быстрее, чем польза от деления задачи, что приведет к снижению эффективности.

### 2. Постановка задачи
**Задано:** Число `N` (размер вектора).

**Требуется:**
1.  На каждом из `P` процессов создать часть вектора.
2.  Заполнить вектор единицами.
3.  Вычислить сумму элементов локального вектора.
4.  Собрать сумму всех локальных сумм на корневом процессе.

**Входные данные:** `int N` (общее количество элементов).
**Выходные данные:** `int TotalSum` (общая сумма).

### 3. Базовый алгоритм (Последовательный)
Последовательная версия выполняет работу за один процесс:
1.  Создается вектор длины `N`.
2.  Вычисляется сумма его элементов с помощью `std::accumulate`.
Сложность: `O(N)`.

### 4. Схема распараллеливания
**Принцип:** SPMD (Single Program, Multiple Data).
1.  **Инициализация:** `MPI_Bcast` используется для рассылки входного параметра `N` от корневого процесса всем остальным.
2.  **Декомпозиция:** Общий объем работы `N` делится между `P` процессами. Каждый процесс обрабатывает примерно `N/P` элементов.
3.  **Локальные вычисления:** Процессы независимо вычисляют суммы своих подмассивов.
4.  **Агрегация:** `MPI_Reduce` с операцией `MPI_SUM` собирает локальные суммы в переменную `global_sum` на процессе с рангом 0.

### 5. Experimental Setup
*   **Hardware:** Контейнеризированная среда (Docker).
*   **Toolchain:** GCC, OpenMPI.
*   **Data:** Вектор размером `N = 100,000` элементов типа `int`.

### 6. Результаты
#### 6.1 Корректность
Корректность проверена функциональными тестами. Реализация успешно обрабатывает:
*   Равномерное распределение данных.
*   Неравномерное распределение (наличие остатка).
*   Малые размеры входных данных (меньше числа процессов).
Результаты совпадают с ожидаемыми значениями.

#### 6.2 Производительность
Замеры времени (`task_run`) выполнения задачи:

| Mode | Count | Time, s | Speedup | Efficiency |
| :--- | :--- | :--- | :--- | :--- |
| seq | 1 | 0.0000136 | 1.00 | 100% |
| mpi | 2 | 0.0000133 | 1.02 | 51% |
| mpi | 4 | 0.0000225 | 0.60 | 15% |

*   **Speedup** = `Time(seq) / Time(mpi)`
*   **Efficiency** = `Speedup / Count`

### 7. Выводы
По результатам эксперимента можно сделать следующие заключения:

1.  **2 процесса:** Наблюдается минимальное ускорение (практически паритет с последовательной версией). Это точка равновесия, где выигрыш от разделения массива пополам примерно компенсирует накладные расходы на инициализацию обмена данными.
2.  **4 процесса:** Происходит деградация производительности (замедление). Накладные расходы на синхронизацию 4-х процессов и сбор данных через `MPI_Reduce` превышают время, сэкономленное на суммировании четверти массива.
3.  **Общий итог:** Для задачи с экстремально малой вычислительной нагрузкой (суммирование 100,000 чисел занимает микросекунды) использование MPI не дает существенного выигрыша. Эффективность параллелизма (Efficiency) резко падает с добавлением новых процессов, что подтверждает закон Амдала в контексте высоких коммуникационных издержек.