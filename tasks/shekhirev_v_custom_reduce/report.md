# Отчет по лабораторной работе №2
## "Передача от всех одному (reduce)"

**Студент:** Шехирев Владислав Эдуардович
**Группа:** 3823Б1ФИ2
**Вариант:** 2

### 1. Введение
**Мотивация:** Агрегация данных (редукция) — ключевой этап многих параллельных алгоритмов. Понимание того, как эта операция устроена "под капотом", необходимо для написания эффективного кода. В данной работе реализуется механизм сбора данных вручную, без использования готовых коллективных операций типа `MPI_Reduce`.

**Проблема:** Встроенные операции MPI (такие как `MPI_Reduce`) обычно используют оптимизированные древовидные алгоритмы ($O(\log P)$). Ручная реализация "в лоб" (линейный сбор, $O(P)$) будет менее эффективной, особенно при увеличении числа процессов. Кроме того, на малых объемах данных накладные расходы на пересылку сообщений (`latency`) доминируют над временем полезных вычислений.

**Ожидаемый результат:** Ожидается, что ручная параллельная реализация будет работать медленнее последовательной из-за высоких накладных расходов на коммуникацию и сериализации приема данных на корневом процессе.

### 2. Постановка задачи
**Задано:** Число `N` (размер вектора).

**Требуется:**
1.  На каждом из `P` процессов создать часть вектора.
2.  Заполнить вектор единицами.
3.  Вычислить сумму элементов локального вектора.
4.  Собрать сумму всех локальных сумм на корневом процессе, используя только операции `MPI_Send` и `MPI_Recv`.

**Входные данные:** `int N` (общее количество элементов).
**Выходные данные:** `int TotalSum` (общая сумма).

### 3. Базовый алгоритм (Последовательный)
Последовательная версия выполняет работу за один процесс:
1.  Создается вектор длины `N`.
2.  Вычисляется сумма его элементов с помощью `std::accumulate`.
Сложность: `O(N)`.

### 4. Схема распараллеливания
**Принцип:** SPMD с ручной агрегацией.
1.  **Инициализация:** `MPI_Bcast` используется для рассылки входного параметра `N`.
2.  **Локальные вычисления:** Общий объем работы делится между процессами. Каждый процесс вычисляет сумму своего подмассива.
3.  **Агрегация (Custom Reduce):** Реализована линейная схема сбора данных:
    *   **Worker-процессы (rank > 0):** Отправляют свою локальную сумму корневому процессу с помощью функции **`MPI_Send`**.
    *   **Root-процесс (rank 0):** В цикле принимает сообщения от всех остальных процессов (`1..size-1`) с помощью **`MPI_Recv`** и прибавляет полученные значения к своей локальной сумме.

### 5. Experimental Setup
*   **Hardware:** Контейнеризированная среда (Docker).
*   **Toolchain:** GCC, OpenMPI.
*   **Data:** Вектор размером `N = 100,000` элементов.

### 6. Результаты
#### 6.1 Корректность
Корректность проверена функциональными тестами. Ручная реализация редукции корректно собирает суммы при разном количестве процессов и различных (в том числе неравномерных) разбиениях данных.

#### 6.2 Производительность
Замеры времени (`task_run`) выполнения задачи:

| Mode | Count | Time, s | Speedup | Efficiency |
| :--- | :--- | :--- | :--- | :--- |
| seq | 1 | 0.0000374 | 1.00 | 100% |
| mpi | 2 | 0.0002719 | 0.137 | 6.8% |
| mpi | 4 | 0.0007173 | 0.052 | 1.3% |

*   **Speedup** = `Time(seq) / Time(mpi)`
*   **Efficiency** = `Speedup / Count`

### 7. Выводы
Результаты экспериментов подтверждают теоретические ожидания для данной постановки задачи:

1.  **Сильное замедление:** Параллельная версия работает на порядок медленнее последовательной (Speedup << 1). Это объясняется тем, что время полезной работы (сложение чисел) ничтожно мало (десятки микросекунд) по сравнению со временем передачи сообщений по сети MPI (сотни микросекунд).
2.  **Неэффективность линейного сбора:** С увеличением числа процессов время выполнения растет (от `0.00027` на 2-х до `0.00071` на 4-х). Это происходит потому, что корневой процесс вынужден последовательно обрабатывать входящие сообщения (`MPI_Recv` блокирующий), что создает "бутылочное горлышко".
3.  **Заключение:** Ручная реализация редукции через линейный сбор данных оправдана только в учебных целях. В реальных задачах следует использовать оптимизированную коллективную операцию `MPI_Reduce`, а распараллеливание применять только тогда, когда объем вычислений достаточно велик, чтобы перекрыть накладные расходы на коммуникацию.