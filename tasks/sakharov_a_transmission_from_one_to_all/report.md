# Передача от одного всем (broadcast)

- Student: Сахаров Александр Владимирович, group 3823Б1ФИ3
- Technology: SEQ | MPI
- Variant: 1

## 1. Введение
Коллективные операции — ключевой элемент стандарта MPI. Одной из базовых является `MPI_Bcast`, обеспечивающая рассылку данных от одного процесса (корня) всем остальным. Реализация с использованием только `MPI_Send` и `MPI_Recv` демонстрирует принципы построения коллективных операций из точечных обменов.

В работе разработана собственная версия Broadcast на основе бинарного дерева. Такая топология обеспечивает логарифмическую сложность по числу процессов $O(\log P)$ и позволяет увидеть влияние схемы распределения на затраты коммуникаций.

## 2. Постановка задачи
**Формальная постановка:** реализовать функцию передачи данных от процесса `root` всем остальным (broadcast) с использованием только `MPI_Send` и `MPI_Recv`, сохранив прототип стандартной `MPI_Bcast`.

**Входные данные:**
- ранг корневого процесса `root`;
- буфер данных на процессе `root` (в тестах — вектор `int`, дополнительные проверки для `float`/`double`).

**Выходные данные:**
- идентичный буфер после завершения рассылки на всех процессах коммуникатора.

**Ограничения и требования:**
- сигнатура совпадает с `MPI_Bcast`;
- поддерживаются типы `MPI_INT`, `MPI_FLOAT`, `MPI_DOUBLE`;
- номер `root` задаётся пользователем и валидируется;
- рассылка строится на «дереве» процессов.

## 3. Последовательная версия
Последовательная (SEQ) постановка — вырожденный случай: данные уже находятся в памяти единственного процесса. Проверяется только валидность `root` (он должен быть 0), после чего выполняется копирование входного вектора в выходной.

```cpp
bool RunImpl() {
   GetOutput() = std::get<1>(GetInput());
   return true;
}
```

Версия служит эталоном корректности и базой для сравнения производительности.

## 4. Параллельная версия

### 4.1. Схема распределения (бинарное дерево)
Используется концепция «виртуального ранга», позволяющая считать корень нулевым независимо от его реального номера: $v\_rank = (rank - root + size) \bmod size$.

Для узла с виртуальным рангом $v$:
- левый потомок: $2v + 1$;
- правый потомок: $2v + 2$;
- родитель: $(v - 1) / 2$.

Обратное преобразование в реальный ранг: $real\_rank = (v\_rank + root) \bmod size$. Такой подход обеспечивает равномерное распределение нагрузки и минимизирует число отправок с корня.

### 4.2. Взаимодействие процессов
- **Процесс-корень (`v_rank = 0`):** сразу рассылает буфер двум потомкам (если они существуют).
- **Любой процесс `v_rank > 0`:** ожидает получение буфера от родителя через `MPI_Recv`, затем перенаправляет данные своим потомкам через `MPI_Send`.
- Перед основным буфером отдельно рассылается его размер, чтобы процессы заранее выделили память.
- Поддержка типов `int`, `float`, `double` проверяется отдельными вызовами `MyBcast` с валидацией значений.

### 4.3. Псевдокод алгоритма MyBcast

```pseudocode
MyBcast(buffer, count, datatype, root, comm):
   rank = CommRank(comm)
   size = CommSize(comm)
   v_rank = (rank - root + size) mod size

   if v_rank != 0:
      v_parent = (v_rank - 1) / 2
      real_parent = (v_parent + root) mod size
      MPI_Recv(buffer, count, datatype, real_parent)

   v_child1 = 2 * v_rank + 1
   v_child2 = 2 * v_rank + 2

   if v_child1 < size:
      real_child1 = (v_child1 + root) mod size
      MPI_Send(buffer, count, datatype, real_child1)

   if v_child2 < size:
      real_child2 = (v_child2 + root) mod size
      MPI_Send(buffer, count, datatype, real_child2)

   return MPI_SUCCESS
```

## 5. Детали реализации

### 5.1. Файловая структура проекта
`tasks/sakharov_a_transmission_from_one_to_all/`:
- `common/include/common.hpp` — типы `InType`, `OutType`, `BaseTask`.
- `mpi/include/ops_mpi.hpp]` — объявление `MyBcast` и класса `SakharovATransmissionFromOneToAllMPI`.
- `mpi/src/ops_mpi.cpp` — реализация дерева рассылки, проверки типов, логика задачи.
- `seq/include/ops_seq.hpp`, `seq/src/ops_seq.cpp` — последовательная заглушка.
- `tests/functional/main.cpp`, `tests/performance/main.cpp` — функциональные и производительные тесты.

### 5.2. Ключевые классы и функции
- `MyBcast` — древовидная реализация broadcast на `MPI_Send`/`MPI_Recv` с поддержкой произвольного `root`.
- `SakharovATransmissionFromOneToAllMPI` — MPI-задача: валидация `root`, рассылка размера, затем данных; проверки для `int`/`float`/`double`.
- `SakharovATransmissionFromOneToAllSEQ` — эталон без коммуникаций, возвращает входной вектор.
- `SakharovARunFuncTestsProcesses` — параметрические функциональные тесты для разных входных наборов.
- `SakharovARunPerfTestProcesses` — нагрузочные тесты, собирают метрики `pipeline` и `task_run`.

### 5.3. Использование памяти
- **SEQ:** хранит только входной/выходной вектор; накладные расходы отсутствуют.
- **MPI:** корень хранит полный буфер; каждый процесс хранит свою копию после получения; дополнительные структуры — лишь константные по размеру переменные для вычисления рангов.

## 6. Экспериментальное окружение

**Аппаратная платформа / ОС:**
- CPU: Intel Core i5-12400F (6 cores, 12 threads, 2.50 GHz);
- RAM: 32 GB DDR4;
- Накопитель: SSD 512 GB;
- ОС: Windows 10 + WSL (Ubuntu 24.04.3 LTS).

**Инструменты разработки:**
- Компилятор: `g++ 13.3.0`;
- MPI: `OpenMPI 4.1.6`;
- Сборка: `CMake 3.28.3`.

**Параметры запуска:**
- число процессов: 6 (`mpirun -n 6 ...`);
- `PPC_NUM_PROC` задаётся конфигурацией тестов;
- тестируемый объём: вектор `int` длиной 10,000,000 элементов (~40 МБ).

## 7. Результаты и обсуждение

### 7.1. Корректность
- Функциональные тесты проверяют разные входы (пустой, малый, большой векторы) и сопоставляют выход SEQ/MPI.
- Дополнительно в `RunImpl` проверяется корректность передачи `float` и `double` с погрешностью сравнения.
- Валидация гарантирует допустимый `root` и одинаковый размер буфера на всех процессах.

### 7.2. Производительность

| Mode | Time, s | Speedup | Efficiency |
|------|---------|---------|------------|
| seq, pipeline | 0.0168 | 1.00 | N/A |
| seq, task_run | 0.0156 | 1.00 | N/A |
| mpi, pipeline | 0.0771 | 0.22 | 3.6% |
| mpi, task_run | 0.0754 | 0.21 | 3.5% |

**Интерпретация:**
- Коммуникационные накладные расходы доминируют над дешёвыми вычислениями копирования, поэтому ускорения нет; скорость ухудшается до ~0.22x.
- Деревянная схема снижает число отправок для корня по сравнению с наивным $O(P)$, но при малой вычислительной нагрузке затраты на `MPI_Recv`/`MPI_Send` и синхронизацию перекрывают выгоду.
- Режимы `pipeline` и `task_run` показывают близкие значения, что указывает на преобладание времени самой рассылки над инфраструктурными этапами.

## 8. Заключение
Реализована и исследована функция `MyBcast`, собирающая коллективную рассылку из точечных обменов. Алгоритм с виртуальными рангами и бинарным деревом корректно работает для произвольного `root` и разных типов данных, что подтверждено функциональными тестами. На выбранном стенде для чистого копирования данных MPI-версия уступает SEQ из-за коммуникационных накладных расходов, что наглядно демонстрирует важность баланса вычислений и обменов в параллельных приложениях.

## 9. Источники
1. Microsoft MPI : документация [Электронный ресурс] // Microsoft Learn. – URL: https://learn.microsoft.com/ru-ru/message-passing-interface/microsoft-mpi (дата обращения: 11.12.2025).
2. Сысоев А. В. Курс лекций по параллельному программированию