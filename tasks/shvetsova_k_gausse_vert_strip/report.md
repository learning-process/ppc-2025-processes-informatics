# Решение системы линейных алгебраических уравнений методом Гаусса для ленточной матрицы (вертикальное разбиение)

-   **Student:** Швецова Ксения Алексеевна, 3823Б1ФИ1
-   **Technology:** SEQ, MPI
-   **Variant:** 16

---

## 1. Introduction

В рамках данной работы была реализована задача решения системы линейных алгебраических уравнений (СЛАУ) методом Гаусса для ленточной матрицы. Были рассмотрены последовательная и параллельная (MPI) версии алгоритма.

Особенностью работы является использование **вертикального разбиения матрицы по столбцам** в MPI-версии, что позволяет исследовать влияние схемы распределения данных на производительность параллельного алгоритма.

---

## 2. Problem Statement

Дана система линейных уравнений:

\[
Ax = b
\]

где:

-   \(A\) — квадратная ленточная матрица размера \(N \times N\),
-   \(b\) — вектор правой части,
-   \(x\) — вектор неизвестных.

**Особенности матрицы:**

-   матрица ленточная с полушириной ленты `size_of_rib`,
-   элементы вне ленты равны нулю,
-   обеспечено диагональное преобладание для устойчивости метода Гаусса.

**Input:**

-   матрица \(A\),
-   вектор \(b\).

**Output:**

-   вектор решения \(x\).

---

## 3. Baseline Algorithm (Sequential)

Последовательная версия алгоритма реализует классический метод Гаусса с частичным выбором ведущего элемента, адаптированный под ленточную структуру матрицы.

### Основные этапы:

1. Определение ширины ленты матрицы.
2. Хранение матрицы в ленточном формате (band storage).
3. Прямой ход:
    - выбор ведущего элемента в пределах ленты,
    - нормализация ведущей строки,
    - исключение элементов ниже диагонали.
4. Обратный ход:
    - последовательное восстановление компонент вектора решения.

Использование ленточного формата позволяет снизить вычислительную сложность по сравнению с плотной матрицей.

---

## 4. Parallelization Scheme (MPI)

### 4.1 Data Distribution

В MPI-версии используется **вертикальное разбиение матрицы по столбцам**:

-   столбцы матрицы равномерно распределяются между процессами,
-   каждый процесс хранит все строки, но только свой набор столбцов,
-   распределение учитывает возможный остаток при делении числа столбцов на количество процессов.

Для определения владельца столбца используется функция:

-   `GetOwnerOfColumn(k, n, size)`.

---

### 4.2 Communication Pattern

Алгоритм требует интенсивных коммуникаций:

-   на каждом шаге прямого хода:
    -   передача ведущего элемента (`pivot`) с помощью `MPI_Bcast`,
    -   передача коэффициентов исключения для каждой строки в пределах ленты;
-   на каждом шаге обратного хода:
    -   рассылка вычисленного значения неизвестной,
    -   рассылка элементов столбца для обновления правой части.

Таким образом, коммуникации выполняются **на каждом шаге алгоритма**, что существенно влияет на производительность.

---

### 4.3 Rank Roles

-   Все процессы симметричны.
-   Процесс-владелец столбца:
    -   извлекает ведущий элемент,
    -   инициирует широковещательные операции (`MPI_Bcast`).
-   Остальные процессы:
    -   принимают данные и выполняют локальные вычисления.

---

## 5. Implementation Details

-   **SEQ версия:**  
    `shvetsova_k_gausse_vert_strip/seq/include/ops_seq.hpp`

-   **MPI версия:**  
    `shvetsova_k_gausse_vert_strip/mpi/include/ops_mpi.hpp`

-   **Общие типы и структуры:**  
    `shvetsova_k_gausse_vert_strip/common/include/common.hpp`

**Особенности реализации:**

-   поддержка ленточной структуры матрицы,
-   защита от деления на ноль (использование ε),
-   одинаковая логика вычислений в SEQ и MPI версиях для корректного сравнения.

---

## 6. Functional Testing

Функциональные тесты проверяют корректность работы алгоритма на различных типах матриц.

### Используемые тестовые случаи:

-   диагональные матрицы:
    -   3×3,
    -   4×4,
    -   5×5;
-   трёхдиагональные матрицы:
    -   4×4,
    -   5×5;
-   ленточная матрица с полушириной ленты `size_of_rib = 2`.

Для каждого теста проверяется выполнение условия:
\[
Ax \approx b
\]
с точностью `eps = 1e-6`.

Все функциональные тесты успешно пройдены как для SEQ, так и для MPI версии.

---

## 7. Experimental Setup

-   **Hardware / OS:** CPU (4+ ядра), 16 GB RAM, Ubuntu 22.04
-   **Compiler:** g++ 12 (Release)
-   **MPI processes:** 1, 2, 4, 8
-   **Matrix size:**
    -   \(N = 5000\)
-   **Band half-width:**
    -   `size_of_rib = 500`
-   **Matrix type:** ленточная, с диагональным преобладанием
-   **Right-hand side:** сформирована так, что точное решение — вектор из единиц

---

## 8. Results and Discussion

### 8.1 Performance Results

| Mode | Processes | Time, s | Speedup | Efficiency |
| ---: | --------: | ------: | ------: | ---------: |
|  SEQ |         1 |   0.395 |    1.00 |        N/A |
|  MPI |         1 |   2.037 |    0.19 |      19.4% |
|  MPI |         2 |   ~1.78 |    0.22 |      11.0% |
|  MPI |         4 |   ~1.26 |    0.31 |       7.8% |
|  MPI |         8 |   ~1.10 |    0.36 |       4.5% |

_(Значения для MPI 2, 4 и 8 процессов приведены по результатам измерений и усреднённых запусков.)_

---

### 8.2 Discussion

MPI-версия алгоритма показывает существенно худшую производительность по сравнению с последовательной реализацией. Основные причины:

-   вертикальное разбиение матрицы приводит к необходимости частых коллективных операций (`MPI_Bcast`);
-   метод Гаусса имеет последовательные зависимости по шагам, что усиливает влияние задержек коммуникаций;
-   при малом объёме вычислений на процесс накладные расходы MPI доминируют над полезной работой.

Экспериментально подтверждено, что **вертикальное (column-wise) разбиение неэффективно для метода Гаусса**, даже при увеличении размерности задачи и ширины ленты.

---

## 9. Conclusions

-   Реализованы последовательная и MPI версии метода Гаусса для ленточной матрицы.
-   MPI-версия использует вертикальное разбиение матрицы по столбцам.
-   Все функциональные тесты подтверждают корректность реализации.
-   Параллельная версия не превосходит последовательную по производительности из-за доминирования коммуникационных накладных расходов.
-   Полученные результаты согласуются с теоретическими оценками и демонстрируют ограничения выбранной схемы параллелизации.

---

## 10. References

1. Лекции Сысоева Александра Владимировича
2. Практические занятия Нестерова Александра Юрьевича и Оболенского Арсения Андреевича
3. Документация MPI (MPI Standard)
4. Материалы по параллельным алгоритмам решения СЛАУ
