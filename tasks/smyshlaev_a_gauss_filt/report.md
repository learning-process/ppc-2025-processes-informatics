# <Линейная фильтрация изображений (блочное разбиение). Ядро Гаусса 3x3.>

- Student: Смышляев Александр Павлович, group 3823Б1ФИ2
- Technology: SEQ | MPI
- Variant: 28

## 1. Introduction
**Мотивация:** Фильтрация изображений, в частности фильтр Гаусса, — это фундаментальная операция в компьютерном зрении и обработке изображений, используемая для шумоподавления и сглаживания. Хотя вычислительная сложность фильтрации растет линейно с количеством пикселей ($O(W \times H)$ при фиксированном размере ядра), огромный объем данных в современных изображениях (миллионы пикселей) делает последовательную обработку медленной. Это, в сочетании со структурой задачи, делает ее отличным кандидатом для распараллеливания, особенно при обработке изображений высокого разрешения.

**Проблема:** При последовательной обработке больших изображений (например, Full HD и выше) время фильтрации может стать неприемлемым для приложений реального времени.

**Ожидаемый результат:** Использование MPI для блочного разбиения изображения должно значительно ускорить процесс фильтрации. Ожидается, что эффективность масштабирования будет высокой, так как объем вычислений для каждого пикселя невелик, но количество пикселей велико. Основные накладные расходы будут связаны с рассылкой блоков процессам.

## 2. Problem Statement
Дано изображение, представленное в виде массива пикселей размером $W \times H$ с $C$ цветовыми каналами. Требуется применить к каждому пикселю изображения фильтр Гаусса (ядро свертки 3x3).

## 3. Baseline Algorithm (Sequential)
Базовый алгоритм последовательно проходит по каждому пикселю изображения. Для каждого пикселя и каждого цветового канала вычисляется взвешенная сумма значений его самого и восьми его соседей. В качестве весов используется ядро Гаусса 3x3. Пиксели, находящиеся на границах изображения, обрабатываются с помощью дублирования крайних строк/столбцов (clamping).

## 4. Parallelization Scheme
Изображение разбивается на двумерную сетку блоков, и каждый блок назначается отдельному процессу MPI.

1.  **Декомпозиция:** Корневой процесс определяет оптимальную сетку процессов (например, для 8 процессов сетка может быть 2x4 или 4x2), чтобы блоки были как можно ближе к квадратной форме.
2.  **Подготовка данных:** Для каждого блока формируется дополнительная рамка толщиной в 1 пиксель, содержащая данные из соседних блоков (или дублированные краевые пиксели для границ всего изображения). Это необходимо, чтобы каждый процесс мог корректно обработать пиксели на своих границах, не запрашивая данные у соседей в цикле.
3.  **Распределение данных:** Корневой процесс готовит один большой буфер, содержащий все блоки, и рассылает их соответствующим процессам с помощью `MPI_Scatterv`.
4.  **Локальная обработка:** Каждый процесс применяет фильтр Гаусса к своему блоку данных. Результатом является обработанный блок без дополнительной рамки.
5.  **Сбор результатов:** Обработанные блоки собираются на корневом процессе с помощью `MPI_Gatherv`, где из них собирается итоговое отфильтрованное изображение. Финальный результат рассылается всем процессам через `MPI_Bcast`.

## 5. Experimental Setup
- **Hardware/OS:** `Intel Core i7-1255U` (10 ядер, 12 потоков), `16GB RAM`, `Windows 11`
- **Toolchain:** `MinGW-w64 (g++)`, `MS-MPI`, `Release`
- **Environment:** `PPC_NUM_PROC`
- **Data:** Изображение размером $1024 \times 1024$ пикселей, 3 канала.

## 6. Results and Discussion

### 6.1 Correctness
Корректность параллельного алгоритма подтверждена с помощью Google Test. Тесты сравнивают результат работы MPI-версии с результатом последовательной реализации на различных размерах изображений, включая неквадратные, с разным числом каналов и на размерах, не кратных числу процессов.

### 6.2 Performance
Результаты замеров времени выполнения (TaskRun) для изображения $1024 \times 1024 \times 3$:

| Mode        | Count | Time, s  | Speedup | Efficiency |
|-------------|-------|----------|---------|------------|
| seq (mpi-1) | 1     | 0.022079 |   1.00  | N/A        |
| mpi         | 2     | 0.014032 |   1.57  | 78.5%      |
| mpi         | 4     | 0.011176 |   1.98  | 49.5%      |
| mpi         | 8     | 0.009153 |   2.41  | 30.1%      |

**Анализ результатов:**
1.  **На 2 процессах** наблюдается хорошее ускорение (1.57x) с высокой эффективностью. Это показывает, что выигрыш от распараллеливания значительно превышает затраты на коммуникацию.
2.  **На 4 процессах** ускорение приближается к 2x, но эффективность падает до ~50%. Это связано с тем, что накладные расходы на `MPI_Scatterv` и `MPI_Gatherv` становятся более заметными.
3.  **На 8 процессах** ускорение продолжает расти (2.41x), но эффективность сильно падает (~30%). На таких малых временах выполнения стоимость подготовки и рассылки данных становится сопоставимой с временем самих вычислений.

## 7. Conclusions
Реализованный параллельный алгоритм фильтра Гаусса с блочным разбиением корректно и эффективно решает поставленную задачу. Эксперименты показывают, что для изображений среднего размера ($1024 \times 1024$) алгоритм хорошо масштабируется до 2-4 процессов. При дальнейшем увеличении числа процессов производительность продолжает расти, но эффективность падает из-за роста доли коммуникационных расходов. Для изображений значительно большего размера (например, 4K) ожидается более высокая эффективность на большем числе процессов.

## 8. References
1.  Лекции по параллельному программированию ННГУ
2.  Стандарт MPI
3.  Гергель В.П. — Теория и практика параллельных вычислений
4.  Michael J. Quinn — Parallel Programming in C with MPI and OpenMP 