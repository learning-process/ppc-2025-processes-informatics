# Интегрирование – метод Монте-Карло
- Student: Долов Вячеслав Васильевич, group 3823Б1ФИ3
- Technology: SEQ | MPI
- Variant: 21

## 1. Introduction
Численное интегрирование многомерных функций является фундаментальной, но требовательной к ресурсам задачей. Метод Монте-Карло (ММК) — универсальный стохастический подход, точность которого напрямую зависит от количества случайных выборок ($N$). Необходимость обработки большого $N$ для достижения приемлемой точности делает ММК идеальной задачей для **распараллеливания** с помощью технологии **MPI**.

**Цель работы:** Реализовать последовательную (SEQ) и параллельную (MPI) версии ММК для интегрирования в областях **Гиперкуба** и **Гиперсферы**, и исследовать эффективность распараллеливания.

---

## 2. Problem Statement
Требуется вычислить значение определенного интеграла функции $f(\mathbf{x}) = \sum_{i=1}^D x_i^2$ на заданной многомерной области (домене) с радиусом $R$.

**Входные данные:**
* Интегрируемая функция: $f(\mathbf{x})$
* Область интегрирования: `kHyperCube` или `kHyperSphere`
* Радиус области: $R$ (для тестов $R=1$)
* Количество выборок: $N$ (для производительности $N=1,000,000$)

**Выходные данные:**
* Одно действительное число: приближенное значение интеграла.

**Эталонные значения (D=2, R=1):**
| Домен | Объем $V$ | Точное Значение $I$ |
| :--- | :--- | :--- |
| **Куб** | $V=4$ | $I = 8/3 \approx 2.66667$ |
| **Сфера** | $V=\pi$ | $I = \pi/2 \approx 1.57080$ |

---

## 3. Baseline Algorithm (Sequential)
Базовый алгоритм основан на оценке среднего значения функции $\langle f \rangle$ в области. Интеграл вычисляется по формуле: $I \approx V \cdot \langle f \rangle$.

1.  **Генерация:** Сгенегировать $N$ случайных точек $\mathbf{x}_i$.
2.  **Обработка Домена:** Если домен — **Гиперсфера**, применить **метод принятия-отклонения** (acceptance-rejection) для учета только точек, попавших внутрь сферы.
3.  **Суммирование:** Вычислить сумму $S = \sum f(\mathbf{x}_i)$.
4.  **Результат:** $I = V \cdot S / N_{total}$.

Ключевой аспект реализации — корректное переключение между логикой генерации точек и расчета объема $V$ для двух доменов.

---

## 4. Parallelization Scheme
Метод Монте-Карло демонстрирует идеальный параллелизм, поскольку каждая выборка независима.

**Схема:**
* **Декомпозиция данных:** Общее число выборок $N$ делится на $P$ процессов, каждый из которых получает $N_p = N/P$ итераций (**блочное распределение**).
* **Локальные Вычисления:** Каждый процесс **независимо** выполняет свою часть выборок $N_p$ и рассчитывает **частичную сумму** $\text{Sum}_p$.
* **Коммуникация:** Для агрегации результатов используется одна коллективная операция **`MPI_Reduce`**.
    * **Оператор:** `MPI_SUM`
    * **Назначение:** Сбор всех частичных сумм в одну глобальную $\text{Sum}_{total}$ на корневом процессе (`rank 0`).
* **Роли:** Корневой процесс выполняет финализирующий расчет интеграла.

Минимальные коммуникационные издержки (передача одного числа с процесса) обеспечивают высокую масштабируемость.

---

## 5. Implementation Details
* **Структура:** Логика реализована в классе `MonteCarloIntegration` с двумя методами `RunImpl` для SEQ и MPI, наследующимися от базового класса задачи.
* **Граничные случаи:** Реализовано строгое ветвление (`if/else if`) для корректного выбора алгоритма генерации точек и формулы объема в зависимости от типа домена (`kHyperCube` или `kHyperSphere`).
* **Контроль Качества:** Для поддержания единого стиля кодирования в проекте использовался хук **`clang-format`**.
* **Память:** Использование памяти низкое, так как на каждом процессе хранится только текущая сумма и параметры задачи.

---

## 6. Experimental Setup
### 6.1. Окружение
* **Hardware/OS:** Среда Docker (Ubuntu)
* **Toolchain:** g++ (версия 13.3.0), OpenMPI, CMake (Сборка **Release**)
* **Процессы:** Тесты производительности запускались для $P=1$ (SEQ) и $P=4$ (MPI).

### 6.2. Данные
* **Количество выборок ($N$):** $1,000,000$
* **Размерность ($D$):** 3D.
* **Тестирование:** Проводились замеры времени выполнения для интегрирования по Гиперсфере.

---

## 7. Results and Discussion

### 7.1 Correctness
Корректность подтверждена успешным прохождением **всех 45 функциональных тестов GoogleTest** в рамках тестового набора проекта.
* **Проверка:** Результаты SEQ и MPI версий сравнивались с точными аналитическими значениями $I_{cube}$ и $I_{sphere}$, подтверждая точность обеих реализаций.
* **Покрытие:** Высокое покрытие кода подтверждает корректную работу сложной логики выбора домена и распределения работы между MPI-процессами.

### 7.2 Performance
Замеры времени выполнения для $N=1,000,000$ выборок:

| Mode | Count ($P$) | Time, s | Speedup ($S_P$) | Efficiency ($E$) |
| :--- | :--- | :--- | :--- | :--- |
| **seq** | 1 | $T_{seq} = 0.0224$ | **1.00** | N/A |
| **mpi** | 4 | $T_{mpi} = 0.0073$ | $\mathbf{3.06}$ | $\mathbf{76.5\%}$ |

* **Ускорение (Speedup):** $S_4 = T_{seq} / T_{mpi} \approx \mathbf{3.06\times}$
* **Эффективность (Efficiency):** $E = S_4 / P \approx \mathbf{76.5\%}$

**Обсуждение:**
Полученное ускорение $\mathbf{3.06\times}$ близко к идеальному линейному ускорению ($4\times$). Эффективность $76.5\%$ является очень высоким показателем, который свидетельствует о том, что накладные расходы на коллективную операцию `MPI_Reduce` невелики по сравнению с объемом полезных вычислений. Это подтверждает, что ММК — это **идеальная задача для параллелизации** с использованием MPI.

---

## 8. Conclusions
Задача полностью решена. Была успешно реализована и протестирована MPI-версия метода Монте-Карло, поддерживающая интегрирование по Гиперкубу и Гиперсфере.

* **Достижения:** Получено значительное ускорение $\mathbf{3.06\times}$ на 4 процессах с высокой эффективностью $\mathbf{76.5\%}$.
* **Вывод:** Высокая эффективность подтверждает, что параллельный подход является оптимальным для масштабирования вычислений ММК, особенно при больших объемах выборок.

---

## 9. References
1.  Документация poznayka: Основные функции MPI - https://poznayka.org/s6430t1.html.
2.  Документация opennet: MPI для начинающих - https://www.opennet.ru/docs/RUS/MPI_intro/#Coll.
3.  Документация habr: объяснение метода интегрирования Монте-Карло - https://habr.com/ru/articles/835870/.

## Appendix (Optional)
```cpp
// Фрагмент кода, демонстрирующий сбор результатов:
// Итоговая сумма локальных результатов собирается на корневом процессе.
double local_sum = 0.0;
// ... (Локальные вычисления N_local раз)
double global_sum = 0.0;

// Коллективная операция: агрегация частичных сумм
MPI_Reduce(&local_sum, &global_sum, 1, MPI_DOUBLE, MPI_SUM, 0, MPI_COMM_WORLD);

if (process_rank == 0) {
    // I = V * global_sum / N_total
    GetOutput() = GetInput().volume * global_sum / GetInput().samples;
}