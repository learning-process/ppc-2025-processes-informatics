# <Построение выпуклой оболочки для компонент бинарного изображения>

- Student: Чёкотов Алексей Павлович, группа 3823Б1ФИ2
- Technology: SEQ | MPI
- Variant: 32

## 1. Introduction
При увеличении размеров матриц до тысяч строк и столбцов последовательные вычисления становятся вычислительно затратными, что создает потребность в эффективных стратегиях распараллеливания.
Ожидается, что MPI-реализация обеспечит значительное ускорение по сравнению с последовательной версией при сохранении высокой эффективности использования вычислительных ресурсов.

## 2. Problem Statement
Необходимо найти для заданной матрицы минимальные значения в каждом из столбцов.
Входные данные: Матрица vector\<vector\<int>>
Выходные данные: Массив vector\<int>

Числовой диапазон - целочисленные значения от -2^31 до 2^31-1.
Также в матрице должны быть строки одинаковой длины.

## 3. Baseline Algorithm (Sequential)
Инициализировать вектор по количеству столбцов в матрице максимальным числом из диапазона.
Для каждого столбца:
- Пройти по строкам матрицы и сравнить с текущим значением в векторе ответа.
- Если число из матрицы меньше, то обновить значение в векторе ответа.

## 4. Parallelization Scheme
1. Определяем количество процессов и размеры матрицы.
2. Нулевой процесс рассылает столбцы между другими процессами.
3. Каждый процесс ищет минимальные значения для своих столбцов.
4. Вычисляем для каждого процесса: сколько значений он вычислил и какое смещение должно быть в итоговом векторе.
5. Сбор результатов со всех процессов.

## 5. Implementation Details
Структура проекта:
- mpi версия: tasks\chyokotov_a_convex_hull_finding\mpi\
- Последовательная версия: tasks\chyokotov_a_convex_hull_finding\seq\
- Функциональные тесты: tasks\chyokotov_a_convex_hull_finding\tests\functional\
- Производительность: tasks\chyokotov_a_convex_hull_finding\tests\performance\

Для рассылки столбцов используется MPI_Send(), для приема данных MPI_Recv().
Для сбора и объеденения данных всех процессов используется MPI_Allgatherv().

Кроме потребления памяти на входные и выходные данные в mpi версии используются:
- Массивы у каждого процесса для вычисления минимальных значений в своих столбцах.
- Два массива для вычисления количества столбцов, в которых процесс проводил вычисления, и смещения в итоговом векторе.

## 6. Experimental Setup
- CPU - AMD Ryzen 5 5500U with Radeon Graphics(2.10 GHz)
- 6 cores, 12 treads.
- OS: Windows 11, 25Н2 version
- compiler: clang version 21.1.0 build type Release
- Data: Генерация матрицы 6000*6000.

## 7. Results and Discussion

### 7.1 Correctness
Функциональные тесты для проверки на корректность:
- Пустая матрица
- Матрица 1*1
- Матрица 2*2
- Матрица 2*4
- Матрица с максимальными значениями
- Матрица с минимальными значениями
- Матрица с одним столбцом
- Матрица с одной строчкой

### 7.2 Performance
Тест на производительность выполнялся на матрице размером 4000*4000.

| Mode        | Count | Time, s | Speedup | Efficiency |
|-------------|-------|---------|---------|------------|
| seq         | 1     | 1.5616  | 1.00    | N/A        |
| mpi         | 2     | 0.9161  | 1.70    | 85.9%      |
| mpi         | 4     | 0.5451  | 2.86    | 71.5%      |
| mpi         | 6     | 0.4081  | 3.82    | 63.8%      |

Наблюдается ускорение с ростом числа процессов, а эффективность постепенно снижается из-за роста наклодных расходов для коммуникации между процессами, что подтверждает оптимальность выбранной схемы распределения данных между процессами.

## 8. Conclusions
Были реализованы SEQ и MPI версии алгоритма и в результате сравнения их работы выяснилось, что решение хорошо масштабируется для задачи.

## 9. References
1. [Документация OpenMPI](https://www.open-mpi.org/doc/)
2. [стандарт с++](https://www.open-std.org)