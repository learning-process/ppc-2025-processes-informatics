# Нахождение минимальных значений по строкам матрицы

- Студентка: Романова Василиса Дмитриевна, группа 3823Б1ФИ3
- Технология: SEQ, MPI
- Вариант: 17

## 1. Введение
В рамках данной работы рассматривается задача освоения базовых возможностей MPI на примере реализации алгоритма поиска минимума в строках матрицы. Эта операция агрегации данных является распространенной в таких областях, как научные вычисления и машинное обучение, однако при последовательном выполнении становится узким местом из-за больших объемов данных. Ожидается, что параллельная реализация позволит добиться снижения времени работы алгоритма за счет распределения вычислительной нагрузки между несколькими процессами.

## 2. Постановка задачи
На вход подается матрица, необходимо найти минимальные значения в каждой из ее строк.

- **Входные данные:** `std::vector<std::vector<int>>`.
- **Выходные данные:** `std::vector<int>` — вектор минимальных значений, на i-ой позиции расположено минимальное значение в i-ой строке матрицы.

**Ограничения:** матрица и её строки должны быть непустыми.

## 3. Базовый алгоритм (последовательный)
Программа итерируется по строкам матрицы. Для каждой строки `res_[i]` инициализируется 0-ым значением в строке, затем запускается обход всех элементов строки. На каждом шаге переменной `res_[i]` присваивается минимум из её текущего значения и значения просматриваемого элемента строки. В конце обхода i-ой строки в `res_[i]` находится значение минимального элемента в строке.

## 4. Описание параллельного алгоритма
Параллельная реализация алгоритма основана на распределении строк матрицы между процессами. Процесс с рангом 0 определяет количество строк для обработки каждым процессом и подготавливает данные для рассылки и последующего сбора. Для удобства пересылки исходный двумерный массив преобразуется в одномерный. Также заполняются вспомогательные массивы `send_counts`, `recv_counts`, `scatt_displs` и `gath_displs`.

Подготовленные данные рассылаются процессам с помощью функций `MPI_Bcast` и `MPI_Scatterv`. Получив свою порцию данных, каждый процесс создаёт массив для записи результата и приступает к поиску минимумов на своей части матрицы.

После завершения вычислений всеми процессами результаты собираются на процессе с рангом 0 с помощью функции `MPI_Gatherv`. Финализация и синхронизация итоговых данных выполняются с применением `MPI_Bcast`.

## 5. Experimental Setup
- Hardware/OS: Intel i5-1135G7, 4 ядра, 8 логических процессов, RAM 8GB, Windows 10
- Toolchain: clang 21.1.0, Release
- Data: матрица размером 1000 на 10000 читается из файла data/matrixForPerfTest.txt

## 6. Результаты

### 6.1 Корректность
Корректность последовательного и параллельного алгоритмов проверена при помощи функциональных тестов разных размеров, включая граничный случай с матрицей 1×1. 

Тестовые сценарии охватывают следующие варианты данных:
- числа в строках расположены в случайном порядке;
- значения отсортированы по возрастанию;
- значения отсортированы по убыванию;
- все элементы строки имеют одинаковые значения.

### 6.2 Производительность
| Mode        | Count | Time, s | Speedup | Efficiency |
|-------------|-------|---------|---------|------------|
| seq         | 1     | 0.057   | 1.00    | N/A        |
| mpi         | 2     | 0.054   | 1.05    | 52.5%      |
| mpi         | 4     | 0.051   | 1.11    | 27.7%      |

Прирост производительности от распараллеливания незначителен. Это объясняется тем, что накладные расходы на обмен данными между процессами превышают время самих вычислений, что типично для задач со сравнительно небольшим объемом входных данных.

## 7. Выводы
В ходе работы были реализованы и протестированы последовательный и параллельный алгоритмы поиска минимумов по строкам матрицы.

Анализ производительности показал ограниченную эффективность параллельного подхода. Основная причина — высокие накладные расходы на межпроцессную коммуникацию, которые превышают время полезных вычислений.

Таким образом, для решения данной задачи распараллеливание с использованием MPI не даёт значимого выигрыша при текущем размере матрицы. Для достижения положительного эффекта требуются либо бо́льшие объёмы данных, либо более сложные вычислительные операции.

## 8. Литература
1. Стандарт MPI.
2. Лекции и практики по параллельному программированию.

## 9. Приложение

```cpp
bool RomanovaVMinByMatrixRowsMPI::RunImpl() {
  int n = 0;
  int rank = 0;
  int delta = 0;
  int extra = 0;

  MPI_Comm_size(MPI_COMM_WORLD, &n);
  MPI_Comm_rank(MPI_COMM_WORLD, &rank);

  std::vector<int> recv_counts(n);
  std::vector<int> send_counts(n);

  std::vector<int> displs_scatt(n);
  std::vector<int> displs_gath(n);

  OutType flat_data;

  if (rank == 0) {
    n_ = in_data_.size();
    m_ = ((!in_data_.empty()) ? in_data_[0].size() : 0);

    delta = static_cast<int>(n_ / n);
    extra = static_cast<int>(n_ % n);

    recv_counts = std::vector<int>(n, delta);
    recv_counts[n - 1] += extra;

    send_counts = std::vector<int>(n, static_cast<int>(delta * m_));
    send_counts[n - 1] += static_cast<int>(extra * m_);

    for (int i = 1; i < n; i++) {
      displs_gath[i] = displs_gath[i - 1] + delta;
      displs_scatt[i] = displs_scatt[i - 1] + static_cast<int>(delta * m_);
    }

    for (const auto &vec : in_data_) {
      flat_data.insert(flat_data.end(), vec.begin(), vec.end());
    }
  }

  MPI_Bcast(&n_, 1, MPI_INT, 0, MPI_COMM_WORLD);
  MPI_Bcast(&m_, 1, MPI_INT, 0, MPI_COMM_WORLD);

  MPI_Bcast(&delta, 1, MPI_INT, 0, MPI_COMM_WORLD);
  MPI_Bcast(&extra, 1, MPI_INT, 0, MPI_COMM_WORLD);

  OutType local_data((delta + (rank == n - 1 ? extra : 0)) * m_);

  MPI_Scatterv(rank == 0 ? flat_data.data() : nullptr, send_counts.data(), displs_scatt.data(), MPI_INT,
               local_data.data(), static_cast<int>(local_data.size()), MPI_INT, 0, MPI_COMM_WORLD);

  OutType temp(delta + (rank == n - 1 ? extra : 0));

  for (size_t i = 0; i < temp.size(); i++) {
    temp[i] = local_data[i * m_];
    for (size_t j = 1; j < m_; j++) {
      temp[i] = std::min(temp[i], local_data[(i * m_) + j]);
    }
  }

  res_ = OutType(n_);

  MPI_Gatherv(temp.data(), static_cast<int>(temp.size()), MPI_INT, res_.data(), recv_counts.data(), displs_gath.data(),
              MPI_INT, 0, MPI_COMM_WORLD);
  MPI_Bcast(res_.data(), static_cast<int>(n_), MPI_INT, 0, MPI_COMM_WORLD);

  return true;
}
```