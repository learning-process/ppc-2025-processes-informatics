# Отчет по реализации алгоритма: повышение контраста полутонового изображения методом линейной растяжки гистограммы

- **Студент**: Киселев Игорь Вячеславович, группа 3823Б1ФИ1  
- **Преподаватели**: Нестеров Александр Юрьевич, Оболенский Арсений Андреевич  
- **Технология**: SEQ | MPI  
- **Вариант**: линейная растяжка гистограммы  

---

## 1. Введение

Цель данной работы — реализовать и исследовать алгоритм повышения контраста полутонового изображения с использованием линейной растяжки гистограммы, а также сравнить последовательную (SEQ) и параллельную (MPI) реализации данного алгоритма.

Повышение контраста является одной из базовых операций обработки изображений. Данный метод используется для улучшения визуального восприятия изображений, особенно в случаях, когда исходный диапазон яркостей ограничен и изображение выглядит «блеклым».

С ростом размеров изображений (например, в задачах обработки спутниковых снимков или медицинских изображений) возрастает время обработки данных. Это делает актуальным использование параллельных вычислений, в частности технологии MPI, для ускорения выполнения подобных алгоритмов.

---

## 2. Постановка задачи

Требуется реализовать алгоритм линейной растяжки гистограммы для полутонового изображения.

**Входные данные:**
- Структура `Image`, содержащая:
  - `width` — ширина изображения;
  - `height` — высота изображения;
  - `pixels` — одномерный массив значений яркости пикселей типа `uint8_t` (в диапазоне от 0 до 255).
- Размер массива `pixels` должен быть равен `width * height`.

**Выходные данные:**
- Вектор `std::vector<uint8_t>`, содержащий обработанное изображение после линейной растяжки гистограммы.

**Ограничения и условия:**
- Значения пикселей находятся в диапазоне [0, 255].
- Входное изображение не должно быть пустым.
- В случае, если все пиксели изображения имеют одинаковое значение, изображение возвращается без изменений.
- Реализации SEQ и MPI должны выдавать одинаковый результат.

---

## 3. Базовый алгоритм (SEQ)

Последовательный алгоритм выполняет следующие шаги:

1. Поиск минимального и максимального значений яркости во всём изображении.
2. Если минимальное и максимальное значения совпадают, изображение копируется без изменений.
3. В противном случае для каждого пикселя применяется формула линейной растяжки:

**Псевдокод:**   
min = minimum(pixels)
max = maximum(pixels)

if min == max:
output = input
else:
for each pixel p:
output[p] = round((p - min) * 255 / (max - min))
return output


**Сложность алгоритма:**  
`O(N)`, где `N = width * height`.

---

## 4. Схема распараллеливания (MPI)

### Распределение данных

1. Корневой процесс (rank 0):
   - определяет общее количество пикселей изображения;
   - вычисляет, сколько пикселей будет передано каждому процессу;
   - формирует массивы `counts` и `offsets` для передачи данных.
2. Распределение пикселей между процессами выполняется с помощью `MPI_Scatterv`.

### Обработка

Каждый процесс:
- получает свой фрагмент изображения;
- находит локальные минимальное и максимальное значения яркости;
- участвует в коллективных операциях `MPI_Allreduce` для вычисления глобальных `min` и `max`;
- выполняет линейную растяжку только для своего фрагмента изображения.

### Сбор результатов

- Обработанные фрагменты собираются на корневом процессе с помощью `MPI_Gatherv`.
- Итоговое изображение формируется только на процессе с `rank = 0`.

---

## 5. Детали реализации

### Структура проекта

- `kiselev_i_linear_histogram_stretch/common/include/common.hpp` — описание структуры входных и выходных данных.
- `kiselev_i_linear_histogram_stretch/seq/include/ops_seq.hpp` и `ops_seq.cpp` — последовательная реализация.
- `kiselev_i_linear_histogram_stretch/mpi/include/ops_mpi.hpp` и `ops_mpi.cpp` — параллельная реализация с использованием MPI.
- `tests/functional` — функциональные тесты.
- `tests/performance` — перфоманс-тесты.

### Обработка пустых данных в MPI

В реализации MPI предусмотрена корректная обработка случаев, когда некоторым процессам достаётся нулевое количество пикселей.  
В таких случаях локальные значения минимума и максимума инициализируются граничными значениями, что предотвращает искажение результата при выполнении `MPI_Allreduce`.

### Использование памяти

- Полный массив изображения хранится только на корневом процессе.
- Каждый процесс хранит только свой фрагмент изображения.
- Дополнительная память используется только для служебных массивов `counts` и `offsets`.

---

## 6. Тестовая конфигурация

**Оборудование:**
- **Процессор:** Intel(R) Core(TM) i5-8265U CPU @ 1.60GHz  
- **ОЗУ:** 8 ГБ  
- **ОС:** Windows 10 Pro  
- **MPI:** Microsoft MPI версии 10.1  
- **Режим компиляции:** Release (`/O2`)

---

## 7. Корректность и тестирование

### 7.1 Корректность

Корректность реализации проверялась с помощью функциональных тестов, которые включают:
- изображения с полным диапазоном яркостей;
- изображения с ограниченным диапазоном значений;
- изображения, состоящие из одинаковых пикселей;
- небольшие изображения, где количество пикселей меньше числа MPI-процессов.

Результаты MPI-версии сравнивались с эталонной SEQ-реализацией. Все тесты были успешно пройдены.

---

### 7.2 Перфоманс-тесты

Для оценки производительности использовалось изображение размером **16000 × 16000 пикселей**.  
Для анализа выбраны значения времени выполнения в режиме **pipeline**, так как он наиболее наглядно отражает полное время обработки задачи.

| Реализация | Режим     | Время, с |Ускорение
|-----------|-----------|----------|
| SEQ       | pipeline  | 0.97     |1.0
| MPI (4)   | pipeline  | 0.38     |2.6

**Как считалось ускорение:**
S = T_seq / T_mpi ≈ 2.6

MPI-реализация показала заметное ускорение по сравнению с последовательной версией за счёт параллельной обработки пикселей изображения.

---

## 8. Заключение

В ходе работы была реализована последовательная и параллельная версии алгоритма линейной растяжки гистограммы для полутоновых изображений.

MPI-реализация позволяет значительно сократить время обработки больших изображений по сравнению с SEQ-версией. Основной выигрыш достигается за счёт равномерного распределения пикселей между процессами и параллельного выполнения вычислений.

При малых размерах изображений накладные расходы на передачу данных могут снижать эффективность MPI, однако при больших объёмах данных параллельный подход оказывается оправданным.

Реализация корректно обрабатывает все граничные случаи и успешно проходит набор функциональных и перфоманс-тестов.

---

## 9. Литература

1. https://learning-process.github.io/parallel_programming_course/ru/  
2. https://learn.microsoft.com/ru-ru/message-passing-interface/mpi-scatterv-function  
3. https://learn.microsoft.com/ru-ru/message-passing-interface/mpi-allreduce-function  
4. https://habr.com/ru/articles/121235/  

## Приложения (Код параллельной реализации)

```
bool KiselevITestTaskMPI::RunImpl() {
  int rank = 0;
  int size = 1;
  MPI_Comm_rank(MPI_COMM_WORLD, &rank);
  MPI_Comm_size(MPI_COMM_WORLD, &size);

  const int root = 0;

  std::size_t total_size = 0;
  if (rank == root) {
    total_size = GetInput().pixels.size();
  }

  // Рассылка общего количества элементов
  MPI_Bcast(&total_size, 1, MPI_UNSIGNED_LONG_LONG, root, MPI_COMM_WORLD);

  const std::size_t base = total_size / size;
  const std::size_t extra = total_size % size;

  std::vector<int> counts(size, 0);
  std::vector<int> offsets(size, 0);

  if (rank == root) {
    std::size_t shift = 0;
    for (int i = 0; i < size; ++i) {
      counts[i] = static_cast<int>(base + (i < static_cast<int>(extra) ? 1 : 0));
      offsets[i] = static_cast<int>(shift);
      shift += counts[i];
    }
  }

  int local_count = 0;
  MPI_Scatter(counts.data(), 1, MPI_INT,
              &local_count, 1, MPI_INT,
              root, MPI_COMM_WORLD);

  std::vector<uint8_t> local_pixels(local_count);

  MPI_Scatterv(GetInput().pixels.data(),
               counts.data(), offsets.data(), MPI_UNSIGNED_CHAR,
               local_pixels.data(), local_count, MPI_UNSIGNED_CHAR,
               root, MPI_COMM_WORLD);

  uint8_t local_min = std::numeric_limits<uint8_t>::max();
  uint8_t local_max = std::numeric_limits<uint8_t>::min();

  if (local_pixels.empty()) {
    local_min = std::numeric_limits<uint8_t>::max();
    local_max = std::numeric_limits<uint8_t>::min();
  }


  for (uint8_t px : local_pixels) {
    if (px < local_min) local_min = px;
    if (px > local_max) local_max = px;
  }

  uint8_t global_min = 0;
  uint8_t global_max = 0;

  MPI_Allreduce(&local_min, &global_min, 1, MPI_UNSIGNED_CHAR, MPI_MIN, MPI_COMM_WORLD);
  MPI_Allreduce(&local_max, &global_max, 1, MPI_UNSIGNED_CHAR, MPI_MAX, MPI_COMM_WORLD);

  if (global_min != global_max) {
    const double scale =
        255.0 / static_cast<double>(global_max - global_min);

    for (std::size_t i = 0; i < local_pixels.size(); ++i) {
      const double value =
          static_cast<double>(local_pixels[i] - global_min) * scale;
      local_pixels[i] = static_cast<uint8_t>(value + 0.5);
    }
  }

  MPI_Gatherv(local_pixels.data(), local_count, MPI_UNSIGNED_CHAR,
              GetOutput().data(), counts.data(), offsets.data(),
              MPI_UNSIGNED_CHAR, root, MPI_COMM_WORLD);

  return true;
}
```