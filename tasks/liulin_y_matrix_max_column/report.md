# **Отчёт по задаче поиска максимума в столбцах матрицы с использованием турнирного алгоритма**

**Студент:** *Люлин Ярослав Сергеевич*
**Группа:** *3823Б1ФИ1*
**Технология:** SEQ | MPI
**Вариант:** *16*

---

# **1. Введение**

Поиск максимальных значений в столбцах матрицы — одна из типичных операций, применяемых в анализе данных, статистической обработке, машинном обучении и численных методах. В реальных системах такие операции часто используются многократно, поэтому эффективность их исполнения влияет на общую производительность приложения.

В данной работе рассматривается задача нахождения максимума в каждом столбце целочисленной матрицы с использованием **турнирного (tournament) алгоритма**, который выполняет последовательные попарные сравнения элементов до выявления максимального значения. Такой подход позволяет:

* сократить глубину сравнения до `O(log n)` для одного столбца,
* естественно распараллеливать обработку разных столбцов,
* адаптироваться к MPI-параллельной версии.

Реализованы две версии:

1. **SEQ (последовательная)** — обработка всех столбцов на одном процессе.
2. **MPI (параллельная)** — распределение столбцов между MPI-процессами с последующей агрегацией результатов.

---

# **2. Постановка задачи**

Дана целочисленная матрица `A` размера `n × m`. Требуется вычислить массив `B` длины `m`, где:

```
B[j] = max(A[i][j]) для всех i = 0..n-1, j = 0..m-1
```

**Входные данные:**

* числа `n` и `m`,
* `n × m` целых чисел в построчном виде.

**Выходные данные:**

* массив из `m` целых чисел — максимумы столбцов.

**Ограничения:**

* Все значения помещаются в 32-битный знаковый тип.

---

# **3. Базовый алгоритм (последовательный)**

Последовательная версия использует **турнирный алгоритм** для каждого столбца.

**Описание алгоритма:**

1. Для каждого столбца матрицы создаётся временный массив `column`.
2. Копия массива `column` помещается в `temp`, по которому выполняется турнирная редукция:

   * элементы сравниваются попарно,
   * победители переходят на следующий раунд,
   * процесс повторяется, пока не останется один элемент — **максимум столбца**.
3. Максимум сохраняется в выходной массив `out[col_idx]`.

**Код:**

```cpp
  const auto &matrix = GetInput();
  auto &out = GetOutput();

  if (matrix.empty() || matrix[0].empty()) {
    return true;
  }
  const int rows = static_cast<int>(matrix.size());
  const int cols = static_cast<int>(matrix[0].size());

  for (int col_idx = 0; col_idx < cols; ++col_idx) {
    std::vector<int> column(rows);
    for (int row = 0; row < rows; ++row) {
      column[row] = matrix[row][col_idx];
    }

    int size = rows;
    std::vector<int> temp = column;

    while (size > 1) {
      int new_size = 0;
      for (int i = 0; i < size; i += 2) {
        temp[new_size] = (i + 1 < size) ? std::max(temp[i], temp[i + 1]) : temp[i];
        ++new_size;
      }
      size = new_size;
    }

    out[col_idx] = temp[0];
  }

  return true;

```

**Преимущества:**

* независимая обработка каждого столбца,
* логарифмическая глубина турнира (`O(log n)`).

**Недостатки:**

* все столбцы обрабатываются последовательно,
* не учитывается балансировка нагрузки между потоками/процессами.

---

# **4. Параллельная версия (MPI)**

MPI-версия распределяет столбцы между процессами:

1. **Root-процесс (rank 0)**:

   * получает размеры матрицы,
   * разворачивает матрицу в одномерный массив `flat_matrix`,
   * вычисляет распределение столбцов для `MPI_Scatterv`.

2. **Распределение столбцов**:

   * каждый процесс получает примерно `m / p` столбцов,
   * первые `remainder = m % p` процессов получают на один столбец больше,
   * лишние процессы получают ноль столбцов.

3. **Локальные вычисления**:

   * каждый процесс применяет турнирный алгоритм к своим столбцам,
   * формирует вектор локальных максимумов.

4. **Сбор результатов**:

   * root-процесс собирает максимумы с помощью `MPI_Gatherv`,
   * распределяет итоговый массив всем процессам через `MPI_Bcast`.

**Фрагмент MPI-кода:**

```cpp
  const auto &in = GetInput();
  auto &out = GetOutput();

  int world_size = 0;
  int world_rank = 0;
  MPI_Comm_size(MPI_COMM_WORLD, &world_size);
  MPI_Comm_rank(MPI_COMM_WORLD, &world_rank);

  int rows = 0;
  int cols = 0;
  if (world_rank == 0 && !in.empty() && !in[0].empty()) {
    rows = static_cast<int>(in.size());
    cols = static_cast<int>(in[0].size());
  }

  MPI_Bcast(&rows, 1, MPI_INT, 0, MPI_COMM_WORLD);
  MPI_Bcast(&cols, 1, MPI_INT, 0, MPI_COMM_WORLD);

  if (rows <= 0 || cols <= 0) {
    out.clear();
    return true;
  }

  out.assign(cols, std::numeric_limits<int>::min());

  std::vector<int> sendcounts;
  std::vector<int> displs;
  PrepareCounts(rows, cols, world_size, sendcounts, displs);

  std::vector<int> flat_matrix;
  if (world_rank == 0) {
    FillFlatMatrix(in, rows, cols, flat_matrix);
  }

  int local_cols = sendcounts[world_rank] / rows;
  int local_elements = local_cols * rows;
  std::vector<int> local_data(local_elements);

  MPI_Scatterv(world_rank == 0 ? flat_matrix.data() : nullptr, sendcounts.data(), displs.data(), MPI_INT,
               local_data.data(), local_elements, MPI_INT, 0, MPI_COMM_WORLD);

  std::vector<int> local_maxes = ComputeLocalMaxes(local_data, rows, local_cols);

  std::vector<int> recvcounts(world_size);
  std::vector<int> displs_gather(world_size, 0);
  for (int proc = 0; proc < world_size; ++proc) {
    recvcounts[proc] = sendcounts[proc] / rows;
    if (proc > 0) {
      displs_gather[proc] = displs_gather[proc - 1] + recvcounts[proc - 1];
    }
  }

  MPI_Gatherv(local_maxes.data(), local_cols, MPI_INT, out.data(), recvcounts.data(), displs_gather.data(), MPI_INT, 0,
              MPI_COMM_WORLD);

  MPI_Bcast(out.data(), cols, MPI_INT, 0, MPI_COMM_WORLD);

  return true;
```

**Преимущества MPI-версии:**

* хорошая масштабируемость при умеренном числе процессов,
* турнирный алгоритм остаётся эффективным локально.

**Недостатки:**

* накладные расходы на `Scatterv`, `Gatherv` и `Bcast`,
* при большом числе процессов коммуникации начинают доминировать над вычислениями.

---

# **5. Детали реализации**

* Загрузка матриц для функциональных тестов из текстовых файлов.
* Большие матрицы для теста производительности (например, 20 000 × 20 000).
* Полная копия матрицы хранится только в root-процессе.
* Все процессы получают равномерные блоки столбцов.
* Турнирный алгоритм реализован через временный массив `temp`.
* Используемые MPI-функции: `MPI_Scatterv`, `MPI_Gatherv`, `MPI_Bcast`.

---

# **6. Тестовое окружение**

**Аппаратное обеспечение / ОС:**
Intel Core i5-12450H, 16 GB DDR4
Ubuntu 24.04.2 LTS X86_64
MPI: OpenMPI

**Инструменты сборки:**
CMake, clang++ (Release)

**Тестовые данные:**

* ручные тесты малого размера,
* случайные матрицы,
* большие матрицы для perf-тестов,

---

# **7. Результаты**

## 7.1 Корректность

Общие тесты показывают идентичность результатов SEQ и MPI:

* матрица 1×1,
* матрица с одним столбцом,
* неквадратные матрицы,
* матрица из нулей,
* случайные матрицы.

---

## 7.2 Производительность

Были выполнены измерения времени работы последовательной и MPI-версии алгоритма поиска максимума по столбцам. При 1 процессе SEQ ожидаемо работает быстрее, так как нет накладных расходов MPI. При увеличении числа процессов до 2–3 MPI показывает небольшое ускорение, потому что работа распределяется между процессами, а объём коммуникаций остаётся низким. Начиная примерно с 4–5 процессов время MPI перестаёт уменьшаться: накладные расходы на передачу данных и синхронизацию становятся сопоставимы с вычислениями. На 7 процессах наблюдается скачок времени из-за перегрузки ресурсов CPU (ядра, кэш, память). Оптимальное число процессов для данной задачи — 2–4.

| Процессы | MPI Pipeline | MPI Task Run | SEQ Pipeline | SEQ Task Run |
| -------- | ------------ | ------------ | ------------ | ------------ |
| 1        | 0.012876     | 0.012471     | 0.007422     | 0.007349     |
| 2        | 0.010768     | 0.010140     | 0.007530     | 0.007613     |
| 3        | 0.010766     | 0.010486     | 0.008637     | 0.008734     |
| 4        | 0.011366     | 0.011191     | 0.009857     | 0.009866     |
| 5        | 0.012946     | 0.014189     | 0.010615     | 0.010667     |
| 6        | 0.013623     | 0.014810     | 0.010777     | 0.010512     |
| 7        | 0.017189     | 0.019338     | 0.022284     | 0.022311     |
| 8        | 0.015934     | 0.017073     | 0.011500     | 0.013307     |

### Анализ
* В целом MPI реализация медленее чем SEQ, связано это с тем что MPI ограничен тем, что вычисления по столбцам простые, а расходы на обмен данными становятся доминирующими.

---

# **8. Заключение**

Реализованы SEQ и MPI версии поиска максимума по столбцам на основе турнирного алгоритма. SEQ быстрее на одном процессе и подходит для небольших матриц. MPI даёт работает относительно быстро при 2–4 процессах, но при дальнейшем увеличении процессов эффективность падает из-за коммуникаций. Для больших матриц MPI может давать выигрыш, но для малых данных предпочтительна последовательная версия.

---

# **9. Источники**

1. OpenMPI: документация [Электронный ресурс] // [https://www.open-mpi.org](https://www.open-mpi.org)
2. Сысоев А. В., Курс лекций по параллельному программированию

---

