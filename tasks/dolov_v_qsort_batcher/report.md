# Быстрая сортировка с четно-нечетным слиянием Бэтчера

- **Student:** Долов Вячеслав Васильевич, group 3823Б1ФИ3
- **Technology:** SEQ | MPI
- **Variant:** 15

## 1. Introduction
Сортировка больших массивов данных является фундаментальной задачей в области параллельных вычислений. Одним из наиболее эффективных методов для распределенных систем является алгоритм, сочетающий локальную **быструю сортировку** (Quicksort) и общую схему обмена данными, известную как **четно-нечетное слияние Бэтчера** (Batcher odd-even mergesort). Данный метод позволяет упорядочить данные между процессами за детерминированное количество шагов, минимизируя простои оборудования и обеспечивая корректное распределение элементов по возрастанию их рангов.

**Цель работы:** Реализовать последовательную (SEQ) и параллельную (MPI) версии алгоритма быстрой сортировки с последующим параллельным четно-нечетным слиянием по принципу Бэтчера.

---

## 2. Problem Statement
Задача заключается в упорядочивании массива вещественных чисел (типа `double`) в условиях распределенной памяти.

**Входные данные:**
* Общее количество элементов: $N$
* Массив: `std::vector<double>`
* Количество MPI процессов: $P$

**Выходные данные:**
* Отсортированный по возрастанию массив, собранный на нулевом процессе.
* Подтверждение корректности результата (проверка функции `std::is_sorted`).

---

## 3. Baseline Algorithm (Sequential)
Последовательный алгоритм выполняет сортировку данных в рамках одного адресного пространства:
1. **Подготовка:** Данные копируются во внутренний буфер для последующей обработки.
2. **Сортировка:** Реализована итеративная версия алгоритма **Quicksort**. Для обеспечения стабильной работы с большими объемами данных и исключения риска переполнения системного стека используется явное управление границами подмассивов через стек задач (`std::stack<std::pair<int, int>>`).
3. **Разбиение:** Применяется классическая схема Хоара (Hoare partition) для выбора опорного элемента и разделения массива на подмассивы.
4. **Результат:** Полностью отсортированный массив возвращается в вызывающую систему.

---

## 4. Parallelization Scheme
Параллельная версия реализует логику распределенного слияния на основе четно-нечетной перестановки:

1. **Scatter (Распределение):** Исходный массив разбивается на части с учетом возможного остатка от деления $N$ на $P$. Размеры порций и смещения вычисляются в `PreProcessingImpl`, после чего данные рассылаются процессам через `MPI_Scatterv`.
2. **Локальная фаза:** Каждый процесс независимо выполняет `FastSort` (итеративный Quicksort) над своим локальным буфером `local_buffer_`, подготавливая данные к этапу слияния.
3. **Фаза четно-нечетного взаимодействия:** Организуется цикл из $P$ итераций (где $P$ — количество процессов) для последовательного упорядочивания данных между парами процессов:
    - На **четных** шагах взаимодействия происходят внутри пар процессов (0,1), (2,3) и т.д.
    - На **нечетных** шагах взаимодействуют пары (1,2), (3,4) и т.д.
    - Процессы-партнеры обмениваются данными с помощью `MPI_Sendrecv`.
    - **Процедура Merge-Split:** В функции `MergeSequences` происходит слияние локального и полученного массивов в единую упорядоченную последовательность. Процесс с меньшим рангом оставляет себе первую (меньшую) половину, а процесс с большим рангом — вторую (большую).
4. **Gather (Сбор):** После завершения всех итераций отсортированные блоки данных собираются на процессе с рангом 0 через `MPI_Gatherv`, формируя итоговый упорядоченный массив.

---

## 5. Implementation Details
* **Классы:** `DolovVQsortBatcherSEQ` и `DolovVQsortBatcherMPI`.
* **Ключевые механизмы:**
    - `FastSort`: итеративный Quicksort, соответствующий требованиям статического анализа (без использования рекурсии).
    - `GetSplitIndex`: расчет индекса разделения по схеме Хоара.
    - `MergeSequences`: слияние двух отсортированных последовательностей и выборка соответствующей половины массива (low/high) в зависимости от взаимного расположения рангов процессов.
    - `ExecuteBatcherParallel`: программная реализация логики четно-нечетных перестановок Бэтчера.

---

## 6. Experimental Setup
* **Hardware/OS:**
  - Ноутбук: Redmi Book Pro 16 2024
  - CPU: Intel(R) Core(TM) Ultra 5 125H (14 ядер, 18 потоков) @ 1.20 GHz
  - RAM: 32 GB
  - OS: Windows 11 Home
  - Среда выполнения: Dev Container (Docker, Ubuntu)
* **Toolchain:**
  - CMake 3.28.3
  - Компилятор: g++ 13.3.0
  - Библиотека: OpenMPI
  - Тип сборки: Release
* **Data:**
  - Массив из 1000000 случайных чисел типа `double`.
  - Измерения проводились на $P=4$ процессах.

---

## 7. Results and Discussion

### 7.1 Correctness
Корректность подтверждена **45 функциональными тестами**.
* Проверены граничные случаи (пустой массив, массив из 1 элемента).
* Проверены специфические наборы данных (уже отсортированные, инвертированные массивы).
* Все тесты прошли успешно (`PASSED`).

### 7.2 Performance
Замеры времени выполнения для различных объемов входных данных и разного количества процессов:

**Результаты для 1,000,000 элементов:**

| Mode | Count ($P$) | Time, s | Speedup ($S_P$) | Efficiency ($E$) |
| :--- | :--- | :--- | :--- | :--- |
| **seq** | 1 | 0.0736998 | **1.000** | N/A |
| **mpi** | 2 | 0.0400613 | **1.840** | 92.0% |
| **mpi** | 4 | 0.0338597 | **2.177** | 54.4% |
| **mpi** | 8 | 0.0283607 | **2.599** | 32.5% |

**Вывод:** При работе с 1 млн элементов параллельная версия демонстрирует стабильный рост производительности. Ускорение на 8 процессах составило **2.6x**. Снижение эффективности при росте числа процессов объясняется увеличением доли накладных расходов на коммуникации относительно времени локальной сортировки.

---

**Результаты для 10,000,000 элементов:**

| Mode | Count ($P$) | Time, s | Speedup ($S_P$) | Efficiency ($E$) |
| :--- | :--- | :--- | :--- | :--- |
| **seq** | 1 | 0.8964242 | **1.000** | N/A |
| **mpi** | 2 | 0.5643199 | **1.589** | 79.5% |
| **mpi** | 4 | 0.3739014 | **2.398** | 60.0% |
| **mpi** | 8 | 0.3178480 | **2.820** | 35.3% |

**Вывод:** Анализ результатов показывает различную динамику эффективности. На малом объеме данных ($10^6$) при $P=2$ достигается максимальная эффективность (**92%**), однако она резко снижается при дальнейшем увеличении числа процессов. На большом объеме данных ($10^7$) начальная эффективность ниже из-за накладных расходов на передачу больших массивов, но она падает значительно медленнее (сохраняя **60%** на 4 процессах против **54.4%**). Это демонстрирует лучшую масштабируемость алгоритма на задачах с высокой вычислительной сложностью, где время локальной сортировки преобладает над временем межпроцессорного обмена.

---

## 8. Conclusions
В ходе работы был успешно реализован алгоритм быстрой сортировки с четно-нечетным слиянием Бэтчера.

* **Эффективность:** Комбинация быстрого локального алгоритма и структурированной схемы обменов Бэтчера показала стабильное ускорение на многопроцессорной системе. В ходе тестов было достигнуто максимальное ускорение в **2.82 раза** на 8 процессах для массива из 10 млн элементов.
* **Качество кода:** Программа соответствует стандартам разработки (использование итеративного подхода вместо рекурсии, безопасная работа с памятью и использование std::move для оптимизации пересылок), что подтверждено прохождением функциональных тестов.
* **Масштабируемость:** Реализованная схема позволяет эффективно использовать вычислительные ресурсы распределенных систем для решения задач сортировки больших массивов. Наблюдается рост эффективности алгоритма при увеличении размерности входных данных.

---

## 9. References
1. Документация habr: Четно-нечетная сортировка слиянием Бэтчера - https://habr.com/ru/articles/261777/.
2. Документация poznayka: Основные функции MPI - https://poznayka.org/s6430t1.html.
3. Лекции ННГУ по курсу "Параллельное программирование".

---

## Appendix
```cpp
// Реализация логики четно-нечетного слияния Бэтчера
for (int step = 0; step < world_size; ++step) {
    int partner = -1;
    // Определение партнера согласно четности шага
    if (step % 2 == 0) {
        partner = (world_rank % 2 == 0) ? world_rank + 1 : world_rank - 1;
    } else {
        partner = (world_rank % 2 != 0) ? world_rank + 1 : world_rank - 1;
    }

    if (partner >= 0 && partner < world_size) {
        neighbor_data.resize(part_sizes_[partner]);
        // Обмен данными между партнерами для слияния
        MPI_Sendrecv(local_buffer_.data(), local_buffer_.size(), MPI_DOUBLE, partner, 0,
                     neighbor_data.data(), neighbor_data.size(), MPI_DOUBLE, partner, 0, 
                     MPI_COMM_WORLD, MPI_STATUS_IGNORE);

        std::vector<double> merged;
        // Слияние и выборка нужной половины массива
        MergeSequences(local_buffer_, neighbor_data, merged, world_rank < partner);
        local_buffer_ = std::move(merged);
    }
    MPI_Barrier(MPI_COMM_WORLD);
}