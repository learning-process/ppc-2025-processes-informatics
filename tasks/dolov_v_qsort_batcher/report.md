# Быстрая сортировка с четно-нечетным слиянием Бэтчера

- **Student:** Долов Вячеслав Васильевич, group 3823Б1ФИ3
- **Technology:** SEQ | MPI
- **Variant:** 15

## 1. Introduction
Сортировка больших массивов данных является фундаментальной задачей в области параллельных вычислений. Одним из наиболее эффективных методов для распределенных систем является алгоритм, сочетающий локальную **быструю сортировку** (Quicksort) и общую схему обмена данными, известную как **четно-нечетное слияние Бэтчера** (Batcher odd-even mergesort). Данный метод позволяет упорядочить данные между процессами за детерминированное количество шагов, минимизируя простои оборудования и обеспечивая корректное распределение элементов по возрастанию их рангов.

**Цель работы:** Реализовать последовательную (SEQ) и параллельную (MPI) версии алгоритма быстрой сортировки с последующим параллельным четно-нечетным слиянием по принципу Бэтчера.

---

## 2. Problem Statement
Задача заключается в упорядочивании массива вещественных чисел (типа `double`) в условиях распределенной памяти.

**Входные данные:**
* Общее количество элементов: $N$
* Массив: `std::vector<double>`
* Количество MPI процессов: $P$

**Выходные данные:**
* Отсортированный по возрастанию массив, собранный на нулевом процессе.
* Подтверждение корректности результата (проверка функции `std::is_sorted`).

---

## 3. Baseline Algorithm (Sequential)
Последовательный алгоритм выполняет сортировку данных в рамках одного адресного пространства:
1. **Подготовка:** Данные копируются во внутренний буфер для последующей обработки.
2. **Сортировка:** Реализована итеративная версия алгоритма **Quicksort**. Для обеспечения стабильной работы с большими объемами данных и исключения риска переполнения системного стека используется явное управление границами подмассивов через стек задач (`std::stack<std::pair<int, int>>`).
3. **Разбиение:** Применяется классическая схема Хоара (Hoare partition) для выбора опорного элемента и разделения массива на подмассивы.
4. **Результат:** Полностью отсортированный массив возвращается в вызывающую систему.

---

## 4. Parallelization Scheme
Параллельная версия реализует алгоритм распределенной сортировки, сочетающий локальную быструю сортировку и глобальную сеть слияния Бэтчера (Batcher's Odd-Even Mergesort):

1. **Scatter (Распределение):** Исходный массив разбивается на части с учетом возможного остатка от деления $N$ на $P$. Размеры порций и смещения вычисляются в `PreProcessingImpl`, после чего данные рассылаются процессам через `MPI_Scatterv`.
2. **Локальная сортировка:** Каждый процесс независимо выполняет быструю сортировку (`FastSort`) над своим локальным набором данных. Это гарантирует, что перед началом этапа слияния каждый процесс владеет уже отсортированной последовательностью.
3. **Сеть слияния Бэтчера (Batcher's Merge Network):** Вместо линейного перебора соседей используется итеративная сеть слияния, состоящая из нескольких этапов:
    - Организуется трехуровневый цикл (параметры $p$, $k$, $j$), который формирует структуру сети сравнений.
    - На каждой итерации определяются пары процессов-партнеров (`p1` и `p2`).
    - Процессы обмениваются данными с помощью `MPI_Sendrecv`.
    - **Процедура Merge-Split:** В функции `MergeSequences` локальные и полученные данные объединяются в один упорядоченный массив. В зависимости от позиции в паре, процесс оставляет себе либо меньшую, либо большую половину объединенного массива, что в конечном итоге приводит к глобальной упорядоченности данных между всеми процессами за $O(\log^2 P)$ шагов взаимодействия.
4. **Gather (Сбор):** После завершения работы сети Бэтчера отсортированные блоки данных собираются на процессе с рангом 0 через `MPI_Gatherv`, формируя итоговый упорядоченный массив.

---

## 5. Implementation Details
* **Классы:** `DolovVQsortBatcherSEQ` и `DolovVQsortBatcherMPI`.
* **Ключевые механизмы:**
    - `FastSort`: итеративный Quicksort, соответствующий требованиям статического анализа (без использования рекурсии).
    - `GetSplitIndex`: расчет индекса разделения по схеме Хоара.
    - `MergeSequences`: слияние двух отсортированных последовательностей и выборка соответствующей половины массива (low/high) в зависимости от взаимного расположения рангов процессов.
    - `ExecuteBatcherParallel`: программная реализация логики четно-нечетных перестановок Бэтчера.

---

## 6. Experimental Setup
* **Hardware/OS:**
  - Ноутбук: Redmi Book Pro 16 2024
  - CPU: Intel(R) Core(TM) Ultra 5 125H (14 ядер, 18 потоков) @ 1.20 GHz
  - RAM: 32 GB
  - OS: Windows 11 Home
  - Среда выполнения: Dev Container (Docker, Ubuntu)
* **Toolchain:**
  - CMake 3.28.3
  - Компилятор: g++ 13.3.0
  - Библиотека: OpenMPI
  - Тип сборки: Release
* **Data:**
  - Массив из 1000000 случайных чисел типа `double`.
  - Измерения проводились на $P=4$ процессах.

---

## 7. Results and Discussion

### 7.1 Correctness
Корректность подтверждена **45 функциональными тестами**.
* Проверены граничные случаи (пустой массив, массив из 1 элемента).
* Проверены специфические наборы данных (уже отсортированные, инвертированные массивы).
* Все тесты прошли успешно (`PASSED`).

## 7.2 Performance
Замеры времени выполнения для различных объемов входных данных и разного количества процессов:

**Результаты для 1,000,000 элементов:**

| Mode | Count ($P$) | Time, s | Speedup ($S_P$) | Efficiency ($E$) |
| :--- | :--- | :--- | :--- | :--- |
| **seq** | 1 | 0.1150926 | **1.000** | N/A |
| **mpi** | 2 | 0.0811884 | **1.418** | 70.9% |
| **mpi** | 4 | 0.0559014 | **2.059** | 51.5% |
| **mpi** | 8 | 0.0368994 | **3.119** | 39.0% |

**Вывод:** При обработке 1 млн элементов параллельная версия демонстрирует устойчивый рост производительности, достигая максимального ускорения **3.12x** на 8 процессах. Снижение эффективности при увеличении числа узлов до 8 обусловлено тем, что на относительно небольшом объеме данных время, затрачиваемое на коммуникации в сети Бэтчера и синхронизацию (`MPI_Barrier`), начинает составлять существенную долю от общего времени работы алгоритма.

---

**Результаты для 10,000,000 элементов:**

| Mode | Count ($P$) | Time, s | Speedup ($S_P$) | Efficiency ($E$) |
| :--- | :--- | :--- | :--- | :--- |
| **seq** | 1 | 1.4117684 | **1.000** | N/A |
| **mpi** | 2 | 0.8012916 | **1.762** | 88.1% |
| **mpi** | 4 | 0.4753996 | **2.970** | 74.3% |
| **mpi** | 8 | 0.3764272 | **3.750** | 46.9% |

**Вывод:** На объеме в 10 млн элементов алгоритм показывает значительно более высокую эффективность масштабирования. Ускорение на 8 процессах составило **3.75x**. Сравнение результатов показывает, что эффективность на 8 процессах увеличилась до **46.9%** (по сравнению с **39.0%** на меньшем объеме). Это подтверждает высокую масштабируемость системы на задачах с большой вычислительной нагрузкой: при увеличении объема локальных данных время чистой сортировки преобладает над коммуникационными расходами, что позволяет более эффективно использовать ресурсы параллельной архитектуры.

---

## 8. Conclusions
В ходе работы был успешно реализован алгоритм быстрой сортировки с четно-нечетным слиянием Бэтчера.

* **Эффективность:** Комбинация быстрого локального алгоритма и структурированной схемы обменов Бэтчера показала стабильное ускорение на многопроцессорной системе. В ходе тестов было достигнуто максимальное ускорение в **3.75 раз** на 8 процессах для массива из 10 млн элементов.
* **Качество кода:** Программа соответствует стандартам разработки (использование итеративного подхода вместо рекурсии, безопасная работа с памятью и использование std::move для оптимизации пересылок), что подтверждено прохождением функциональных тестов.
* **Масштабируемость:** Реализованная схема позволяет эффективно использовать вычислительные ресурсы распределенных систем для решения задач сортировки больших массивов. Наблюдается рост эффективности алгоритма при увеличении размерности входных данных.

---

## 9. References
1. Документация habr: Четно-нечетная сортировка слиянием Бэтчера - https://habr.com/ru/articles/261777/.
2. Документация poznayka: Основные функции MPI - https://poznayka.org/s6430t1.html.
3. Лекции ННГУ по курсу "Параллельное программирование".

---

## Appendix

```cpp
// Итеративная реализация сети слияния Бэтчера
for (int p = 1; p < world_size; p <<= 1) {
    for (int k = p; k >= 1; k >>= 1) {
        for (int j = k % p; j <= world_size - 1 - k; j += (k << 1)) {
            for (int i = 0; i < k; ++i) {
                int p1 = i + j;
                int p2 = i + j + k;

                // Проверка принадлежности пары к одному блоку слияния
                if ((p1 / (p * 2)) == (p2 / (p * 2))) {
                    if (world_rank == p1 || world_rank == p2) {
                        int partner = (world_rank == p1) ? p2 : p1;
                        if (partner < world_size) {
                            std::vector<double> neighbor_data(part_sizes_[partner]);
                            
                            // Обмен данными между процессами-партнерами
                            MPI_Sendrecv(local_buffer_.data(), static_cast<int>(local_buffer_.size()), 
                                         MPI_DOUBLE, partner, 0,
                                         neighbor_data.data(), part_sizes_[partner], 
                                         MPI_DOUBLE, partner, 0, 
                                         MPI_COMM_WORLD, MPI_STATUS_IGNORE);

                            std::vector<double> merged;
                            // Слияние последовательностей: меньший ранг забирает min-элементы, больший - max
                            MergeSequences(local_buffer_, neighbor_data, merged, world_rank == p1);
                            local_buffer_ = std::move(merged);
                        }
                    }
                }
            }
        }
        // Синхронизация всех процессов после каждого этапа сети слияния
        MPI_Barrier(MPI_COMM_WORLD);
    }
}