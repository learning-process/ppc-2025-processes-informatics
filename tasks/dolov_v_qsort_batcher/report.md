# Быстрая сортировка с четно-нечетным слиянием Бэтчера

- **Student:** Долов Вячеслав Васильевич, group 3823Б1ФИ3
- **Technology:** SEQ | MPI
- **Variant:** 15

## 1. Introduction
Сортировка больших массивов данных является фундаментальной задачей в области параллельных вычислений. Одним из наиболее эффективных методов для распределенных систем является алгоритм, сочетающий локальную **быструю сортировку** (Quicksort) и глобальную схему обмена данными, известную как **четно-нечетное слияние Бэтчера** (Batcher odd-even mergesort). Данный метод позволяет упорядочить данные между процессами за детерминированное количество шагов, минимизируя простои оборудования и обеспечивая корректное распределение элементов по возрастанию их рангов.

**Цель работы:** Реализовать последовательную (SEQ) и параллельную (MPI) версии алгоритма быстрой сортировки с последующим параллельным четно-нечетным слиянием по принципу Бэтчера.

---

## 2. Problem Statement
Задача заключается в упорядочивании массива вещественных чисел (типа `double`) по возрастанию в условиях распределенной памяти.

**Входные данные:**
* Общее количество элементов: $N$
* Массив: `std::vector<double>`
* Количество MPI процессов: $P$

**Выходные данные:**
* Отсортированный по возрастанию массив, собранный на нулевом процессе.
* Подтверждение корректности результата (проверка функции `std::is_sorted`).

---

## 3. Baseline Algorithm (Sequential)
Последовательный алгоритм выполняет сортировку данных в рамках одного адресного пространства:
1. **Подготовка:** Данные копируются во внутренний буфер для последующей обработки.
2. **Сортировка:** Используется итеративный алгоритм **Quicksort**. Для обеспечения безопасности и исключения переполнения стека при работе с большими объемами данных рекурсия заменена на работу с явным стеком (`std::stack<std::pair<int, int>>`).
3. **Разбиение:** Применяется классическая схема Хоара (Hoare partition) для выбора опорного элемента и разделения массива на подмассивы.
4. **Результат:** Полностью отсортированный массив возвращается в вызывающую систему.

---

## 4. Parallelization Scheme
Параллельная версия реализует логику распределенного четно-нечетного слияния:
1. **Scatter:** Исходный массив разбивается на части (с учетом возможного остатка от деления $N$ на $P$) и рассылается процессам через `MPI_Scatterv`.
2. **Локальная фаза:** Каждый процесс выполняет `FastSort` (итеративный Quicksort) над своей частью данных.
3. **Фаза четно-нечетного слияния Бэтчера:** Организуется цикл из $P$ шагов (где $P$ — количество процессов):
    - На **четных** шагах взаимодействуют пары процессов (0,1), (2,3) и т.д.
    - На **нечетных** шагах взаимодействуют пары (1,2), (3,4) и т.д.
    - Процессы-партнеры обмениваются данными через `MPI_Sendrecv`.
    - Выполняется процедура **Merge-Split**: после слияния локального и полученного массивов один процесс оставляет себе элементы с меньшими значениями, другой — с большими.
4. **Gather:** После завершения всех итераций отсортированные блоки собираются на `rank 0` через `MPI_Gatherv`.

---

## 5. Implementation Details
* **Классы:** `DolovVQsortBatcherSEQ` и `DolovVQsortBatcherMPI`.
* **Ключевые механизмы:**
    - `FastSort`: итеративный Quicksort, соответствующий требованиям статического анализа (без использования рекурсии).
    - `GetSplitIndex`: расчет индекса разделения по схеме Хоара.
    - `MergeSequences`: слияние двух отсортированных последовательностей и выборка соответствующей половины массива (low/high) в зависимости от взаимного расположения рангов процессов.
    - `ExecuteBatcherParallel`: программная реализация логики четно-нечетных перестановок Бэтчера.

---

## 6. Experimental Setup
* **Hardware/OS:**
  - Ноутбук: Redmi Book Pro 16 2024
  - CPU: Intel(R) Core(TM) Ultra 5 125H (14 ядер, 18 потоков) @ 1.20 GHz
  - RAM: 32 GB
  - OS: Windows 11 Home
  - Среда выполнения: Dev Container (Docker, Ubuntu)
* **Toolchain:**
  - CMake 3.28.3
  - Компилятор: g++ 13.3.0
  - Библиотека: OpenMPI
  - Тип сборки: Release
* **Data:**
  - Массив из 1000000 случайных чисел типа `double`.
  - Измерения проводились на $P=4$ процессах.

---

## 7. Results and Discussion

### 7.1 Correctness
Корректность подтверждена **45 функциональными тестами**.
* Проверены граничные случаи (пустой массив, массив из 1 элемента).
* Проверены специфические наборы данных (уже отсортированные, инвертированные массивы).
* Все тесты прошли успешно (`PASSED`).

### 7.2 Performance
Результаты замера времени выполнения в режиме `pipeline`:

| Mode | Count ($P$) | Time, s | Speedup ($S_P$) |
| :--- | :--- | :--- | :--- |
| **seq** | 1 | 0.0780019 | **1.000** |
| **mpi** | 4 | 0.0319961 | **2.438** |

**Вывод:** Параллельная версия работает в **2.44 раза** быстрее последовательной на 4 процессах. Учитывая накладные расходы на пересылку данных большого объема через `MPI_Scatterv`/`MPI_Gatherv` и необходимость барьерной синхронизации на каждой итерации слияния, такой показатель ускорения является весомым результатом.

---

## 8. Conclusions
В ходе работы был успешно реализован алгоритм быстрой сортировки с четно-нечетным слиянием Бэтчера.

* **Эффективность:** Комбинация быстрого локального алгоритма и структурированной схемы обменов Бэтчера показала стабильное ускорение на многопроцессорной системе.
* **Качество кода:** Программа соответствует стандартам разработки (использование итеративного подхода вместо рекурсии, безопасная работа с памятью), что подтверждено прохождением функциональных тестов.
* **Масштабируемость:** Реализованная схема позволяет эффективно использовать вычислительные ресурсы распределенных систем для решения задач сортировки больших массивов.

---

## 9. References
1. Документация habr: Четно-нечетная сортировка слиянием Бэтчера - https://habr.com/ru/articles/261777/.
2. Документация poznayka: Основные функции MPI - https://poznayka.org/s6430t1.html.
3. Лекции ННГУ по курсу "Параллельное программирование".

---

## Appendix
```cpp
// Реализация логики четно-нечетного слияния Бэтчера (Batcher Odd-Even Merge):
for (int step = 0; step < world_size; ++step) {
    int partner = -1;
    // Определение партнера согласно четности шага (четно-нечетное слияние)
    if (step % 2 == 0) {
        partner = (world_rank % 2 == 0) ? world_rank + 1 : world_rank - 1;
    } else {
        partner = (world_rank % 2 != 0) ? world_rank + 1 : world_rank - 1;
    }

    if (partner >= 0 && partner < world_size) {
        neighbor_data.resize(part_sizes_[partner]);
        // Обмен данными между партнерами для слияния
        MPI_Sendrecv(local_buffer_.data(), local_buffer_.size(), MPI_DOUBLE, partner, 0,
                     neighbor_data.data(), neighbor_data.size(), MPI_DOUBLE, partner, 0, 
                     MPI_COMM_WORLD, MPI_STATUS_IGNORE);

        std::vector<double> merged;
        // Слияние и выборка нужной половины массива (Merge-Split)
        MergeSequences(local_buffer_, neighbor_data, merged, world_rank < partner);
        local_buffer_ = std::move(merged);
    }
    MPI_Barrier(MPI_COMM_WORLD);
}