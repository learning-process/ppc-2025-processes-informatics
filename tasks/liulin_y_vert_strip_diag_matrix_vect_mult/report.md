# **Отчёт по задаче: умножение матрицы на вектор по вертикально-ленточной схеме**

**Студент:** Люлин Ярослав Сергеевич  
**Группа:** 3823Б1ФИ1  
**Технология:** SEQ | MPI  
**Вариант:** 12 

# **1. Введение**

Умножение матрицы на вектор является одной из базовых операций линейной алгебры и широко применяется в численном моделировании, машинном обучении, обработке сигналов и компьютерной графике. В реальных приложениях такие вычисления часто выполняются многократно над большими матрицами, поэтому эффективность реализации существенно влияет на общую производительность системы.

В данной работе реализованы две версии алгоритма умножения плотной целочисленной матрицы `A` размера `n × m` на вектор `x` длины `m`:

1. **SEQ (последовательная)** — классическое матричное умножение на одном процессе.
2. **MPI (параллельная)** — распределение вычислений по вертикально-ленточной схеме (разделение по столбцам матрицы и соответствующим элементам вектора).

# **2. Постановка задачи**

Дана целочисленная матрица `A` размера `n × m` и вектор `x` длины `m`.  
Требуется вычислить вектор-результат `y` длины `n`, где:

```
y[i] = Σ_{j=0}^{m-1} A[i][j] * x[j]    (i = 0..n-1)
```

**Входные данные:**  
матрица `A` (n строк по m элементов) и вектор `x` (m элементов).

**Выходные данные:**  
вектор `y` длины `n`.

**Ограничения:**  
все значения и промежуточные суммы помещаются в 32-битный знаковый тип `int`.

# **3. Базовый алгоритм (последовательный)**

Последовательная версия реализует стандартное матричное умножение с накоплением:

внешний цикл по строкам результата, внутренний — по столбцам матрицы.

**Фрагмент кода:**
```cpp
  const auto &input = GetInput();
  const auto &matrix = std::get<0>(input);
  const auto &vect = std::get<1>(input);
  auto &out = GetOutput();

  if (matrix.empty() || matrix[0].empty()) {
    return true;
  }

  const int rows = static_cast<int>(matrix.size());
  const int cols = static_cast<int>(matrix[0].size());

  for (int i = 0; i < rows; ++i) {
    for (int j = 0; j < cols; ++j) {
      out[i] += matrix[i][j] * vect[j];
    }
  }

  return true;

```

**Преимущества:**  
сложность `O(n·m)`,  
хорошая локальность доступа к данным по строкам,  
полная независимость вычислений по строкам результата.

**Недостатки:**  
все вычисления выполняются последовательно на одном процессе.

# **4. Параллельная версия (MPI)**

MPI-версия использует вертикально-ленточную схему распределения данных:

1. **Root-процесс (rank 0)** получает полную матрицу и вектор, разворачивает матрицу в одномерный массив в column-major порядке.
2. **Распределение** осуществляется с помощью `MPI_Scatterv`: каждый процесс получает примерно `m / p` столбцов матрицы и соответствующие элементы вектора `x`.
3. **Локальные вычисления**: каждый процесс вычисляет частичные суммы для всех строк результата, используя свои столбцы.
4. **Агрегация** результатов выполняется через `MPI_Reduce(MPI_SUM)` на root-процессе с последующим `MPI_Bcast` итогового вектора всем процессам.

**Ключевые MPI-операции:**  
`MPI_Scatterv` (матрица и вектор), `MPI_Reduce`, `MPI_Bcast`.

**Преимущества:**  
высокая степень параллелизма при большом числе столбцов,  
объём коммуникаций пропорционален `n·m / p`.

**Недостатки:**  
значительные накладные расходы на коммуникации,  
при небольшом числе столбцов или большом числе процессов коммуникации начинают доминировать.
**Фрагмент кода:**
```cpp
  const auto &input = GetInput();
  auto &out = GetOutput();

  int world_size = 0;
  int world_rank = 0;
  MPI_Comm_size(MPI_COMM_WORLD, &world_size);
  MPI_Comm_rank(MPI_COMM_WORLD, &world_rank);

  int rows = 0;
  int cols = 0;
  if (world_rank == 0) {
    const auto &matrix = std::get<0>(input);
    if (!matrix.empty() && !matrix[0].empty()) {
      rows = static_cast<int>(matrix.size());
      cols = static_cast<int>(matrix[0].size());
    }
  }

  MPI_Bcast(&rows, 1, MPI_INT, 0, MPI_COMM_WORLD);
  MPI_Bcast(&cols, 1, MPI_INT, 0, MPI_COMM_WORLD);

  if (rows <= 0 || cols <= 0) {
    out.clear();
    return true;
  }

  out.assign(rows, 0);

  std::vector<int> sendcounts;
  std::vector<int> displs;
  PrepareCounts(rows, cols, world_size, sendcounts, displs);

  std::vector<int> recvcounts(world_size);
  std::vector<int> displs_gather(world_size, 0);
  for (int proc = 0; proc < world_size; ++proc) {
    recvcounts[proc] = sendcounts[proc] / rows;
    if (proc > 0) {
      displs_gather[proc] = displs_gather[proc - 1] + recvcounts[proc - 1];
    }
  }

  int local_cols = recvcounts[world_rank];
  int local_elements = local_cols * rows;
  std::vector<int> local_data(local_elements);

  std::vector<int> flat_matrix;
  if (world_rank == 0) {
    const auto &matrix = std::get<0>(input);
    FillFlatMatrix(matrix, rows, cols, flat_matrix);
  }

  MPI_Scatterv(world_rank == 0 ? flat_matrix.data() : nullptr, sendcounts.data(), displs.data(), MPI_INT,
               local_data.data(), local_elements, MPI_INT, 0, MPI_COMM_WORLD);

  std::vector<int> local_vect(local_cols);
  if (world_rank == 0) {
    const auto &vect = std::get<1>(input);
    MPI_Scatterv(vect.data(), recvcounts.data(), displs_gather.data(), MPI_INT, local_vect.data(), local_cols, MPI_INT,
                 0, MPI_COMM_WORLD);
  } else {
    MPI_Scatterv(nullptr, recvcounts.data(), displs_gather.data(), MPI_INT, local_vect.data(), local_cols, MPI_INT, 0,
                 MPI_COMM_WORLD);
  }

  std::vector<int> local_partial = ComputeLocalPartialRes(local_data, local_vect, rows, local_cols);

  MPI_Reduce(local_partial.data(), out.data(), rows, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);

  MPI_Bcast(out.data(), rows, MPI_INT, 0, MPI_COMM_WORLD);

  return true;
```

# **5. Детали реализации**

Вход представлен как `std::tuple<std::vector<std::vector<int>>, std::vector<int>>`.  
Функциональные тесты загружают данные из файлов (матрица построчно, затем вектор).  
Производительность измерялась на квадратных матрицах размером 8192 × 8192.  
Валидация упрощена для совместимости с тестовым фреймворком.  
Конструкторы используют раздельное копирование элементов tuple для стабильной сборки под GCC 14.


# **6. Тестовое окружение**

**Аппаратное обеспечение / ОС:**
Intel Core i5-12450H, 16 GB DDR4
Ubuntu 24.04.2 LTS X86_64
MPI: OpenMPI

**Инструменты сборки:**
CMake, clang++ (Release)

**Тестовые данные:**  
10 ручных функциональных тестов (малые, прямоугольные, с отрицательными значениями и т.д.),  
случайные матрицы 8192 × 8192 для perf-тестов.

---

# **7. Результаты**

## 7.1 Корректность

Все функциональные тесты пройдены успешно для SEQ и MPI версий:  
квадратные и прямоугольные матрицы,  
матрицы 1×1, 1×m, n×1,  
нулевой вектор,  
отрицательные элементы.  
Результаты SEQ и MPI идентичны.

## 7.2 Производительность

Измерения проводились на квадратной матрице 8192 × 8192.  
Указаны средние значения времени выполнения (в секундах).

| Количество процессов | SEQ Pipeline | SEQ Task Run | MPI Pipeline | MPI Task Run |
|----------------------|--------------|--------------|--------------|--------------|
| 1                    | 0.001426     | 0.001438     | 0.010282     | 0.009446     |
| 2                    | 0.001615     | 0.001622     | 0.011110     | 0.010023     |
| 3                    | 0.002388     | 0.001822     | 0.011505     | 0.011075     |
| 4                    | 0.002032     | 0.001896     | 0.012816     | 0.011133     |
| 5                    | 0.002059     | 0.002079     | 0.012816     | 0.011577     |
| 6                    | 0.004288     | 0.004232     | 0.018208     | 0.017047     |
| 7                    | 0.002398     | 0.002415     | 0.015513     | 0.014233     |
| 8                    | 0.004699     | 0.004907     | 0.019474     | 0.017688     |

### Анализ

При одном процессе последовательная версия значительно быстрее MPI (в 6–7 раз) за счёт отсутствия коммуникационных накладных расходов.  
При росте числа процессов MPI-версия показывает небольшое улучшение относительно одного процесса, однако остаётся медленнее SEQ.  
Начиная с 6–8 процессов время MPI резко возрастает из-за доминирования затрат на коммуникации (`Scatterv`, `Reduce`, `Bcast`) и конкуренции за ресурсы.  
Последовательная версия демонстрирует стабильное время выполнения независимо от количества процессов (фактически используется один процесс).

**Вывод:** для данной задачи и размера данных последовательная реализация является наиболее эффективной. MPI-версия не даёт ускорения из-за высоких коммуникационных затрат при относительно простой вычислительной нагрузке на процесс.

# **8. Заключение**

Реализованы корректные SEQ и MPI версии умножения матрицы на вектор по вертикально-ленточной схеме.  
Последовательная версия показывает наилучшую производительность на всех конфигурациях.  
MPI-реализация страдает от значительных накладных расходов на обмен данными, что делает её менее эффективной для данной задачи на тестовом размере матрицы.  
Вертикально-ленточная схема потенциально выгодна при существенно большем числе столбцов или на кластерах с быстрым interconnect, однако в текущих условиях предпочтительна последовательная версия.

# **9. Источники**

1. OpenMPI Documentation [Электронный ресурс]. URL: https://www.open-mpi.org  
2. Сысоев А. В., Курс лекций по параллельному программированию