Отчёт
Scatter-распределение от одного ко всем (scatter)
Студент: Ашихмин Даниил, группа 3823Б1ФИ1

Технология: SEQ-MPI

Вариант: Начальный

1. Введение
В данной работе реализуется параллельный алгоритм scatter-распределения (разбиение массива данных на части и рассылка их от одного процесса всем остальным) с использованием технологии MPI. Цель — продемонстрировать способность распределения данных между процессами с применением tree-based алгоритма и провести оценку корректности и производительности решения.

2. Постановка задачи
Дан массив data длины N. Требуется разбить его на равные части по elements_per_process элементов и разослать эти части от выбранного процесса root всем остальным процессам, включая сам корневой процесс.

Входные данные:

Массив data длины N = elements_per_process * world_size произвольного типа (int, float, double)

Целое число elements_per_process > 0

Номер процесса root, с которого идёт рассылка

Выходные данные:

У каждого процесса локальный буфер размера elements_per_process, содержащий соответствующую часть исходного массива

Требования:

Алгоритм должен корректно работать на любом числе процессов

Рассылка должна выполняться по дереву с использованием только MPI_Send и MPI_Recv

Поддержка трёх типов данных: MPI_INT, MPI_FLOAT, MPI_DOUBLE

Последовательная версия должна корректно воспроизводить результат для сравнения

3. Базовый алгоритм (последовательный)
Последовательная версия имитирует scatter-операцию:

Проверяется, является ли процесс корневым (root == 0)

Если процесс корневой, то копируются первые elements_per_process элементов в локальный буфер

Для некорневых процессов (или root ≠ 0 в последовательной версии) заполняет буфер нулями

4. Схема распараллеливания (MPI)
Распределение данных:
Процесс root содержит исходный массив размера elements_per_process * world_size

Массив логически разбивается на world_size блоков по elements_per_process элементов

Каждый процесс должен получить свой соответствующий блок

Схема взаимодействия процессов:
Каждый процесс вычисляет virtual_rank = (rank - root + size) % size

На каждом шаге mask проверяется:

Если (virtual_rank & mask) == 0, процесс отправляет данные потомкам:
dest_virtual = virtual_rank | mask, dest_real = (dest_virtual + root) % size

Если процесс является корневым, он отправляет данные из соответствующей части исходного массива

Иначе отправляет свой текущий локальный буфер

Иначе процесс получает данные от предка:
src_virtual = virtual_rank & (~mask), src_real = (src_virtual + root) % size

После получения данных процесс становится узлом, который может отправлять дальше

Псевдокод:
virtual_rank = (rank - root + size) % size
local_data = new vector[elements_per_process]

mask = 1
while mask < size:
    if (virtual_rank & mask) == 0:
        dest_virtual = virtual_rank | mask
        if dest_virtual < size:
            dest_real = (dest_virtual + root) % size
            if rank == root:
                offset = dest_virtual * elements_per_process
                send data[offset:offset+elements_per_process] to dest_real
            else:
                send local_data to dest_real
    else:
        src_virtual = virtual_rank & (~mask)
        src_real = (src_virtual + root) % size
        recv local_data from src_real
        break
    mask <<= 1

if rank == root:
    offset = virtual_rank * elements_per_process
    copy data[offset:offset+elements_per_process] to local_data

Топология:
Используется стандартный коммуникатор MPI_COMM_WORLD

Дерево бинарного типа, построенное на основе виртуальных рангов

5. Детали реализации
Структура кода:

common.hpp — определение структуры ScatterParams и базовых типов

ops_seq.hpp/cpp — последовательный вариант

ops_mpi.hpp/cpp — MPI-вариант, реализующий tree-based scatter через MPI_Send и MPI_Recv

tests/functional/main.cpp — функциональные тесты

tests/performance/main.cpp — тесты производительности

Особенности реализации:

Вспомогательные функции вынесены в анонимное пространство имен для улучшения читаемости

Использованы шаблоны для поддержки разных типов данных

Алгоритм корректно обрабатывает любые значения root через нормализацию

Проверка границ при обращении к массиву для избежания выхода за пределы

Основные функции:

GetMPIDataType<T>() — сопоставление C++ типов с MPI типами

VirtualToRealRank() — преобразование виртуального ранга в реальный

SendBlock() и ReceiveBlock() — вспомогательные функции для отправки/приема блоков данных

6. Экспериментальная установка
Аппаратное обеспечение: CPU: AMD Ryzen 5 3500X (3.6 - 4.1 GHz)(6 ядер / 6 потоков) RAM: 16 ГБ OS: Windows 11 Pro x64 MPI: Microsoft-MPI 10.1.1

Инструменты: Сборщик: CMake Компилятор: MSVC 19.x Конфигурация: Release

Переменные окружения: PPC_NUM_PROC=4 PPC_NUM_THREADS=1 Генерация данных: Тестовые наборы генерируются автоматически в тестах (случайные числа с фиксированным seed).

7. Результаты и обсуждение
7.1 Проверка корректности
Функциональные тесты проверяют:

Для MPI-версии: размер выходного буфера равен elements_per_process

Для последовательной версии при root = 0: проверяется корректность копирования данных

Для последовательной версии при root ≠ 0: проверяется, что буфер не пустой

Результаты тестирования:

Все 9 тестовых комбинаций пройдены успешно

Поддерживаются типы int, float, double

Корректная работа для разных значений root

Отсутствие ошибок сегментации и гонок данных

8. Заключение
В работе успешно реализован параллельный алгоритм scatter-распределения с использованием MPI:

Достигнутые результаты:

Реализован tree-based алгоритм scatter с использованием только MPI_Send и MPI_Recv

Обеспечена поддержка трех типов данных: int, float, double

Алгоритм корректно работает при любых значениях корневого процесса

Реализована последовательная версия для верификации результатов

Проведено комплексное тестирование на корректность и производительность

9. References
Стандарт MPI — https://www.mpi-forum.org/docs/
Microsoft MPI Documentation — https://learn.microsoft.com/en-us/message-passing-interface
cppreference.com — C++ reference