#Отчёт

## Scatter-распределение от одного ко всем (scatter)

Студент: Ашихмин Даниил
Группа: 3823Б1ФИ1

Технология: SEQ–MPI

------

## Введение

В данной работе реализуется параллельный алгоритм scatter-распределения - разбиение массива данных на части и рассылка их от одного процесса всем остальным - с использованием технологии MPI.

Цель работы - продемонстрировать корректное распределение данных между процессами с применением tree-based алгоритма, а также провести оценку корректности и производительности решения.

------

## Постановка задачи

Дан массив data длины N. Требуется разбить его на равные части по elements_per_process элементов и разослать эти части от выбранного процесса root всем остальным процессам, включая сам корневой процесс.

Входные данные

• Массив data длины
N = elements_per_process × world_size
произвольного типа (int, float, double)

• Целое число elements_per_process > 0
• Номер процесса root, с которого выполняется рассылка

Выходные данные

• У каждого процесса локальный буфер размера elements_per_process, содержащий соответствующую часть исходного массива

Требования

• Алгоритм должен корректно работать при любом числе процессов
• Рассылка должна выполняться по дереву с использованием только MPI_Send и MPI_Recv
• Поддержка трёх типов данных: MPI_INT, MPI_FLOAT, MPI_DOUBLE
• Последовательная версия должна корректно воспроизводить результат для сравнения

------

## Базовый алгоритм (последовательный)

Последовательная версия имитирует scatter-операцию:

• Проверяется, является ли процесс корневым (root == 0)
• Если процесс корневой, копируются первые elements_per_process элементов в локальный буфер
• Для некорневых процессов (или при root ≠ 0 в последовательной версии) локальный буфер заполняется нулями

------

## Схема распараллеливания (MPI)

Распределение данных

• Процесс root содержит исходный массив размера

elements_per_process × world_size

• Массив логически разбивается на world_size блоков по elements_per_process элементов
• Каждый процесс получает свой соответствующий блок

------

Схема взаимодействия процессов

Каждый процесс вычисляет виртуальный ранг:

virtual_rank = (rank - root + size) % size

На каждом шаге используется битовая маска mask.

• Если (virtual_rank & mask) == 0, процесс отправляет данные потомкам:
• dest_virtual = virtual_rank | mask
• dest_real = (dest_virtual + root) % size
Если процесс является корневым, он отправляет соответствующую часть исходного массива, иначе - свой локальный буфер.
• Иначе процесс получает данные от предка:
• src_virtual = virtual_rank & (~mask)
• src_real = (src_virtual + root) % size

После получения данных процесс может участвовать в дальнейшей рассылке.

------

Псевдокод

virtual_rank = (rank - root + size) % size
local_data = new vector[elements_per_process]

mask = 1
while mask < size:
  if (virtual_rank & mask) == 0:
    dest_virtual = virtual_rank | mask
    if dest_virtual < size:
      dest_real = (dest_virtual + root) % size
      if rank == root:
       offset = dest_virtual * elements_per_process
       send data[offset : offset + elements_per_process] to dest_real
      else:
       send local_data to dest_real
  else:
    src_virtual = virtual_rank & (~mask)
    src_real = (src_virtual + root) % size
    recv local_data from src_real
    break
  mask <<= 1

if rank == root:
  offset = virtual_rank * elements_per_process
  copy data[offset : offset + elements_per_process] to local_data

------

Топология

• Используется стандартный коммуникатор MPI_COMM_WORLD
• Бинарное дерево, построенное на основе виртуальных рангов

------

## Детали реализации

Структура кода

• common.hpp - структура ScatterParams и базовые типы
• ops_seq.hpp / ops_seq.cpp - последовательная реализация
• ops_mpi.hpp / ops_mpi.cpp - MPI-реализация (tree-based scatter)
• tests/functional/main.cpp - функциональные тесты
• tests/performance/main.cpp - тесты производительности

------

Особенности реализации

• Вспомогательные функции вынесены в анонимное пространство имён
• Использованы шаблоны для поддержки разных типов данных
• Алгоритм корректно обрабатывает любые значения root за счёт нормализации рангов
• Реализована проверка границ массива для предотвращения выхода за пределы

------

Основные функции

• GetMPIDataType<T>() — сопоставление C++ типов с MPI-типами
• VirtualToRealRank() — преобразование виртуального ранга в реальный
• SendBlock() — отправка блока данных
• ReceiveBlock() — приём блока данных

------

## Экспериментальная установка

Аппаратное обеспечение

• CPU: AMD Ryzen 5 3500X (3.6–4.1 GHz, 6 ядер / 6 потоков)
• RAM: 16 ГБ
• OS: Windows 11 Pro x64

Программное обеспечение

• MPI: Microsoft MPI 10.1.1
• Сборщик: CMake
• Компилятор: MSVC 19.x
• Конфигурация: Release

Переменные окружения

PPC_NUM_PROC = 4
PPC_NUM_THREADS = 1

Генерация данных

Тестовые наборы данных генерируются автоматически в тестах
(случайные значения с фиксированным seed).

------

## Результаты и обсуждение

Проверка корректности

Функциональные тесты проверяют:

• Для MPI-версии — размер выходного буфера равен elements_per_process
• Для последовательной версии при root = 0 — корректность копирования данных
• Для последовательной версии при root ≠ 0 — корректность инициализации локального буфера

------

Результаты тестирования

• Все 9 тестовых комбинаций успешно пройдены
• Поддерживаются типы: int, float, double
• Корректная работа при различных значениях root
• Отсутствуют ошибки сегментации и гонки данных

Ниже представлена таблица измерений времени выполнения для разных режимов:

| Mode | Count | Time (s)  | Speed up | Efficiency |
|------|-------|-----------|----------|------------|
| seq  | 1     | 0.003345  | 1        | 1          |
| mpi  | 2     | 0.004867  | 0.687    | 0.343      |
| mpi  | 4     | 0.005999  | 0.558    | 0.140      |


------

## Заключение

В работе успешно реализован параллельный алгоритм scatter-распределения с использованием MPI.

Достигнутые результаты:

• Реализован tree-based алгоритм scatter с использованием только MPI_Send и MPI_Recv
• Обеспечена поддержка трёх типов данных: int, float, double
• Алгоритм корректно работает при любых значениях корневого процесса
• Реализована последовательная версия для верификации результатов
• Проведено комплексное тестирование на корректность и производительность

------

## References

• MPI Standard — https://www.mpi-forum.org/docs/
• Microsoft MPI Documentation — https://learn.microsoft.com/en-us/message-passing-interface
• cppreference.com — https://en.cppreference.com/