# Многошаговая схема решения двумерных задач глобальной оптимизации. Распараллеливание по характеристикам.

- Студент: Круглова Алёна Витальевна, группа 3823Б1ФИ2
- Технология: SEQ | MPI 
- Вариант: 13

## 1. Введение
Задачи глобальной оптимизации функций нескольких переменных широко применяются в вычислительной математике и прикладных задачах. Их особенностью является необходимость поиска абсолютного минимума целевой функции на заданной области, что затруднено при наличии множества локальных экстремумов.

Одним из подходов к решению таких задач являются многошаговые методы, основанные на выборе интервалов с наибольшей характеристикой. В двумерном случае задача сводится к последовательному поиску минимума по одной переменной с решением одномерных подзадач по другой переменной, для чего используется алгоритм Стронгина.

С целью ускорения вычислений применяется распараллеливание по характеристикам интервалов. На каждом шаге несколько наиболее перспективных интервалов обрабатываются независимо различными процессами, а полученные результаты объединяются в общий набор данных. Это позволяет эффективно использовать параллельные вычислительные ресурсы.

В данной лабораторной работе реализованы последовательная и параллельная версии алгоритма двумерной глобальной оптимизации с использованием библиотеки MPI, а также выполнено их сравнение по корректности и производительности.

## 2. Постановка задачи
**Формальная постановка:** 
Рассматривается задача глобальной оптимизации вещественной функции двух переменных

f(x, y),  (x, y) ∈ D,

где область допустимых значений D задаётся прямоугольником

x ∈ [x_min, x_max],  
y ∈ [y_min, y_max].

Требуется найти точку глобального минимума (x*, y*) и соответствующее минимальное значение функции f(x*, y*) с заданной точностью ε с использованием многошагового алгоритма глобальной оптимизации.

В рамках данной работы в качестве целевой функции используется тестовая негладкая многомодальная функция, обладающая несколькими локальными минимумами, что позволяет оценить корректность и эффективность реализованного алгоритма.

**Входные данные:** 
Входные данные задаются структурой `InType` и включают следующие параметры:

- `x_min` — нижняя граница области поиска по переменной x  
- `x_max` — верхняя граница области поиска по переменной x  
- `y_min` — нижняя граница области поиска по переменной y  
- `y_max` — верхняя граница области поиска по переменной y  
- `eps` — требуемая точность поиска  
- `max_iters` — максимальное количество итераций алгоритма  

**Выходные данные:** 
Результат работы алгоритма возвращается в структуре `OutType`:

- `x` — значение координаты x, в которой достигается минимум функции  
- `y` — значение координаты y, в которой достигается минимум функции  
- `f_value` — минимальное значение целевой функции 

**Ограничения:**
- Должны выполняться условия: `x_max > x_min`, `y_max > y_min`
- Параметр точности `eps` должен быть положительным
- Максимальное количество итераций `max_iters` должно быть больше нуля
- Алгоритм должен корректно работать для произвольных допустимых границ области поиска
- Параллельная MPI-версия должна поддерживать произвольное количество процессов и реализовывать распараллеливание по характеристикам интервалов, распределяя вычисление новых точек между процессами

## 3. Последовательная версия(Baseline)
Последовательная версия реализует многошаговый алгоритм двумерной глобальной оптимизации без использования параллельных вычислений. Алгоритм основан на декомпозиции исходной задачи на последовательность одномерных подзадач и использовании метода Стронгина для поиска минимума.

На начальном этапе на отрезке [x_min, x_max] формируется начальное множество точек. Для каждой точки x последовательно решается одномерная задача минимизации по переменной y, в результате чего вычисляется значение целевой функции f(x, y). Полученные точки сортируются по возрастанию координаты x и образуют начальный набор испытаний.

На каждой итерации алгоритма:
- вычисляется оценка константы Липшица на основе текущего набора точек;
- для всех соседних интервалов по переменной x рассчитываются их характеристики;
- выбирается интервал с максимальной характеристикой;
- внутри выбранного интервала вычисляется новая точка x;
- для найденного значения x последовательно решается одномерная задача оптимизации по y;
- новая точка добавляется в общее множество испытаний.

Итерационный процесс продолжается до достижения заданной точности ε либо до исчерпания максимального числа итераций. После завершения алгоритма в качестве результата выбирается точка с минимальным значением целевой функции.

Для иллюстрации одного шага алгоритма ниже приведён фрагмент кода, демонстрирующий вычисление новой точки по переменной x и последующее решение одномерной подзадачи оптимизации по переменной y:

```cpp
double x_new = (0.5 * (x_right + x_left)) -
               ((z_right - z_left) / (2.0 * m_scaled));

double y_new = 0.0;
double z_new = Solve1DStrongin(
    [&](double y) { return ObjectiveFunction(x_new, y); },
    y_min, y_max, eps, max_iters, y_new);
```

### Вычислительная сложность

Временная сложность последовательного алгоритма определяется количеством итераций внешнего цикла и числом вычислений одномерных задач оптимизации. В общем виде сложность можно оценить как:

O(K · N₁ · N₂),

где K — число итераций двумерного алгоритма,  
N₁ — количество испытаний по переменной x,  
N₂ — количество итераций одномерного алгоритма Стронгина по переменной y.

### Использование памяти

Алгоритм использует дополнительную память для хранения текущего набора испытаний, размер которого линейно растёт с числом итераций. Дополнительное потребление памяти оценивается как O(N), где N — количество точек в множестве испытаний.

## 4. Параллельная версия

Параллельная версия алгоритма реализует многошаговую схему двумерной глобальной оптимизации с использованием библиотеки MPI. Распараллеливание осуществляется по характеристикам интервалов, что позволяет одновременно вычислять несколько новых испытательных точек на каждом шаге алгоритма.

### 4.1. Разделение данных и вычислений

Параллелизм реализуется на уровне внешнего одномерного поиска по переменной x. На каждой итерации алгоритма корневой процесс формирует набор наиболее перспективных интервалов по координате x на основе их характеристик. Количество таких интервалов соответствует числу процессов.

Каждому процессу передаётся один интервал вида  
[x₁, x₂] с известными значениями целевой функции f(x₁) и f(x₂).  
Внутри полученного интервала процесс независимо выполняет:
- вычисление новой точки x;
- решение одномерной задачи глобальной оптимизации по переменной y;
- вычисление значения целевой функции f(x, y).

Таким образом, каждый процесс выполняет независимый вычислительный шаг, что позволяет эффективно распараллелить наиболее затратную часть алгоритма.

### 4.2. Взаимодействие процессов

Взаимодействие между процессами организовано по схеме «master–worker»:

1. **Инициализация**  
   Все процессы получают свой `rank` и общее количество процессов `size` с помощью `MPI_Comm_rank` и `MPI_Comm_size`.

2. **Начальная инициализация (процесс 0)**  
   Корневой процесс формирует начальное множество точек по переменной x и для каждой из них последовательно решает одномерную задачу оптимизации по y.

3. **Выбор интервалов**  
   На каждой итерации процесс 0:
   - вычисляет характеристики всех текущих интервалов;
   - выбирает `size` интервалов с наибольшими характеристиками;
   - формирует массив интервалов для распределения между процессами.

4. **Распределение интервалов**  
   Выбранные интервалы передаются процессам с использованием `MPI_Scatter`.  
   Каждый процесс получает один интервал для обработки.

5. **Локальные вычисления**  
   Каждый процесс независимо:
   - вычисляет новую точку x внутри своего интервала;
   - решает одномерную задачу оптимизации по y;
   - формирует локальный результат (x, y, f).

6. **Сбор результатов**  
   Локальные результаты передаются на корневой процесс с помощью `MPI_Gather`.

7. **Обновление множества точек**  
   Процесс 0 добавляет полученные точки в общее множество испытаний и проверяет условие остановки.

8. **Завершение**  
   После завершения итерационного процесса найденный минимум рассылается всем процессам с использованием `MPI_Bcast`.


### 4.3. Иллюстрация параллельного шага

Ниже приведён фрагмент кода, демонстрирующий вычисление новой точки внутри интервала и последующее решение одномерной подзадачи оптимизации на каждом процессе:

```cpp
double x_new = (0.5 * (my_interval.x1 + my_interval.x2)) -
               ((my_interval.f2 - my_interval.f1) / (2.0 * m_local));

double y_res = 0.0;
double f_res = Solve1DStrongin(
    [&](double y) { return ObjectiveFunction(x_new, y); },
    in.y_min, in.y_max, in.eps,
    std::max(25, in.max_iters / 20), y_res);
```

### 4.3. Псевдокод алгоритма
```pseudocode
MPI_Init()
Получить rank и size

Если rank == 0:
    сформировать начальное множество точек

Для iter = 1 .. max_iters:
    Если rank == 0:
        выбрать size интервалов с максимальной характеристикой
    MPI_Bcast(stop_flag)

    Если stop_flag:
        выйти из цикла

    MPI_Scatter(интервалы → локальный интервал)

    вычислить новую точку x
    решить одномерную задачу по y
    получить локальный результат (x, y, f)

    MPI_Gather(локальные результаты → процесс 0)

    Если rank == 0:
        обновить множество точек

Если rank == 0:
    выбрать глобальный минимум

MPI_Bcast(результат)
MPI_Finalize()
```

## 5. Детали реализации

### 5.1. Файловая структура проекта
kruglova_a_2d_multistep_par_opt/  
├── common/include/common.hpp  
├── seq/include/ops_seq.hpp  
├── seq/src/ops_seq.cpp  
├── mpi/include/ops_mpi.hpp  
├── mpi/src/ops_mpi.cpp  
├── tests/functional/main.cpp  
├── tests/performance/main.cpp  
└── data/  

### 5.2. Ключевые классы и функции

- **InType** — структура входных данных задачи двумерной глобальной оптимизации, содержащая:
  - границы области поиска по x и y (`x_min`, `x_max`, `y_min`, `y_max`);
  - параметр точности `eps`;
  - максимальное число итераций `max_iters`.

- **OutType** — структура результата вычислений, включающая:
  - координаты найденной точки минимума `(x*, y*)`;
  - значение целевой функции `f(x*, y*)`.

- **`RunImpl()`** — основная реализация параллельного алгоритма.  
  Функция организует многошаговый процесс оптимизации, в котором:
  - корневой процесс выбирает наиболее перспективные интервалы по переменной x;
  - интервалы распределяются между процессами с использованием MPI;
  - каждый процесс независимо вычисляет новую точку и решает одномерную подзадачу оптимизации по y;
  - результаты собираются и используются для обновления множества испытательных точек.

- **`ValidationImpl()`** — проверка корректности входных параметров задачи:
  - корректность границ области поиска;
  - положительность параметра точности `eps`;
  - корректность максимального числа итераций.

- **`PreProcessingImpl()`** — начальная инициализация выходной структуры и подготовка данных перед запуском основного алгоритма.

- **`PostProcessingImpl()`** — завершающий этап вычислений; дополнительная обработка результатов не требуется.

- **Вспомогательные функции и структуры:**
  - **`Solve1DStrongin()`** — реализация одномерного глобального поиска минимума по алгоритму Стронгина;
  - **`CalculateM1D()`** — вычисление оценки липшицевой константы по текущим испытательным точкам;
  - **`MasterCalculateIntervals()`** — выбор интервалов с наибольшими характеристиками для последующего распараллеливания;
  - **`Trial1D`, `Trial2D`** — структуры для хранения одномерных и двумерных испытательных точек;
  - **`IntervalData`** — структура, описывающая интервал по переменной x и значения функции на его границах.

- **Тестовые классы:**
  - **`KruglovaA2DMuitSEQ`** — последовательная версия алгоритма двумерной оптимизации;
  - **`KruglovaA2DMuitMPI`** — параллельная версия с использованием MPI, реализующая распределение интервалов между процессами.

### 5.3. Использование памяти

**В последовательной версии (SEQ):**
- В памяти хранится весь набор пробных точек `Trial2D` для координаты `x` и соответствующие значения `y` и `f`.
- Хранится входная информация `InType` и результирующий объект `OutType`.

**В параллельной версии (MPI):**
- Процесс 0 хранит:
  - Полный массив пробных точек `Trial2D`.
  - Вспомогательные массивы выбранных интервалов `IntervalData` для рассылки на другие процессы.
- Остальные процессы хранят:
  - Локальный интервал `IntervalData`, полученный через `MPI_Scatter`.
  - Локальные результаты `x_new`, `y_res`, `f_res` для передачи на процесс 0 через `MPI_Gather`.

**Особенности организации памяти:**
- Каждому процессу достаточно хранить только свои локальные данные, что снижает объём памяти и обеспечивает масштабируемость при увеличении числа MPI-процессов.
- На процессе 0 аккумулируются и упорядочиваются новые пробные точки, что позволяет корректно формировать глобальное множество испытаний для следующей итерации.
- Использование простых структур (`Trial1D`, `Trial2D`, `IntervalData`) упрощает сериализацию для MPI и снижает накладные расходы на управление памятью.

Таким образом, память распределена эффективно: процессы работают с минимально необходимым набором данных, а процесс-координатор хранит полное состояние задачи для управления итерациями и окончательного выбора оптимальной точки.

## 6. Экспериментальное окружение
Экспериментальные исследования проводились на вычислительной системе с процессором **AMD Ryzen 5 5500U**, оснащённым **6** вычислительными ядрами с поддержкой одновременной многопоточности. Аппаратная конфигурация включает **16 ГБ** оперативной памяти **DDR4** и твердотельный накопитель объёмом **512 ГБ**, функционирующий под управлением операционной системы **Windows 10**.

Разработка и компиляция программного кода выполнялись в среде **Microsoft Visual Studio 2019** с использованием компилятора C++. Для реализации параллельных вычислений применялась библиотека **Microsoft MPI** версии **10.0.12498.5**, а сборка проекта осуществлялась с помощью системы **CMake версии 3.30.4** в режиме **Release**.

Рассматривалась задача глобальной оптимизации функции Растригина в 2D. Проверялись **последовательная (`KruglovaA2DMuitSEQ`) и параллельная MPI-версии (`KruglovaA2DMuitMPI`)**. Тесты выполнялись для 1, 2, 4 и 6 MPI-процессов.  

Корректность проверялась сравнением значений функции в найденных экстремумах, производительность — усреднением времени многократных запусков. Эксперименты оценивали **точность, масштабируемость и эффективность параллельного алгоритма**.

## 7. Результаты

### 7.1 Корректность
 Корректность параллельной реализации подтверждена с использованием фреймворка Google Test. Результаты, полученные MPI-версией алгоритма многошаговой 2D оптимизации (`KruglovaA2DMuitMPI`), сравнивались с эталонной последовательной реализацией (`KruglovaA2DMuitSEQ`).

### 7.2 Производительность
Измерения проводились для функции Растригина на области `[-5.12, 5.12] × [-5.12, 5.12]` с параметрами:
- ε = `10⁻⁶`
- max_iters = `2000`


| Mode        | Count | Time, s | Speedup | Efficiency |
|-------------|-------|---------|---------|------------|
| seq         | 1     | 4.94    | 1.00    | N/A        |
| mpi         | 2     | 3.12    | 1.58    | 79.0%      |
| mpi         | 4     | 1.89    | 2.61    | 65.3%      |
| mpi         | 6     | 1.42    | 3.48    | 58.0%      |

**Анализ результатов:**

**Общая характеристика:**
- Сублинейное ускорение с ростом числа процессов
- Эффективность снижается: `79%` → `65%` → `58%`
- Наибольший выигрыш: `1` → `2` процесса (`-37%` времени)


## 8. Заключение
В результате выполнения работы был успешно разработан и реализован параллельный алгоритм многошаговой глобальной оптимизации для функции двух переменных. Основной задачей являлось создание эффективного алгоритма поиска глобального минимума функции Растригина на заданной области с использованием технологии MPI для параллельных вычислений.

Ключевым достижением работы стала разработка адаптивного алгоритма на основе метода Стронгина, который продемонстрировал высокую эффективность при решении задачи глобальной оптимизации. Алгоритм успешно находит минимум функции с заданной точностью, что подтверждено комплексным тестированием на различных конфигурациях входных данных. Параллельная реализация с использованием MPI показала способность ускорять вычисления, обеспечивая ускорение до 3.48 раз при использовании 6 процессов.

Тестирование подтвердило корректность работы обеих версий алгоритма (последовательной и параллельной), которые демонстрируют идентичные результаты во всех тестовых случаях. Анализ производительности выявил, что наилучшая эффективность (79%) достигается при использовании 2 процессов, при дальнейшем увеличении числа процессов эффективность снижается из-за роста коммуникационных накладных расходов.

Разработанный алгоритм обладает практической ценностью и может применяться для решения широкого класса задач оптимизации мультимодальных функций. Его основные преимущества включают гарантированную сходимость, адаптивность к характеристикам функции и возможность распараллеливания вычислений.

## 9. Источники
1. Microsoft MPI : документация [Электронный ресурс] // Microsoft Learn. – URL: https://learn.microsoft.com/ru-ru/message-passing-interface/microsoft-mpi (дата обращения: 12.11.2025).
2. Сысоев А. В. Курс лекций по параллельному программированию
3. Нестеров А.Ю и Оболенский А.А Практические занятия

## 9. Приложение
Основной алгоритм MPI
```cpp
bool KruglovaA2DMuitMPI::RunImpl() {
    int rank = 0;
    int size = 0;
    MPI_Comm_rank(MPI_COMM_WORLD, &rank);
    MPI_Comm_size(MPI_COMM_WORLD, &size);

    const auto& in = GetInput();
    std::vector<Trial2D> trials;

    if (rank == 0) {
        const int init_points = 20;
        for (int i = 0; i < init_points; ++i) {
            double x = in.x_min + (in.x_max - in.x_min) 
                     * static_cast<double>(i) 
                     / static_cast<double>(init_points - 1);
            double y_best = 0.0;
            double f = Solve1DStrongin(
                [&](double y) { return ObjectiveFunction(x, y); },
                in.y_min, in.y_max, in.eps,
                std::max(20, in.max_iters / 20), y_best);
            trials.push_back({.x = x, .y = y_best, .f = f});
        }
        std::sort(trials.begin(), trials.end(),
            [](const Trial2D& a, const Trial2D& b) { return a.x < b.x; });
    }

    for (int iter = 0; iter < in.max_iters; ++iter) {
        int stop_flag = 0;
        std::vector<IntervalData> selected_intervals(static_cast<size_t>(size));

        if (rank == 0) {
            MasterCalculateIntervals(trials, selected_intervals, 
                                    size, in.eps, stop_flag);
        }

        MPI_Bcast(&stop_flag, 1, MPI_INT, 0, MPI_COMM_WORLD);
        if (stop_flag != 0) {
            break;
        }

        IntervalData my_interval{};
        MPI_Scatter(selected_intervals.data(), sizeof(IntervalData), MPI_BYTE,
                   &my_interval, sizeof(IntervalData), MPI_BYTE,
                   0, MPI_COMM_WORLD);

        double dx_local = my_interval.x2 - my_interval.x1;
        double m_local = (std::abs(my_interval.f2 - my_interval.f1) / dx_local);
        m_local = (m_local > 0.0) ? (2.0 * m_local) : 1.0;

        double x_new = (0.5 * (my_interval.x1 + my_interval.x2))
                     - ((my_interval.f2 - my_interval.f1) / (2.0 * m_local));
        
        double y_res = 0.0;
        double f_res = Solve1DStrongin(
            [&](double y) { return ObjectiveFunction(x_new, y); },
            in.y_min, in.y_max, in.eps,
            std::max(25, in.max_iters / 20), y_res);

        std::array<double, 3> send_res = {x_new, y_res, f_res};
        std::vector<double> recv_res(static_cast<size_t>(size) * 3);
        MPI_Gather(send_res.data(), 3, MPI_DOUBLE,
                  recv_res.data(), 3, MPI_DOUBLE,
                  0, MPI_COMM_WORLD);

        if (rank == 0) {
            for (int i = 0; i < size; ++i) {
                size_t base = static_cast<size_t>(i) * 3;
                Trial2D res{
                    .x = recv_res[base],
                    .y = recv_res[base + 1],
                    .f = recv_res[base + 2]
                };
                auto it = std::lower_bound(trials.begin(), trials.end(), res,
                    [](const Trial2D& a, const Trial2D& b) { return a.x < b.x; });
                if (it == trials.end() || std::abs(it->x - res.x) > 1e-12) {
                    trials.insert(it, res);
                }
            }
        }
    }

    std::array<double, 3> final_res = {0.0, 0.0, 0.0};
    if (rank == 0) {
        auto best_it = std::min_element(trials.begin(), trials.end(),
            [](const Trial2D& a, const Trial2D& b) { return a.f < b.f; });
        final_res = {best_it->x, best_it->y, best_it->f};
    }

    MPI_Bcast(final_res.data(), 3, MPI_DOUBLE, 0, MPI_COMM_WORLD);
    GetOutput() = {final_res[0], final_res[1], final_res[2]};
    return true;
}
  
```