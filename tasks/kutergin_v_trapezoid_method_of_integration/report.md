# Отчет по лабораторной работе №1
## "Интегрирование методом трапеций"

**Студент:** Кутергин Валентин
**Группа:** 3823Б1ФИ3
**Вариант:** 20

### 1. Введение
**Мотивация:** Численное интегрирование — фундаментальная задача во многих научных и инженерных областях. Цель данной работы — реализовать параллельную версию алгоритма интегрирования методом трапеций с использованием технологии MPI и исследовать эффективность распараллеливания по сравнению с последовательным вариантом.

**Проблема:** Метод трапеций, как и многие численные методы, хорошо поддается декомпозиции. Однако, каждая отдельная итерация (вычисление площади одной трапеции) имеет очень низкую вычислительную сложность. Это создает риск, что накладные расходы на организацию параллельного взаимодействия (коммуникации) могут превысить выгоду от параллельных вычислений.

**Ожидаемый результат:** Ожидается, что MPI-версия покажет ускорение только на задачах с очень большим количеством итераций (`n`), где время вычислений станет сопоставимым с затратами на коммуникацию. Для задач с низкой или средней вычислительной нагрузкой параллельная версия, скорее всего, будет уступать последовательной.

### 2. Постановка задачи
**Задано:** Функция `f(x)`, отрезок интегрирования `[a, b]` и количество шагов разбиения `n`.

**Требуется:** Численно вычислить определенный интеграл функции `f(x)` на заданном отрезке.

**Входные данные:** `double a`, `double b`, `int n`.
**Выходные данные:** `double result` — приближенное значение интеграла.

### 3. Базовый алгоритм (Последовательный)
Последовательный алгоритм реализует формулу метода трапеций:
`Интеграл ≈ h * [ (f(a) + f(b))/2 + f(x_1) + f(x_2) + ... + f(x_{n-1}) ]`, где `h = (b-a)/n`.

Алгоритм состоит из одного цикла, который итеративно вычисляет и суммирует значения функции в точках разбиения. Сложность алгоритма — `O(n)`.

### 4. Схема распараллеливания
**Декомпозиция данных:**
Основная вычислительная нагрузка — это суммирование `n` значений функции. Эта сумма была разделена между `P` MPI-процессами. Используется стратегия **блочного распределения**: общее число итераций `n` делится на `P`.

**Обработка остатка:**
Чтобы алгоритм работал для любых `n` и `P`, реализована корректная обработка остатка от деления. Первые `n % P` процессов получают на одну итерацию больше, чем остальные. Это обеспечивает максимально равномерное распределение нагрузки.

**Вычисления на процессе:**
Каждый процесс независимо вычисляет свою **локальную сумму** (`local_sum`) на своем уникальном подотрезке.

**Коммуникация:**
*   В начале работы (этап `PreProcessing`) корневой процесс (`rank 0`) рассылает всем остальным исходные данные (`a`, `b`, `n`) с помощью коллективной операции **`MPI_Bcast`**.
*   После завершения локальных вычислений все процессы участвуют в коллективной операции **`MPI_Reduce`** с операцией `MPI_SUM`. Все `local_sum` суммируются, и итоговый результат (`global_sum`) становится доступен только на корневом процессе.

### 5. Экспериментальная установка
*   **Hardware/OS:** AMD Ryzen 5 7500F, 6 ядер, 32GB RAM, Windows 10
*   **Toolchain & Environment:**
    *   Работа производилась внутри **Dev Container** (Ubuntu)
    *   Компилятор: **GCC 14.2.0**
    *   MPI-библиотека: **Open MPI 3.1**
    *   Тип сборки: **Release**
*   **Data:** Для замеров производительности использовалась функция `f(x) = x*x` на отрезке `[0.0, 10.0]` с `n = 25,200,000`.

### 6. Результаты и обсуждение
#### 6.1 Корректность
Корректность обеих реализаций (`seq` и `mpi`) проверена набором функциональных тестов, включая:
*   Тесты с фиксированными эталонными значениями.
*   Тесты со случайно сгенерированными границами интегрирования и числом шагов `n`.
Все тесты успешно пройдены, что подтверждает корректность вычислений.

#### 6.2 Производительность
Замеры производительности проводились на системе, указанной в п.5. В таблице представлено время выполнения только вычислительной части (`RunImpl`).

| Mode | Count | Time, s | Speedup | Efficiency |
| :--- | :--- | :--- | :--- | :--- |
| seq | 1 | 0.0000289 | 1.00 | N/A |
| mpi | 4 | 0.0002408 | **0.12** | **3.0%** |

*   **Speedup** = `Time(seq) / Time(mpi)`
*   **Efficiency** = `Speedup / Count`

Как видно из результатов, параллельная реализация на 4-х процессах оказалась значительно (приблизительно в 8.3 раза) **медленнее** последовательной. Ускорение составило всего 0.12, что говорит о сильной деградации производительности.

### 7. Выводы
Реализация параллельного алгоритма интегрирования методом трапеций с использованием MPI продемонстрировала свою **неэффективность для данной задачи**.

Основная причина — **крайне низкая вычислительная интенсивность** операции. Время, затрачиваемое на вычисление `x*x`, ничтожно мало по сравнению с **накладными расходами на MPI-коммуникации**. Операции `MPI_Bcast` и `MPI_Reduce` требуют упаковки данных, пересылки и синхронизации, что в данном случае "съедает" весь потенциальный выигрыш от параллельных вычислений и приводит к существенному замедлению.

ДЭффективность MPI достигается только тогда, когда время, затраченное на локальные вычисления, **значительно превышает** время на коммуникацию между процессами.

### 8. Приложение: Исходный код MPI-реализации (`RunImpl`)
```cpp
bool TrapezoidIntegrationMPI::RunImpl() {
  int process_rank = 0;
  int process_count = 0;
  MPI_Comm_rank(MPI_COMM_WORLD, &process_rank);
  MPI_Comm_size(MPI_COMM_WORLD, &process_count);

  double a = GetInput().a;
  double b = GetInput().b;
  int n = GetInput().n;
  double h = (b - a) / n;

  const int base_n = n / process_count;     // целое часть от деления числа разбиений на число процессов
  const int remainder = n % process_count;  // остаток от деления числа разбиений на число процессов

  const int local_n = base_n + (process_rank < remainder ? 1 : 0);  // количество разбиений (трапеций) на один процесс

  int start_index = 0;
  if (process_rank < remainder) {
    start_index = process_rank * (base_n + 1);
  } else {
    start_index = (remainder * (base_n + 1)) + ((process_rank - remainder) * base_n);
  }

  double local_a = a + (start_index * h);  // начало отрезка для текущего процесса

  // локальные вычисления
  double local_sum = 0.0;
  if (local_n > 0) {
    local_sum = (Func(local_a) + Func(local_a + (local_n * h))) / 2.0;
  }

  for (int i = 1; i < local_n; ++i) {
    local_sum += Func(local_a + (i * h));
  }

  // агрегация
  double global_sum = 0.0;
  MPI_Reduce(&local_sum, &global_sum, 1, MPI_DOUBLE, MPI_SUM, 0, MPI_COMM_WORLD);

  if (process_rank == 0) {
    GetOutput() = global_sum * h;
  }

  return true;
}