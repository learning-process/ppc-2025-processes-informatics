# Отчет по лабораторной работе №3
## "Решение СЛАУ методом сопряженных градиентов"

**Студент:** Шехирев Владислав Эдуардович
**Группа:** 3823Б1ФИ2
**Вариант:** 6

### 1. Введение
**Мотивация:** Решение систем линейных алгебраических уравнений (СЛАУ) вида $Ax = b$ — одна из самых частых задач в численном моделировании. Метод сопряженных градиентов (CG) является эффективным итерационным методом для решения систем с симметричными положительно определенными матрицами. Распараллеливание этого метода необходимо для решения систем огромной размерности, возникающих в реальных инженерных задачах.

**Проблема:** Итерационные методы требуют синхронизации на каждом шаге. В методе CG необходимо вычислять скалярные произведения (требует глобальной редукции) и умножать матрицу на вектор. Для умножения строки матрицы на вектор направления $p$ процессу требуется знать **весь** вектор $p$, который распределен между процессами. Сбор этого вектора (`MPI_Allgatherv`) создает значительные коммуникационные накладные расходы.

**Ожидаемый результат:** Эффективность параллельной реализации зависит от соотношения объема вычислений ($O(N^2/P)$) к объему передаваемых данных ($O(N)$). Ожидается, что на матрицах среднего размера накладные расходы на пересылку векторов будут существенными, что может привести к отсутствию ускорения по сравнению с последовательной версией, выполняемой в оперативной памяти одного узла.

### 2. Постановка задачи
**Задано:** Симметричная положительно определенная матрица $A$ размера $N \times N$ и вектор $b$ размера $N$.

**Требуется:** Найти вектор $x$, такой что $Ax = b$, с заданной точностью $\epsilon$, используя метод сопряженных градиентов.

### 3. Алгоритм (Последовательный)
Реализован стандартный алгоритм CG:
1. Инициализация: $r_0 = b - Ax_0$, $p_0 = r_0$.
2. Итерационный процесс:
   - $\alpha_k = (r_k, r_k) / (p_k, A p_k)$
   - $x_{k+1} = x_k + \alpha_k p_k$
   - $r_{k+1} = r_k - \alpha_k A p_k$
   - Проверка сходимости: если $||r_{k+1}|| < \epsilon$, выход.
   - $\beta_k = (r_{k+1}, r_{k+1}) / (r_k, r_k)$
   - $p_{k+1} = r_{k+1} + \beta_k p_k$

### 4. Схема распараллеливания
**Декомпозиция:** Используется блочное разбиение матрицы $A$ по строкам (1D row-wise decomposition). Векторы $x, b, r$ также распределены блоками между процессами.

**Коммуникации:**
1.  **Матрично-векторное умножение ($Ap$):** Каждый процесс владеет полосой матрицы $A$. Для умножения строки на вектор $p$ необходимо иметь доступ ко всем элементам $p$. Поэтому на каждой итерации выполняется `MPI_Allgatherv` для сбора полного вектора $p$ на каждом процессе.
2.  **Скалярные произведения:** Для вычисления $\alpha$ и $\beta$ требуются глобальные скалярные произведения. Локальные частичные суммы объединяются с помощью `MPI_Allreduce` с операцией `MPI_SUM`.

### 5. Experimental Setup
*   **Hardware:** Контейнеризированная среда (Docker).
*   **Toolchain:** GCC, OpenMPI.
*   **Data:** Диагональная SPD матрица размером $N = 10000$ (размер данных ~800 МБ).

### 6. Результаты
#### 6.1 Корректность
Корректность алгоритма проверена функциональными тестами на системах с известным аналитическим решением (единичная матрица, диагональная матрица, малая плотная система 2x2). Результаты параллельной версии совпадают с эталонными с точностью до $10^{-4}$.

#### 6.2 Производительность
Замеры времени (`task_run`) выполнения задачи для $N=10000$:

| Mode | Count | Time, s | Speedup | Efficiency |
| :--- | :--- | :--- | :--- | :--- |
| seq | 1 | 0.15508 | 1.00 | 100% |
| mpi | 2 | 0.74053 | 0.21 | 10.5% |
| mpi | 4 | 0.65708 | 0.24 | 6.0% |

*   **Speedup** = `Time(seq) / Time(mpi)`
*   **Efficiency** = `Speedup / Count`

### 7. Выводы
В ходе эксперимента на матрице размера $10,000 \times 10,000$ получены следующие результаты:

1.  **Влияние коммуникаций:** Параллельная MPI-версия работает медленнее последовательной. Это объясняется высокими накладными расходами на передачу данных. На каждой итерации алгоритма необходимо собрать полный вектор направления $p$ (размером 80 КБ) на каждом из процессоров с помощью `MPI_Allgatherv`. В условиях общей памяти (SMP), где работает Docker, пересылка данных через MPI (копирование памяти) оказывается дороже, чем прямой доступ к памяти в последовательной программе.
2.  **Положительная динамика масштабируемости:** Несмотря на проигрыш последовательной версии, наблюдается сокращение времени выполнения при увеличении числа процессов с 2 до 4 (с 0.74с до 0.65с). Это свидетельствует о том, что распараллеливание вычислений (матричного умножения) начинает приносить плоды и частично компенсировать коммуникационные издержки.
3.  **Заключение:** Для эффективного применения MPI к методу сопряженных градиентов на плотных матрицах требуются задачи значительно большей размерности ($N \gg 10000$), либо распределенная система с большим количеством узлов, где объем оперативной памяти одного узла является ограничивающим фактором. Для текущей задачи ($N=10000$) последовательная версия остается наиболее эффективной.