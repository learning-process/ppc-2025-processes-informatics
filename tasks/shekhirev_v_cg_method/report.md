# Отчет по лабораторной работе №24
## "Решение СЛАУ методом сопряженных градиентов"

**Студент:** Шехирев Владислав Эдуардович
**Группа:** 3823Б1ФИ2
**Вариант:** 24

### 1. Введение
**Мотивация:** Решение систем линейных алгебраических уравнений (СЛАУ) вида $Ax = b$ — одна из самых частых задач в численном моделировании. Метод сопряженных градиентов (CG) является эффективным итерационным методом для решения систем с симметричными положительно определенными матрицами. Распараллеливание этого метода необходимо для решения систем огромной размерности, возникающих в реальных инженерных задачах.

**Проблема:** Итерационные методы требуют синхронизации на каждом шаге. В методе CG необходимо вычислять скалярные произведения (требует глобальной редукции) и умножать матрицу на вектор (требует обмена вектором направления $p$). Если размерность системы невелика, время на синхронизацию процессов превышает время полезных вычислений.

**Ожидаемый результат:** Для тестовой матрицы малого размера ($N=100$) ожидается замедление параллельной версии по сравнению с последовательной, так как накладные расходы на инициализацию MPI и коллективные операции (`MPI_Allgatherv`, `MPI_Allreduce`) будут доминировать.

### 2. Постановка задачи
**Задано:** Симметричная положительно определенная матрица $A$ размера $N \times N$ и вектор $b$ размера $N$.

**Требуется:** Найти вектор $x$, такой что $Ax = b$, с заданной точностью $\epsilon$, используя метод сопряженных градиентов.

### 3. Алгоритм (Последовательный)
Реализован стандартный алгоритм CG:
1. Инициализация: $r_0 = b - Ax_0$, $p_0 = r_0$.
2. Итерационный процесс:
   - $\alpha_k = (r_k, r_k) / (p_k, A p_k)$
   - $x_{k+1} = x_k + \alpha_k p_k$
   - $r_{k+1} = r_k - \alpha_k A p_k$
   - Проверка сходимости: если $||r_{k+1}|| < \epsilon$, выход.
   - $\beta_k = (r_{k+1}, r_{k+1}) / (r_k, r_k)$
   - $p_{k+1} = r_{k+1} + \beta_k p_k$

### 4. Схема распараллеливания
**Декомпозиция:** Используется блочное разбиение матрицы $A$ по строкам (1D row-wise decomposition). Векторы $x, b, r$ также распределены блоками между процессами.

**Коммуникации:**
1.  **Матрично-векторное умножение ($Ap$):** Каждый процесс владеет полосой матрицы $A$. Для умножения строки на вектор $p$ необходимо иметь доступ ко всем элементам $p$. Поэтому на каждой итерации выполняется `MPI_Allgatherv` для сбора полного вектора $p$ на каждом процессе.
2.  **Скалярные произведения:** Для вычисления $\alpha$ и $\beta$ требуются глобальные скалярные произведения. Локальные частичные суммы объединяются с помощью `MPI_Allreduce` с операцией `MPI_SUM`.

### 5. Experimental Setup
*   **Hardware:** Контейнеризированная среда (Docker).
*   **Toolchain:** GCC, OpenMPI.
*   **Data:** Диагональная SPD матрица размером $N = 100$.

### 6. Результаты
#### 6.1 Корректность
Корректность алгоритма проверена функциональными тестами на системах с известным аналитическим решением (единичная матрица, диагональная матрица, малая плотная система 2x2). Результаты параллельной версии совпадают с эталонными с точностью до $10^{-4}$.

#### 6.2 Производительность
Замеры времени (`task_run`) выполнения задачи для $N=100$:

| Mode | Count | Time, s | Speedup | Efficiency |
| :--- | :--- | :--- | :--- | :--- |
| seq | 1 | 0.0000062 | 1.00 | 100% |
| mpi | 2 | 0.0000215 | 0.29 | 14.5% |
| mpi | 4 | 0.0000497 | 0.12 | 3.0% |

*   **Speedup** = `Time(seq) / Time(mpi)`
*   **Efficiency** = `Speedup / Count`

### 7. Выводы
В ходе эксперимента получены следующие результаты:

1.  **Замедление на малых данных:** Параллельная версия работает значительно медленнее последовательной (в ~3.5 раза на 2 процессах и в ~8 раз на 4 процессах).
2.  **Причина:** Размер задачи ($N=100$) слишком мал.
    *   Последовательное решение занимает всего **6.2 микросекунды**.
    *   Латентность одной пары операций `MPI_Allgatherv` + `MPI_Allreduce`, выполняемых на *каждой* итерации цикла, составляет десятки микросекунд.
3.  **Масштабируемость:** С увеличением числа процессов время выполнения растет. Это объясняется тем, что объем вычислений на один процесс уменьшается ($N^2/P$), а накладные расходы на синхронизацию растут с увеличением числа участников коммуникации.

**Заключение:**
Для эффективного использования MPI в методе сопряженных градиентов размерность системы должна быть существенно выше (порядка $N > 2000...5000$), чтобы время умножения матрицы на вектор (сложность $O(N^2)$) перекрывало время коммуникаций (сложность $O(N)$). Реализованный алгоритм корректен, но требует большей вычислительной нагрузки для демонстрации ускорения.