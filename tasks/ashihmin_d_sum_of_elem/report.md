# Суммирование элементов вектора

- Студент: Ашихмин Даниил Викторович, group 3823Б1ФИ2
- Технология: SEQ, MPI
- Вариант: 1

1. Введение
- Задача заключается в вычислении суммы всех элементов целочисленного вектора. Последовательный алгоритм прост, однако при больших размерах массива вычисления становятся длительными.
- Использование параллельных вычислений с помощью MPI позволяет разделить работу между несколькими процессами и тем самым ускорить выполнение.
- Ожидаемый результат — корректная и эффективная параллельная реализация, а также экспериментальный анализ ускорения.

2. Постановка задачи
Дан вектор целых чисел:
Вход:
    - Одномерный вектор std::vector<int> размера N > 0.
Выход:
    - Одно целое число — сумма всех элементов входного вектора.
Ограничения на размер входа отсутствуют; допускаются миллионы элементов.
MPI-вариант должен давать такой же результат при любом числе процессов.
Алгоритм должен корректно работать как последовательно, так и параллельно.

3. Базовый алгоритм (последовательный)
Последовательный алгоритм:
 1. Инициализировать аккумулятор: sum = 0.
 2. Последовательно пройти по всем элементам вектора.
 3. Прибавить каждый элемент к сумме.
 4. Вернуть итоговое значение.

Сложность по времени: O(N)
Сложность по памяти: O(1)

4. Схема распараллеливания (MPI)
- Процесс с рангом 0 владеет исходным вектором.
- Вектор делится на части примерно одинакового размера.
- Если размер N не делится на size, первые N % size процессов получают на 1 элемент больше.

- MPI_Scatterv распределяет части массива по процессам.
- Каждый процесс вычисляет локальную сумму.
- MPI_Allreduce объединяет локальные суммы в одну глобальную.

- Rank 0: формирует массивы counts и displs, вызывает MPI_Scatterv, участвует в редукции.
- Остальные ранги: принимают данные, считают сумму, участвуют в редукции.

Псевдокод:
counts = распределение нагрузки
displs = смещения
локальный_вектор = MPI_Scatterv(глобальный_вектор)
локальная_сумма = сумма(локальный_вектор)
глобальная_сумма = MPI_Allreduce(локальная_сумма, MPI_SUM)

Код RunImpl:
bool AshihminDElemVecsSumMPI::RunImpl() {
  const auto &vec = GetInput();

  int size = 0;
  int rank = 0;
  MPI_Comm_size(MPI_COMM_WORLD, &size);
  MPI_Comm_rank(MPI_COMM_WORLD, &rank);

  std::vector<int> counts(size);
  std::vector<int> displs(size);

  size_t n = vec.size();
  int base = static_cast<int>(n / size);
  int rem = static_cast<int>(n % size);

  for (int i = 0; i < size; ++i) {
    counts[i] = base;
    if (i < rem) {
      counts[i]++;
    }
  }

  displs[0] = 0;
  for (int i = 1; i < size; ++i) {
    displs[i] = displs[i - 1] + counts[i - 1];
  }

  std::vector<int> local(counts[rank]);
  MPI_Scatterv(vec.data(), counts.data(), displs.data(), MPI_INT, local.data(), counts[rank], MPI_INT, 0,
               MPI_COMM_WORLD);

  OutType local_sum = 0;
  for (int x : local) {
    local_sum += x;
  }

  OutType global_sum = 0;
  MPI_Allreduce(&local_sum, &global_sum, 1, MPI_LONG_LONG, MPI_SUM, MPI_COMM_WORLD);

  GetOutput() = global_sum;
  return true;
}

5. Детали реализации

mpi/include/ops_mpi.hpp - объявление классов 
mpi/src/ops_mpi.cpp - MPI-реализация
seq/include/ops_seq.hpp - объявление классов
seq/src/ops_seq.cpp - Последовательный вариант

- ValidationImpl() — проверяет, что входной вектор не пуст
- PreProcessingImpl() — подготовка не требуется
- RunImpl() — Scatterv + локальная сумма + Allreduce
- PostProcessingImpl() — завершающая стадия без действий

- Корректная работа со знаковыми и беззнаковыми типами (size_t vs int).
- MPI_Allreduce использует тип MPI_LONG_LONG, что позволяет избежать переполнения.
- counts и displs вычисляются автоматически.

6. Экспериментальная установка

Аппаратное обеспечение:
CPU: AMD Ryzen 5 3500X (3.6 - 4.1 GHz)(6 ядер / 6 потоков)
RAM: 16 ГБ
OS: Windows 11 Pro x64
MPI: Microsoft-MPI 10.1.1 

Инструменты:
Сборщик: CMake
Компилятор: MSVC 19.x
Конфигурация: Release 

Переменные окружения:
PPC_NUM_PROC=4
PPC_NUM_THREADS=1 
Генерация данных:
Тестовые наборы генерируются автоматически в тестах (случайные числа с фиксированным seed).

7. Результаты и обсуждение

7.1. Проверка корректности
Корректность проверялась с помощью:
- функциональных тестов PPC
- сравнения с последовательной реализацией
- тестов на:
- маленькие массивы
- отрицательные значения
- большие числа
- неравномерное распределение (N % size != 0)

8. Заключение

Была разработана параллельная MPI-реализация суммирования элементов вектора.
Алгоритм корректен, масштабируется и показывает ускорение при увеличении числа процессов.
Основные ограничения связаны с коммуникационными затратами и характеристиками целевого оборудования.

9. Список литературы

- Стандарт MPI - https://legacyupdate.net/download-center/download/57467/microsoft-mpi-v10.0
- Документация Microsoft MPI — https://learn.microsoft.com/en-us/message-passing-interface
- cppreference.com — Справочник по C++
- Внутренняя документация PPC-Framework

