# Сортировка пузырьком (алгоритм чет-нечетной перестановки)

- Студент: Никитин Антон Александрович, группа 3823Б1ФИ1
- Технология: SEQ, MPI 
- Вариант: 21

## 1. Введение
Сортировка пузырьком — классический учебный алгоритм, демонстрирующий базовые принципы сравнения и обмена элементов. В данном проекте реализуется параллельная версия алгоритма odd-even sort с использованием MPI для демонстрации ускорения сортировки больших массивов за счёт распределения данных между несколькими процессами. Ожидается, что MPI-версия покажет существенное ускорение по сравнению с последовательной реализацией, особенно для массивов значительного размера.

## 2. Постановка задачи
Задача: Сортировка пузырьком (алгоритм чет-нечетной перестановки)
Задача состоит в реализации параллельной сортировки одномерного массива вещественных чисел по неубыванию с использованием алгоритма odd-even sort и технологии MPI. 
Входные данные: Вектор элементов типа `double` произвольного размера `n` (0 ≤ `n` ≤ 10⁵). 
Выходные данные: отсортированный вектор того же размера. 
Основное ограничение: необходимость корректной работы с массивами произвольного размера, включая тривиальные случаи (n = 0, 1), и обеспечение масштабируемости при увеличении числа процессов.

## 3. Базовый алгоритм (Последовательный)
Базовым алгоритмом является классическая пузырьковая сортировка (bubble sort). Алгоритм проходит по массиву размера `n` и последовательно сравнивает соседние элементы. Если текущий элемент больше следующего, происходит их обмен. За каждый полный проход по массиву наибольший из неотсортированных элементов "всплывает" к концу массива. Процесс повторяется `n-1` раз, гарантируя полную сортировку. Вычислительная сложность алгоритма составляет O(n²), что делает его неэффективным для больших массивов, но простым для понимания и реализации.

Псевдокод последовательной версии:
```
Для i от 0 до n-2:
	Для j от 0 до n-i-2:
		Если arr[j] > arr[j+1]:
			Обменять arr[j] и arr[j+1]
```

Ключевые характеристики:
- Детерминированный алгоритм
- Устойчивость (сохраняет порядок равных элементов)
- Работа "на месте" (in-place) без дополнительной памяти
- Пространственная сложность O(1)
- Асимптотическая сложность: O(n²)

## 4. Схема распараллеливания

### 4.1 Алгоритм odd-even sort
Для параллельной реализации использован алгоритм **odd-even sort** — вариация пузырьковой сортировки, естественным образом поддающаяся распараллеливанию. Алгоритм делит сортировку на чётные и нечётные фазы, что позволяет выполнять независимые сравнения на разных процессах.

### 4.2 Распределение данных
- Массив размера `n` равномерно распределяется между `p` процессами
- Каждый процесс получает свою часть массива (`local`)
- Распределение сбалансированное: первые `n % p` процессов получают на 1 элемент больше
- Вычисляются смещения (`displs`) и количества элементов (`counts`) для каждой части

### 4.3 Топология коммуникации
- **Линейная топология**: процессы упорядочены по рангам (0, 1, ..., p-1)
- **Парные обмены**: на каждой фазе процессы обмениваются граничными элементами с соседями
- **Чередование направлений**: чётные фазы — сравнения внутри процессов, нечётные — обмен с соседями

### 4.4 Роли процессов
- **Процесс 0 (root)**: инициализация данных, сбор результатов, управление выполнением
- **Все процессы**: локальная сортировка своей части, обмен граничными элементами

### 4.5 Особенности реализации
- **Минимальная коммуникация**: обмен только граничными элементами
- **Синхронизация**: на каждой фазе через MPI_Sendrecv
- **Обработка граничных случаев**: корректная работа при n ≤ 1 и при некратном делении

Код MPI алгоритма представлен в приложении

## 5. Детали реализации

### 5.1 Структура кода
tasks/nikitin_a_buble_sort/
├── common/
│ └── include/common.hpp.hpp 
├── mpi/
│ ├── include/ops_mpi.hpp # Объявление класса NikitinABubleSortMPI
│ └── src/ops_mpi.cpp # Реализация MPI-версии
├── seq/
│ ├── include/ops_seq.hpp # Объявление класса NikitinABubleSortSEQ
│ └── src/ops_seq.cpp # Реализация последовательной версии
├── tests/
│ ├── functional/main.cpp # Функциональные тесты (20 тестовых случаев)
│ └── performance/main.cpp # Производительные тесты (3 типа данных)
├── report.md # Данный отчёт
├── info.json 
└── settings.json


**Ключевые классы и функции:**
- `NikitinABubleSortMPI` - основной класс MPI-реализации
- `ValidationImpl()` - проверка входных данных
- `RunImpl()` - основной цикл сортировки с MPI-операциями
- `LocalSort()` - локальная odd-even сортировка части массива
- `ExchangeRight()`, `ExchangeLeft()` - обмен граничными элементами

### 5.2 Важные допущения и граничные случаи

**Предположения:**
1. Только процесс 0 изначально владеет входными данными
2. Количество процессов задаётся через `mpiexec -n`
3. Все процессы выполняют одинаковый код (SPMD модель)

**Обработка граничных случаев:**
1. **Пустой массив (n = 0)** - немедленный возврат пустого результата
2. **Один элемент (n = 1)** - массив считается отсортированным, данные рассылаются всем процессам
3. **Количество процессов > размера массива** - некоторые процессы получают пустые части
4. **Неравномерное распределение** - корректное вычисление counts/displs с учётом остатка

### 5.3 Соображения по использованию памяти

**Распределение памяти:**
1. **Процесс 0**: хранит полный входной массив (~80 КБ для 10000 double)
2. **Каждый процесс**: хранит свою часть массива + временные буферы
3. **Временные буферы**: по 1 элементу для обмена с соседями

**Оценка использования:**
- Последовательная версия: O(1) дополнительной памяти
- MPI версия: O(n/p) на процесс + O(1) для коммуникации
- Пиковая память: на процессе 0 во время Scatterv ~ O(n)

**Оптимизации:**
- Использование `std::move()` для избежания лишних копий
- Локальные операции in-place без выделения дополнительных массивов
- Использование `MPI_Sendrecv` для минимизации deadlock'ов

## 6. Экспериментальная установка
- **Оборудование/ОС**: Windows 11 , Intel Core i7-12700H (14 ядер, 20 потоков), 8 ГБ ОЗУ
- **Инструментарий**: MSVC 19.41.34120, CMake 3.28.1, сборка Release
- **Окружение**: Количество процессов задаётся через `mpiexec -n N` (2, 4, 8 процессов)
- **Данные**: Генерация в тестах: 3 типа массивов по 10000 элементов (случайные, по возрастанию, по убыванию)

## 7. Результаты и обсуждение

### 7.1 Корректность
Корректность реализации проверялась через 20 функциональных тестов, охватывающих все граничные случаи: пустые массивы, один элемент, уже отсортированные данные, обратный порядок, случайные числа, дубликаты, отрицательные значения, большие массивы (1000 элементов). В каждом тесте результат сравнивается с эталонным отсортированным массивом, полученным через `std::sort`. Для вещественных чисел использовано сравнение с допустимой погрешностью 1e-10. Все тесты успешно проходят с различным количеством MPI-процессов (1, 2, 4, 8), что подтверждает корректность алгоритма и коммуникационной логики.

### 7.2 Производительность

| Режим | Кол-во процессов | Время, с | Ускорение | Эффективность |
|-------|------------------|----------|-----------|---------------|
| seq   | 1                | 0.871    | 1.00      | N/A           |
| mpi   | 2                | 0.538    | 1.62      | 81.0%         |
| mpi   | 4                | 0.368    | 2.37      | 59.3%         |
| mpi   | 8                | 0.403    | 2.16      | 27.0%         |


## 8. Выводы
Реализованная MPI-версия odd-even sort демонстрирует положительное ускорение по сравнению с последовательной реализацией. Наилучшие результаты достигаются при 4 процессах с ускорением 2.37× и эффективностью 59.3%. При увеличении числа процессов до 8 наблюдается падение эффективности до 27.0% из-за возрастающих накладных расходов на коммуникацию между процессами.

**Ключевые результаты:**
1. Алгоритм корректно работает для всех типов входных данных (проверено 20 функциональными тестами)
2. Максимальное ускорение достигается при среднем числе процессов (2-4)
3. Эффективность снижается при большом количестве процессов из-за частых обменов граничными элементами

**Ограничения:**
1. Алгоритм odd-even sort имеет сложность O(n²), что ограничивает применимость для очень больших массивов
2. Частые синхронизации между фазами ограничивают масштабируемость
3. При 8 процессах коммуникационные накладные расходы превышают выигрыш от параллелизации


## 9. Литература
1. [Открытая документация MPI](https://www.open-mpi.org/doc/) 
2. Лекции и практические занятия по предемту "Параллельное программирование" ННГУ
3. [cppreference.com](https://en.cppreference.com/)

## Приложение 
```
// Код параллельного алгоритма 

// Распределение данных между процессами
const int base = n / comm_size;
const int rem = n % comm_size;
const int count = base + (rank < rem ? 1 : 0);
const int offset = rank * base + std::min(rank, rem);

// Подготовка для Scatterv/Allgatherv
std::vector<int> counts(comm_size);
std::vector<int> displs(comm_size);
int current_offset = 0;
for (int i = 0; i < comm_size; ++i) {
    counts[i] = base + (i < rem ? 1 : 0);
    displs[i] = current_offset;
    current_offset += counts[i];
}

// Распределение данных по процессам
std::vector<double> local(count);
MPI_Scatterv(rank == 0 ? data.data() : nullptr, counts.data(), displs.data(), 
             MPI_DOUBLE, local.data(), count, MPI_DOUBLE, 0, MPI_COMM_WORLD);

// Основной цикл odd-even sort
for (int phase = 0; phase < n; ++phase) {
    const int parity = phase & 1;      // Чётность фазы
    const int tag = phase;             // Уникальный тег

    // Локальная сортировка и обмен с соседями
    LocalSort(local, offset, parity);
    ExchangeRight(local, counts, displs, rank, comm_size, parity, tag);
    ExchangeLeft(local, counts, displs, rank, parity, tag);
}

// Сбор результатов
std::vector<double> result(n);
MPI_Allgatherv(local.data(), count, MPI_DOUBLE, 
               result.data(), counts.data(), displs.data(), MPI_DOUBLE,
               MPI_COMM_WORLD);
```