# Итеративные методы (Якоби)

- Студентка: Романова Василиса Дмитриевна, группа 3823Б1ФИ3
- Технология: SEQ, MPI
- Вариант: 18

## 1. Введение
Решение систем линейных алгебраических уравнений является фундаментальной задачей вычислительной математики. Итерационный метод Якоби основан на последовательных приближениях и особенно эффективен для больших разреженных систем, где прямые методы, такие как метод Гаусса, становятся неприменимыми из-за высоких вычислительных затрат. В условиях постоянно растущих требований к производительности и объёмам обрабатываемых данных актуальной становится разработка параллельных версий классических алгоритмов. Данная лабораторная работа посвящена реализации и сравнительному анализу последовательной и параллельной версий метода Якоби, с целью исследования возможностей ускорения вычислений за счёт распределения обработки данных между несколькими вычислительными узлами.

## 2. Постановка задачи
На вход подается матрица $A$, вектора $x_0, b$ и параметры $eps$ и $maxIterations$. Необходимо найти вектор $x$ такой, что $|Ax - b| < eps$. Алгоритм заканчивается, если достигнута искомая точность или если число итераций превысило заданное.

**Входные данные:** 
- матрица $A$ (`std::vector<std::vector<double>>`)
- вектор $b$ (`std::vector<double>`)
- начальный вектор $x_0$ (`std::vector<double>`)
- искомая точность `eps` (`double`)
- ограничение на количество итераций `maxIterations` (`size_t`).

**Выходные данные:** искомый вектор $x$ (`std::vector<int>`).

**Ограничения:** матрица $A$ должна обладать свойством строгого диагонального преобладания, т.е. $\forall i \ |a_{ii}| > \sum_{j \neq i}|a_{ij}|$.

При соблюдении ограничений метод Якоби сходится, поэтому в качестве условия остановки используется условие $\max_{i=1}^{n} |x^{(k)}[i] - x^{(k-1)}[i+1]| < eps$, где $x^{(k)}$ — значения вектора $x$ после $k$ итераций. Это позволяет значительно снизить вычислительную сложность алгоритма, поскольку мы не выполняем перемножение матрицы на вектор каждую итерацию.

## 3. Последовательный алгоритм
Последовательный алгоритм реализует классический метод Якоби, в котором на каждой итерации следующее значение $x$ вычисляется по формуле

 $x_i^{(k+1)} = \frac{1}{a_{ii}} \left( b_i - \sum_{j \neq i} a_{ij} x_j^{(k)} \right)$

 **Алгоритм:**
 1. ***Валидация:*** выполняется проверка корректности входных данных, включая проверку строгого диагонального преобладания матрицы системы.
 2. ***Итерационный процесс:***
    - Вычисление нового приближения: $x_i^{(k+1)} = \frac{1}{a_{ii}} \left( b_i - \sum_{j \neq i} a_{ij} x_j^{(k)} \right)$
    - Проверка условия сходимости: $|x_i^{k+1} - x_i^{k}| < eps$
    - Если условие выполнено или достигнуто максимальное число итераций, то выполнение прекращается.

## 4. Параллельный алгоритм
Параллельная реализация метода Якоби основана на распределении строк матрицы коэффициентов между процессами. 
### 4.1 Подготовка данных
Распределение данных между процессами осуществляется по следующей схеме. Пусть `n_` — общее количество строк матрицы, `n` — число процессов. Тогда:

`delta = n_ / n` — базовое количество строк на процесс

`extra = n_ % n` — количество "лишних" строк, которые будут распределены по одному на первые extra процессов

Для каждого процесса с рангом `rank` вычисляется:

`local_n_ = delta + (rank < extra ? 1 : 0)` — количество строк, обрабатываемых данным процессом

`st_row_ = rank * delta + (rank < extra ? rank : extra)` — индекс первой строки матрицы, обрабатываемой данным процессом

Процесс с рангом 0 подготавливает данные для рассылки и последующего сбора. Для удобства пересылки исходный двумерный массив преобразуется в одномерный вектор. Также заполняются вспомогательные массивы `vector_counts` и `vector_displs`, определяющие количество элементов и смещения для каждого процесса.

Подготовленные данные рассылаются процессам с помощью функций `MPI_Bcast` (для скалярных значений и общего вектора решений) и `MPI_Scatterv` (для распределения строк матрицы A и вектора b). Каждый процесс получает свою порцию строк матрицы и соответствующих элементов вектора правых частей, а так же полный вектор x.

### 4.2 Итерационный процесс

1. Каждый процесс вычисляет новые значения для своего набора неизвестных по формуле метода Якоби, используя значения предыдущей итерации со всех процессов.
2. Вычисляется локальная максимальная разность между текущими и предыдущими значениями.
3. С помощью `MPI_Allgatherv` все процессы обмениваются вычисленными значениями, формируя полный актуальный вектор решения.
4. С помощью `MPI_Allreduce` с операцией `MPI_MAX` находится глобальная максимальная разность по всем процессам.

Итерации продолжаются до достижения заданной точности или превышения максимального числа итераций.

## 5. Experimental Setup
- **Hardware/OS:** Intel i5-1135G7, 4 ядра, 8 логических процессов, RAM 8GB, Windows 10
- **Toolchain:** clang 21.1.0, Release
## 5.1 Входные данные:

**Матрица размером 200×200:**

Диагональные элементы: $a_{ii} = 10.01$ для всех $i = 0, \dots, 199$

Элементы, смежные с диагональю: $a_{i,i+1} = a_{i,i-1} = 5.0$ для соответствующих индексов

Остальные элементы равны 0

**Вектор правых частей $b$:**

$b_i = 20.01$ для $i = 1, \dots, n-2$

Граничные элементы: $b_0 = b_{199} = 15.01$

**Начальное приближение:**

$x_i = -1000.0$ для всех $i = 0, \dots, 199$

Такая матрица обладает слабым диагональным преобладанием, за счет чего обеспечивается медленная сходимость (~25000 итераций)

## 6. Результаты

### 6.1 Корректность
Корректность работы последовательного и параллельного алгоритмов подтверждена проведением функциональных тестирований с системами уравнений различных размеров и структур. Тесты охватывают как граничные случаи (матрица 1×1), так и системы большей размерности с различными свойствами матриц.

Тестовые сценарии включают следующие варианты:
- Матрицы общего вида различной размерности 
- Случаи быстрой и медленной сходимости
- Матрицы со знакопеременными элементами
- Трёхдиагональные матрицы
- Блочно-диагональные матрицы
- Разреженные матрицы


### 6.2 Производительность
| Mode        | Count | Time, s | Speedup | Efficiency |
|-------------|-------|---------|---------|------------|
| seq         | 1     | 0.938   | 1.00    | N/A        |
| mpi         | 2     | 0.112   | 8.38    | 400%       |
| mpi         | 3     | 0.087   | 10.8    | 359%       |
| mpi         | 4     | 0.079   | 11.9    | 297%       |

## 7. Выводы
В ходе работы были реализованы и протестированы последовательная и параллельная версии метода Якоби для решения систем линейных уравнений.

Параллельный алгоритм демонстрирует высокую производительность: при использовании 2-4 процессов достигается ускорение 8.38-11.9 раз с суперлинейным ускорением при двух процессах, что обусловлено эффективным использованием кэш-памяти. Наибольшее абсолютное ускорение получено при 4 процессах, однако наиболее эффективное использование ресурсов наблюдается при 2-3 процессах.

Алгоритм показал хорошую масштабируемость, хотя с увеличением числа процессов эффективность снижается из-за роста коммуникационных затрат. Результаты подтверждают целесообразность применения параллельной реализации метода Якоби для решения СЛАУ средней размерности на многопроцессорных системах.

## 8. Литература
1. Стандарт MPI.
2. Лекции и практики по параллельному программированию.
3. Saad, Yousef. Iterative Methods for Sparse Linear Systems (2nd ed.).

## 9. Приложение

```cpp
bool RomanovaVJacobiMethodMPI::PreProcessingImpl() {
  int rank = 0;
  int n = 0;

  MPI_Comm_rank(MPI_COMM_WORLD, &rank);
  MPI_Comm_size(MPI_COMM_WORLD, &n);

  std::vector<int> send_countsA(n);
  vector_counts_ = std::vector<int>(n);

  std::vector<int> displs_scattA(n);
  vector_displs_ = std::vector<int>(n);

  if (rank == 0) {
    n_ = std::get<1>(GetInput()).size();
  }

  MPI_Bcast(&n_, 1, MPI_INT, 0, MPI_COMM_WORLD);
  x_ = OutType(n_);

  if (rank == 0) {
    std::vector<OutType> tempA;
    std::tie(x_, tempA, b_, eps_, maxIterations_) = GetInput();

    for (const auto &vec : tempA) {
      A_.insert(A_.end(), vec.begin(), vec.end());
    }
  }

  MPI_Bcast(&eps_, 1, MPI_DOUBLE, 0, MPI_COMM_WORLD);
  MPI_Bcast(&maxIterations_, 1, MPI_INT, 0, MPI_COMM_WORLD);

  size_t delta = n_ / n;
  size_t extra = n_ % n;

  local_n_ = delta + (static_cast<size_t>(rank) < extra ? 1 : 0);
  st_row_ = rank * delta + (static_cast<size_t>(rank) < extra ? rank : extra);

  if (rank == 0) {
    send_countsA = std::vector<int>(n, static_cast<int>(delta * n_));
    vector_counts_ = std::vector<int>(n, static_cast<int>(delta));
    for (size_t i = 0; i < extra; i++) {
      send_countsA[i] += n_;
      vector_counts_[i]++;
    }

    for (int i = 1; i < n; i++) {
      displs_scattA[i] = displs_scattA[i - 1] + send_countsA[i - 1];
      vector_displs_[i] = vector_displs_[i - 1] + vector_counts_[i - 1];
    }
  }

  MPI_Bcast(vector_counts_.data(), n, MPI_INT, 0, MPI_COMM_WORLD);
  MPI_Bcast(vector_displs_.data(), n, MPI_INT, 0, MPI_COMM_WORLD);

  OutType local_data = OutType(local_n_ * n_);
  OutType local_b = OutType(local_n_);

  MPI_Bcast(x_.data(), n_, MPI_DOUBLE, 0, MPI_COMM_WORLD);

  MPI_Scatterv(rank == 0 ? A_.data() : nullptr, send_countsA.data(), displs_scattA.data(), MPI_DOUBLE,
               local_data.data(), static_cast<int>(local_data.size()), MPI_DOUBLE, 0, MPI_COMM_WORLD);

  MPI_Scatterv(rank == 0 ? b_.data() : nullptr, vector_counts_.data(), vector_displs_.data(), MPI_DOUBLE,
               local_b.data(), static_cast<int>(local_b.size()), MPI_DOUBLE, 0, MPI_COMM_WORLD);

  A_ = std::move(local_data);
  b_ = std::move(local_b);

  return true;
}

bool RomanovaVJacobiMethodMPI::RunImpl() {
  size_t k = 0;
  OutType prev(x_.size(), 0.0);
  OutType local_x(local_n_);
  double diff = 0.0, glob_diff = 0.0;
  do {
    diff = 0.0;
    prev = x_;
    for (size_t i = 0; i < local_n_; i++) {
      double sum = 0.0;
      for (size_t j = 0; j < n_; j++) {
        sum += ((st_row_ + i) != j ? A_[i * n_ + j] * prev[j] : 0);
      }
      local_x[i] = (b_[i] - sum) / A_[i * (n_ + 1) + st_row_];

      diff = std::max(diff, abs(local_x[i] - prev[st_row_ + i]));
    }
    k++;
    MPI_Allgatherv(local_x.data(), local_n_, MPI_DOUBLE, x_.data(), vector_counts_.data(), vector_displs_.data(),
                   MPI_DOUBLE, MPI_COMM_WORLD);

    MPI_Allreduce(&diff, &glob_diff, 1, MPI_DOUBLE, MPI_MAX, MPI_COMM_WORLD);

  } while (glob_diff >= eps_ && k <= maxIterations_);
  return true;
}

```