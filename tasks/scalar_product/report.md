## Скалярное произведение векторов

**Студент:** Лобанов Дмитрий Александрович  
**Группа:** 3823Б1ФИ2  
**Вариант:** 9

### Введение
**Мотивация.** Скалярное произведение векторов является одной из базовых операций линейной алгебры, нашедшей применение в различных областях: машинное обучение, обработка сигналов, научные вычисления. Операция скалярного произведения сама по себе очень простая и придумать алгоритм делающий эту операцию еще проще сложно представить - поэтому было принято решение написать параллельную версию и сравнить ее с последовательной
**Проблематика.** Возможно, издержки реализации MPI не получится компенсировать скоростью работы 
**Ожидаемый результат.** 
Ожидаем что распараллеленная версия будет превосходить последовательную версию только на сильно больших по размерам векторах 

### Постановка задачи
**Задано:** два целочисленных вектора имеющих одинаковую длину.  
**Требуется:** вычислить скалярное произведение двух векторов.  
**Входные данные:** Входные данные: пара векторов std::vector<int>.  
**Выходные данные:** целочисленное значение скалярного произведения.

### Последовательный алгоритм (seq)
1. Проверка корректности входных данных (одинаковый размер, непустые векторы)
2. Инициализация переменной для накопления суммы
3. Последовательное вычисление суммы произведений элементов
4. Сохранение результата

### Параллельная версия
**Распределение данных**: Главный процесс (rank 0) распределяет данные между всеми процессами  
**Декомпозиция**: Векторы разбиваются на равные (или почти равные) части с учетом остатка  
**Локальные вычисления**: Каждый процесс вычисляет частичную сумму своего блока данных  
**Редукция**: С использованием MPI_Reduce с операцией MPI_SUM собираются все частичные суммы  
**Распространение результата**: Результат рассылается всем процессам через MPI_Bcast  

### Экспериментальная установка
* Процессор: AMD Ryzen 5 5600G  (6 физических ядер, 12 логических потока).
* Оперативная память: 16 GB.
* Операционная система: Microsoft Windows 10.
* Конфигурация Release x64.

### Результаты
#### Корректность
Проверка выполнена командой `python scripts/run_tests.py --running-type processes --counts n`. Где n - число процессов. Мною были проверены n от 1 до 8, никаких ошибок не было, все мои тесты показали [passed]

#### Производительность
Измерения проводились с помощью `ppc_perf_tests.exe --gtest_filter=*vector_scalar_product*`, размер векторов был 50.000.000. Последовательный вариант запускался на одном процессе, параллельный на 2,4,8 потоках

| Реализация | Процессов | Время, сек|
|-----------:|----------:|---------- |
| seq        | 1         | 0.2532    |
| mpi        | 2         | 0.2040    |
| mpi        | 4         | 0.1625    |
| mpi        | 8         | 0.1425    |

И на размере вектора в 10 млн

| Реализация | Процессов | Время, сек |
|-----------:|----------:|----------  |
| seq        | 1         | 0.04044    |
| mpi        | 2         | 0.03577    |
| mpi        | 4         | 0.03365    |
| mpi        | 8         | 0.03133    | 

### Выводы
Seq реализация неплохо сработала на большом размере векторов, но все же видно что параллельная версия значительно быстрее. На маленьких по размерности векторах разницу еще сложнее заметить, так что параллельная версия, как и ожидалась будет сильно превосходить последовательную на больших по размерности векторах.

### Практическая значимость
Разработанная реализация может быть использована как основа для более сложных параллельных вычислений, требующих операций редукции, и демонстрирует типичные паттерны MPI-программирования для задач с декомпозицией данных