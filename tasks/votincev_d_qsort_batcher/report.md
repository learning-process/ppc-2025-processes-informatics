# Отчет по реализации алгоритма быстрой сортировки с четно-нечетным слиянием Бэтчера

**Дисциплина:** Параллельное программирование  
**Преподаватель:** Нестеров Александр Юрьевич и Оболенский Арсений Андреевич  
**Студент:** Вотинцев Дмитрий Сергеевич, группа 3823Б1ФИ3  
**Вариант:** 15  

---

## Введение

В рамках данной лабораторной работы был реализован алгоритм быстрой сортировки
с четно-нечетным слиянием Бэтчера. 

В данной работе как в последовательной, так и в параллельной версиях используется
собственная реализация алгоритма быстрой сортировки (quicksort), что позволяет
исключить влияние библиотечных оптимизаций и обеспечить корректное сравнение
времени выполнения SEQ и MPI реализаций.

Основной целью работы является исследование эффективности параллельной сортировки
и анализ причин, влияющих на ускорение или его отсутствие при использовании MPI.

---

## Постановка задачи

Необходимо реализовать алгоритм сортировки массива вещественных чисел:

- последовательную (SEQ) версию алгоритма;
- параллельную (MPI) версию алгоритма с использованием четно-нечетного
  слияния Бэтчера.

В рамках работы необходимо:
- реализовать собственный алгоритм быстрой сортировки;
- обеспечить корректную сортировку входных данных;
- провести серию вычислительных экспериментов;
- измерить время выполнения SEQ и MPI версий;
- оценить ускорение, получаемое при использовании нескольких процессов.

---

## Описание алгоритма

Последовательная версия реализована на основе классического алгоритма быстрой
сортировки. В качестве опорного элемента выбирается центральный элемент массива,
после чего массив разбивается на две части: элементы меньше и элементы больше
опорного. Процедура рекурсивно применяется к полученным подмассивам.

Средняя временная сложность алгоритма составляет O(n log n), сортировка выполняется
на месте без использования дополнительной памяти.

Параллельная версия алгоритма строится следующим образом:
- исходный массив разбивается на несколько блоков;
- каждый процесс получает свой блок данных;
- локальная сортировка выполняется с использованием той же реализации быстрой
  сортировки;
- далее применяется алгоритм четно-нечетного слияния Бэтчера, в ходе которого
  процессы попарно обмениваются данными и оставляют у себя либо меньшую, либо
  большую часть объединённого массива;
- после завершения всех фаз массив становится глобально отсортированным.

---

## Описание схемы параллельного алгоритма

Параллельная версия реализована с использованием библиотеки MPI и включает
следующие этапы:

1. Нулевой процесс получает входные данные.
2. Размер массива передается всем процессам.
3. Данные распределяются между процессами с помощью MPI_Scatterv.
4. Каждый процесс сортирует свой локальный подмассив с помощью собственной
   реализации быстрой сортировки.
5. Выполняется четно-нечетное слияние Бэтчера, включающее несколько фаз обмена
   данными между соседними процессами.
6. Итоговый отсортированный массив собирается на нулевом процессе с помощью
   MPI_Gatherv.

Данная схема обеспечивает корректность сортировки, однако сопровождается
значительными коммуникационными издержками.

---

## Результаты экспериментов и выводы

Для проверки корректности работы алгоритма были проведены следующие тесты:

- Тест 1: отсортированный массив  
- Тест 2: массив, отсортированный в обратном порядке  
- Тест 3: только положительные числа  
- Тест 4: только отрицательные числа  
- Тест 5: массив из нулей  
- Тест 6: массив из одинаковых элементов  
- Тест 7: 1000 случайных чисел из диапазона [-50, 50]  
- Тест 8: 1000 случайных чисел из диапазона [-500, 500]  
- Тест 9: 1000 случайных чисел из диапазона [-2000, 2000]  
- Тест 10: 2000 случайных чисел из диапазона [-1, 1]  


### Результаты для 2-х процессов

| N        | SEQ версия (с) | MPI версия (с) | Ускорение |
|----------|----------------|----------------|-----------|
| 100000   | 0.053          | 0.052          | 1.02x     |
| 1000000  | 0.442          | 0.446          | 0.99x     |
| 10000000 | 4.949          | 4.264          | 1.16x     |
| 20000000 | 9.015          | 9.702          | 0.93x     |
| 40000000 | 18.786         | 19.101         | 0.98x     |

### Результаты для 4-х процессов

| N        | SEQ версия (с) | MPI версия (с) | Ускорение |
|----------|----------------|----------------|-----------|
| 100000   | 0.048          | 0.056          | 0.86x     |
| 1000000  | 0.460          | 0.476          | 0.97x     |
| 10000000 | 4.949          | 4.264          | 1.16x     |
| 20000000 | 9.470          | 11.348         | 0.83x     |
| 40000000 | 20.780         | 21.612         | 0.96x     |

Анализ результатов показывает, что ускорение при использовании MPI является
невысоким. В ряде случаев параллельная версия работает медленнее последовательной,
что объясняется большими затратами на межпроцессные обмены данных и сравнительно
небольшой вычислительной нагрузкой алгоритма.

---

## Заключение

В ходе выполнения лабораторной работы были реализованы последовательная и
параллельная версии алгоритма быстрой сортировки с четно-нечетным слиянием
Бэтчера с использованием собственной реализации quicksort.

Экспериментальные результаты показали, что при использовании MPI ускорение
ограничено и во многих случаях коммуникационные издержки превышают выигрыш
от распараллеливания.

---

## Литература

1. Лекции Сысоева Александра Владимировича  
2. Практические занятия Нестерова Александра Юрьевича и Оболенского Арсения Андреевича  
3. Интернет  


## Приложения (код параллельной реализации)

'''
bool VotincevDQsortBatcherMPI::RunImpl() {
  int proc_n = 0, rank = 0;
  MPI_Comm_size(MPI_COMM_WORLD, &proc_n);
  MPI_Comm_rank(MPI_COMM_WORLD, &rank);

  int total_size = 0;
  if (rank == 0) {
    total_size = static_cast<int>(GetInput().size());
  }
  MPI_Bcast(&total_size, 1, MPI_INT, 0, MPI_COMM_WORLD);

  // ---------- один процесс ----------
  if (proc_n == 1) {
    if (rank == 0) {
      auto out = GetInput();
      QuickSort(out.data(), 0, static_cast<int>(out.size()) - 1);
      GetOutput() = out;
    }
    return true;
  }

  // ---------- sizes & offsets ----------
  std::vector<int> sizes(proc_n), offsets(proc_n);

  int base = total_size / proc_n;
  int extra = total_size % proc_n;

  for (int i = 0; i < proc_n; i++) {
    sizes[i] = base + (i < extra ? 1 : 0);
  }

  offsets[0] = 0;
  for (int i = 1; i < proc_n; i++) {
    offsets[i] = offsets[i - 1] + sizes[i - 1];
  }

  // ---------- Scatter ----------
  std::vector<double> local(sizes[rank]);

  MPI_Scatterv(
      rank == 0 ? GetInput().data() : nullptr,
      sizes.data(), offsets.data(),
      MPI_DOUBLE,
      local.data(), sizes[rank],
      MPI_DOUBLE,
      0, MPI_COMM_WORLD);

  // ---------- Local manual quicksort ----------
  if (!local.empty()) {
    QuickSort(local.data(), 0, static_cast<int>(local.size()) - 1);
  }

  // ---------- Buffers ----------
  int max_block = *std::max_element(sizes.begin(), sizes.end());
  std::vector<double> recv_buf(max_block);
  std::vector<double> merge_buf(sizes[rank] + max_block);

  // ---------- Odd-even Batcher ----------
  for (int phase = 0; phase < proc_n; phase++) {
    int partner;
    if (phase % 2 == 0) {
      partner = (rank % 2 == 0) ? rank + 1 : rank - 1;
    } else {
      partner = (rank % 2 == 1) ? rank + 1 : rank - 1;
    }

    if (partner < 0 || partner >= proc_n) continue;

    MPI_Sendrecv(
        local.data(), sizes[rank], MPI_DOUBLE, partner, 0,
        recv_buf.data(), sizes[partner], MPI_DOUBLE, partner, 0,
        MPI_COMM_WORLD, MPI_STATUS_IGNORE);

    std::merge(local.begin(), local.end(),
               recv_buf.begin(), recv_buf.begin() + sizes[partner],
               merge_buf.begin());

    if (rank < partner) {
      std::copy(merge_buf.begin(),
                merge_buf.begin() + sizes[rank],
                local.begin());
    } else {
      std::copy(merge_buf.begin() + sizes[partner],
                merge_buf.begin() + sizes[partner] + sizes[rank],
                local.begin());
    }
  }

  // ---------- Gather ----------
  if (rank == 0) {
    std::vector<double> result(total_size);
    MPI_Gatherv(local.data(), sizes[rank], MPI_DOUBLE,
                result.data(), sizes.data(), offsets.data(),
                MPI_DOUBLE, 0, MPI_COMM_WORLD);
    GetOutput() = result;
  } else {
    MPI_Gatherv(local.data(), sizes[rank], MPI_DOUBLE,
                nullptr, nullptr, nullptr,
                MPI_DOUBLE, 0, MPI_COMM_WORLD);
  }

  return true;
}
'''