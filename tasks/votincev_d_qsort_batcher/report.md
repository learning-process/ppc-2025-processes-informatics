# Отчет по реализации алгоритма быстрой сортировки с четно-нечетным слиянием Бэтчера

**Дисциплина:** Параллельное программирование  
**Преподаватель:** Нестеров Александр Юрьевич и Оболенский Арсений Андреевич  
**Студент:** Вотинцев Дмитрий Сергеевич, группа 3823Б1ФИ3  
**Вариант:** 15  

---

## Введение

В рамках данной лабораторной работы был реализован алгоритм параллельной сортировки, сочетающий быструю сортировку (Quicksort) для локальной обработки данных и четно-нечетное слияние Бэтчера (Batcher Odd-Even Merge Sort) для глобальной сортировки.

В данной работе как в последовательной, так и в параллельной версиях используется собственная реализация алгоритма быстрой сортировки (quicksort), что позволяет исключить влияние библиотечных оптимизаций и обеспечить корректное сравнение времени выполнения SEQ и MPI реализаций.

Основной целью работы является исследование эффективности параллельной сортировки и анализ причин, влияющих на ускорение или его отсутствие при использовании MPI.

---

## Постановка задачи

Необходимо реализовать алгоритм сортировки массива вещественных чисел:
- последовательную (SEQ) версию алгоритма;
- параллельную (MPI) версию алгоритма с использованием четно-нечетного слияния Бэтчера.

В рамках работы необходимо:
- реализовать собственный алгоритм быстрой сортировки;
- обеспечить корректную сортировку входных данных;
- провести серию вычислительных экспериментов;
- измерить время выполнения SEQ и MPI версий;
- оценить ускорение, получаемое при использовании нескольких процессов

---

## Описание алгоритма

### Последовательная версия (QuickSort)

Последовательная версия реализована на основе классического итеративного алгоритма быстрой сортировки. В качестве опорного элемента (pivot) выбирается центральный элемент массива, после чего массив разбивается на две части: элементы меньше и элементы больше опорного. Процедура повторяется с использованием стека для обработки подмассивов.

Средняя временная сложность алгоритма составляет $O(N \log N)$.

### Параллельная версия (QuickSort + Batcher Odd-Even Merge)

Параллельная версия использует гибридный подход:

1.  **Локальная сортировка (QuickSort):** Исходный массив размера $N$ разбивается на $P$ блоков, и каждый процесс (ранг) независимо сортирует свой локальный блок размера $N/P$. Сложность этого этапа составляет $O((N/P) \log(N/P))$.
2.  **Глобальное слияние (Batcher Odd-Even Merge):** Для достижения глобальной сортировки используется алгоритм Бэтчера. Алгоритм состоит из $P$ фаз, где процессы в каждой фазе попарно обмениваются своими локально отсортированными массивами, выполняют слияние и оставляют себе либо меньшую, либо большую часть объединённого массива, гарантируя глобальную отсортированность после всех фаз. 

---

## Описание схемы параллельного алгоритма

Параллельная версия реализована с использованием библиотеки MPI и включает следующие этапы:
1. Нулевой процесс получает входные данные.
2. Размер массива передается всем процессам.
3. Данные распределяются между процессами с помощью MPI_Scatterv.
4. Каждый процесс сортирует свой локальный подмассив с помощью собственной реализации быстрой сортировки.
5. Выполняется четно-нечетное слияние Бэтчера, включающее несколько фаз обмена данными между соседними процессами.
6. Итоговый отсортированный массив собирается на нулевом процессе с помощью MPI_Gatherv.


---

### Результаты экспериментов

## Результаты для 2 процессов

| N       | SEQ версия (с) | MPI версия (с) | Ускорение |
|---------|----------------|----------------|-----------|
| 70000   | 0.164          | 0.101          | 1.62x     |
| 300000  | 0.723          | 0.411          | 1.76x     |
| 600000  | 1.467          | 0.887          | 1.65x     |
| 2000000 | 4.958          | 2.892          | 1.71x     |
| 5000000 | 12.522         | 7.231          | 1.73x     |

## Результаты для 4 процессов

| N       | SEQ версия (с) | MPI версия (с) | Ускорение |
|---------|----------------|----------------|-----------|
| 70000   | 0.167          | 0.062          | 2.69x     |
| 300000  | 0.720          | 0.262          | 2.75x     |
| 600000  | 1.490          | 0.531          | 2.81x     |
| 2000000 | 5.113          | 2.004          | 2.55x     |
| 5000000 | 13.104         | 4.739          | 2.77x     |


Можно заметить, что в среднем для 2-х процессов ускорение находится в пределах 1.62x-1.76x (среднее 1.7x), а для 4-х процессов в пределах 2.55x-2.81x (среднее 2.65x).

Такое ускорение объясняется относительно низкими накладными расходами на коммуникацию процессов и их синхронизацию.

Ускорение неидеально - MPI занимает время на передачу данных (слияние по алгоритму Бэтчера).

Для 4-х процессов эффективность поменьше - потому что при увеличении числа процессов будет увеличиваться количество фаз в алгоритме Бэтчера, а значит будет больше коммуникаций.
---

## Заключение

В ходе выполнения лабораторной работы были реализованы последовательная и параллельная версии алгоритма быстрой сортировки с четно-нечетным слиянием Бэтчера с использованием собственной реализации QuickSort.

Эксперименты показали, что MPI версия обеспечивает существенное ускорение в сравнении с SEQ версией:
* Для 2 процессов достигнуто ускорение до $\approx 1.76x$.
* Для 4 процессов достигнуто ускорение до $\approx 2.76x$.

Снижение эффективности при переходе от $P=2$ к $P=4$ объясняется увеличением накладных расходов на коммуникацию (фазы обмена `MPI_Sendrecv` в слиянии Бэтчера) и операциями распределения/сбора (`MPI_Scatterv`/`MPI_Gatherv`).

---

## Литература

1.  Лекции Сысоева Александра Владимировича  
2.  Практические занятия Нестерова Александра Юрьевича и Оболенского Арсения Андреевича  
3.  Интернет  

## Приложения (код параллельной реализации)

```
bool VotincevDQsortBatcherMPI::RunImpl() {
  // получаю кол-во процессов
  int proc_n = 0;
  MPI_Comm_size(MPI_COMM_WORLD, &proc_n);

  // получаю ранг процесса
  int rank = 0;
  MPI_Comm_rank(MPI_COMM_WORLD, &rank);

  // если процесс 1 - то просто как в SEQ
  if (proc_n == 1) {
    auto out = GetInput();
    if (!out.empty()) {
      QuickSort(out.data(), 0, static_cast<int>(out.size()) - 1);
    }
    GetOutput() = out;
    return true;
  }

  // размер массива знает только 0-й процесс
  int total_size = 0;
  if (rank == 0) {
    total_size = static_cast<int>(GetInput().size());
  }

  // рассылаем размер массива всем процессам
  MPI_Bcast(&total_size, 1, MPI_INT, 0, MPI_COMM_WORLD);

  if (total_size == 0) {
    return true;
  }

  // вычисление размеров и смещений
  std::vector<int> sizes;
  std::vector<int> offsets;
  ComputeDistribution(proc_n, total_size, sizes, offsets);

  // распределение данных (Scatter)
  std::vector<double> local(sizes[rank]);
  ScatterData(rank, sizes, offsets, local);

  // локальная сортировка
  // каждый процесс сортирует свой кусок
  if (!local.empty()) {
    QuickSort(local.data(), 0, static_cast<int>(local.size()) - 1);
  }

  // чет-нечет слияние Бэтчера
  BatcherMergeSort(rank, proc_n, sizes, local);

  // 0й процесс собирает результыт от других процессов
  GatherResult(rank, total_size, sizes, offsets, local);

  return true;
}
```