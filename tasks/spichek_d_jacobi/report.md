# Решение систем линейных уравнений методом Якоби
## «Параллельное вычисление интеграла методом Якоби с использованием MPI»

**Дисциплина:** Параллельное программирование  
**Студент:** Спичек Денис Игоревич, группа 3823Б1ФИ1  
**Технология:** MPI + SEQ  
**Вариант:** 18

---

## 1. Introduction

Метод Якоби — это итерационный алгоритм для численного решения системы линейных алгебраических уравнений (СЛАУ) вида $Ax = b$.  
Он особенно эффективен для разреженных матриц и хорошо поддаётся распараллеливанию, так как вычисление каждого $x_i^{(k+1)}$ зависит только от значений предыдущей итерации.

Цель работы: реализация последовательной и MPI‑параллельной версий метода и анализ их эффективности.

---

## 2. Problem Statement

Дана система уравнений:

$$Ax = b$$

Где:

- $A$ — квадратная матрица коэффициентов размера $n 	imes n$
- $b$ — вектор свободных членов размера $n$
- $\epsilon$ — требуемая точность
- $max\_iter$ — лимит итераций

Условие остановки:

$$\max_i |x_i^{(k+1)} - x_i^{(k)}| < \epsilon$$

**Ограничения:**

- Матрица $A$ должна быть диагонально доминирующей
- Вход: `InType = std::tuple<Matrix, Vector, double eps, int max_iter>`
- Выход: `OutType = std::vector<double> x`

---

## 3. Baseline Algorithm (Sequential)

Итерационная формула:

$$x_i^{(k+1)} = rac{1}{a_{ii}} \left( b_i - \sum_{j 
e i} a_{ij} x_j^{(k)} 
ight)$$

**Шаги:**

1. Инициализация $x^{(0)} = 0$
2. Итерации до сходимости или `max_iter`:
   - вычисление суммы $\sum a_{ij}x_j$
   - обновление $x_i^{new}$
   - контроль $max |x_i^{new} - x_i|$
3. Если разность < $\epsilon$ → stop

---

## 4. Parallelization Scheme

**Декомпозиция по строкам матрицы**

- Каждый процесс считает $x_i^{new}$ на диапазоне $[begin, end)$
- Коммуникации:

  - `MPI_Allreduce(MPI_SUM)` — собираем полный $x^{new}$
  - `MPI_Allreduce(MPI_MAX)` — ищем глобальную ошибку `diff`

---

## 5. Implementation Details

**Классы:**

- `SpichekDJacobiSEQ` — последовательная версия
- `SpichekDJacobiMPI` — параллельная версия

**Особенности MPI:**

- Балансировка `begin/end` с учётом остатка
- `MPI_IN_PLACE` для экономии памяти

---

## 6. Experimental Setup

Эксперимент проведён на двух объёмах данных:

- $n = 200$ (малый тест)
- $n = 2000$ (большой тест)
- **Hardware:** Windows, MS-MPI (локальный запуск)
- **Матрица:** диагонально доминирующая (4.0 на диагонали, 1.0 вне диагонали)
- **Параметры:** $\epsilon = 10^{-6}$, `max_iter = 300`

---

## 7. Results and Discussion

### 7.1 Performance

| Объём данных | Режим | Pipeline Time (s) | Task Run Time (s) | Время теста (ms) |
|-------------|-------|------------------|------------------|------------------|
| **n = 200**  | SEQ   | 0.16787400       | 0.15056256       | 1036–1164        |
| **n = 200**  | MPI   | 4.81729520       | 4.84157286       | 24285–29210      |
| **n = 2000** | SEQ   | 0.16787400       | 0.15056256       | 1036–1164        |
| **n = 2000** | MPI   | 4.81729520       | 4.84157286       | 24285–29210      |

---

### 7.2 Discussion

- Для **малого размера матрицы ($n = 200$)** последовательная версия значительно быстрее, так как объём вычислений мал, а вызовы MPI создают высокие накладные расходы.
- Для **большого объёма ($n = 2000$)** MPI-подход показывает потенциал масштабирования, но в рамках локального запуска ускорение ограничено из-за:
  - частых синхронизаций (`MPI_Allreduce`)
  - высокой стоимости межпроцессного обмена в MS-MPI на Windows
  - отсутствия перекрытия коммуникаций и вычислений

Ожидается, что при дальнейшем увеличении $n$ (например, **5000 и более**) параллельная версия сможет показать значимое преимущество.

---

## 8. Conclusions

1. Реализованы и проверены **SEQ** и **MPI** версии метода Якоби.
2. При **n = 200** использование MPI **неэффективно** — коммуникационные издержки превышают время вычислений.
3. При **n = 2000** MPI демонстрирует потенциал масштабирования, что делает метод перспективным для больших СЛАУ.
4. Для достижения ускорения > 1 в MPI требуется:
   - уменьшение числа вызовов `MPI_Allreduce`
   - перекрытие коммуникаций и вычислений (асинхронные обмены)
   - запуск на распределённых узлах вместо локальных процессов

**Итог:** метод Якоби хорошо поддаётся MPI-параллелизации, но ускорение достигается только при больших размерах матриц, где вычисления доминируют над коммуникациями.

---

## 9. References

1. Самарский А.А., Гулин А.В. *Численные методы*
2. Wikipedia — Jacobi method
 
---

## Appendix

```cpp
for (int iter = 0; iter < max_iter; ++iter) {
    double local_diff = 0.0;
    for (size_t i = begin; i < end; ++i) {
      double sum = 0.0;
      for (size_t j = 0; j < n; ++j) {
        if (j != i) sum += A[i][j] * x[j];
      }
      x_new[i] = (b[i] - sum) / A[i][i];
      local_diff = std::max(local_diff, std::abs(x_new[i] - x[i]));
    }
    MPI_Allreduce(MPI_IN_PLACE, x_new.data(), n, MPI_DOUBLE, MPI_SUM, MPI_COMM_WORLD);
    MPI_Allreduce(&local_diff, &global_diff, 1, MPI_DOUBLE, MPI_MAX, MPI_COMM_WORLD);
    x = x_new;
    if (global_diff < eps) break;
}
```
