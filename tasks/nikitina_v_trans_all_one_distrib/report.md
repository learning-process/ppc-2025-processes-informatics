# Отчет по задаче: Распределение и сбор данных (All-Reduce Binary Tree)

- **Студент:** Никитина Валерия Владимировна
- **Группа:** 3823Б1ФИ2
- **Вариант:** 3
- **Технология:** MPI, SEQ

## 1. Введение

В параллельном программировании эффективность коллективных операций (таких как редукция или рассылка данных) напрямую зависит от используемой топологии коммуникации. Наивные алгоритмы, где один процесс взаимодействует со всеми остальными линейно, создают "бутылочное горлышко" и плохо масштабируются.

Данная лабораторная работа посвящена реализации схемы **глобальной редукции (AllReduce)** с использованием логической топологии **бинарного дерева**. Такой подход позволяет распараллелить накладные расходы на передачу данных и сложение векторов, снижая латентность операции с $O(P)$ до $O(\log P)$, где $P$ — число процессов.

## 2. Постановка задачи

**Входные данные:** Вектор целых чисел `std::vector<int>`, инициализированный на каждом процессе.
**Выходные данные:** Результирующий вектор, содержащий поэлементную сумму векторов всех процессов. В текущей реализации итоговый результат формируется на корневом узле и рассылается обратно всем участникам (AllReduce).

**Требования:**
1.  Реализовать последовательную версию (SEQ) для базового сравнения.
2.  Реализовать параллельную версию (MPI) без использования встроенных коллективных операций (`MPI_Reduce`/`MPI_Allreduce`).
3.  Использовать топологию **бинарного дерева** для этапов сбора (Reduce) и рассылки (Broadcast) данных, чтобы минимизировать падение производительности при росте числа процессов.

## 3. Описание алгоритмов

### 3.1. Последовательный алгоритм (SEQ)
Последовательная реализация выполняет работу в рамках одного процесса. В контексте данной задачи она эмулирует поведение системы из одного узла, выполняя базовые операции над памятью (копирование входных данных).

*Сложность:* $O(N)$, где $N$ — размер вектора.

### 3.2. Параллельный алгоритм (MPI)
Вместо линейных схем используется алгоритм на основе логического бинарного дерева. Для процесса с рангом $i$:
*   **Родитель:** $(i - 1) / 2$
*   **Левый ребенок:** $2i + 1$
*   **Правый ребенок:** $2i + 2$

Алгоритм состоит из двух фаз:

1.  **Сбор вверх (Reduce):**
    *   Листовые процессы отправляют свои данные родителю.
    *   Промежуточные узлы принимают данные от детей, складывают их поэлементно (`std::ranges::transform` с `std::plus`) со своим вектором, а затем отправляют результат родителю.
    *   Процесс продолжается до корня (ранг 0).

2.  **Рассылка вниз (Broadcast):**
    *   Корневой процесс, получив итоговую сумму, отправляет её своим детям.
    *   Каждый узел ретранслирует полученные данные своим детям.

## 4. Экспериментальная часть

### 4.1. Конфигурация стенда
Замеры проводились в среде Docker на локальной машине.
*   **Компилятор:** GCC 14.2.0.
*   **Библиотека:** OpenMPI 4.1.
*   **Ресурсы:** Ограничение Docker-контейнера — 4 физических ядра.

### 4.2. Тестовые данные
Для тестов производительности использовался вектор типа `int` размером **20 000 000 элементов**.

### 4.3. Результаты измерений
Ниже приведены усредненные результаты времени выполнения (5 запусков для каждого случая). Ускорение ($S$) вычисляется как $T_{seq} / T_{mpi}$.

| Число процессов (P) | Время выполнения (сек) | Ускорение ($S$) | Эффективность ($E$) |
| :---: | :---: | :---: | :---: |
| **SEQ (1)** | **0.082** | 1.00 | 100% |
| **MPI (1)** | 0.085 | 0.96 | 96% |
| **MPI (2)** | 0.051 | 1.60 | 80% |
| **MPI (3)** | 0.046 | 1.78 | 59% |
| **MPI (4)** | 0.048 | 1.70 | 42% |
| **MPI (8)** | 0.065 | 1.26 | 15% |

### 4.4. Анализ результатов
1.  **Положительное ускорение:** В диапазоне от 2 до 4 процессов наблюдается снижение времени выполнения (ускорение до 1.78x). Это свидетельствует о том, что распределение вычислительной нагрузки (сложение векторов) и использование суммарной пропускной способности памяти нескольких ядер перекрывают накладные расходы на пересылку данных по MPI.
2.  **Пик производительности:** Оптимальное время достигается на 3-4 процессах, что соответствует количеству физических ядер, выделенных контейнеру (4 ядра).
3.  **Деградация на 8 процессах:** При запуске 8 процессов на 4 ядрах (oversubscription) происходит увеличение времени выполнения (0.065 сек) и резкое падение эффективности ($15\%$). Это связано с конкуренцией потоков за процессорное время, увеличением количества переключений контекста и ростом высоты коммуникационного дерева ($h=3$), что увеличивает латентность передачи данных.

## 5. Выводы

В ходе лабораторной работы была реализована топология бинарного дерева для задачи AllReduce:
1.  Реализована логика определения связей (родитель-потомок) в дереве процессов.
2.  Выполнен переход к агрегации данных (сложение векторов) в фазе сбора.
3.  Экспериментально подтверждена эффективность подхода: на доступных аппаратных ресурсах получено ускорение до 1.78 раз по сравнению с последовательной версией.
4.  Выявлен предел масштабируемости, обусловленный физическими ограничениями стенда (число ядер) и накладными расходами на коммуникацию при увеличении глубины дерева.

## 6. Приложение

Файловая структура проекта соответствует требованиям:
- `mpi/src/ops_mpi.cpp` — реализация класса `TestTaskMPI` (логика дерева).
- `seq/src/ops_seq.cpp` — реализация класса `TestTaskSEQ`.
- `tests/functional` — тесты корректности (GoogleTest).
- `tests/performance` — тесты производительности.

## 7. Источники

1.  MPI Forum. MPI: A Message-Passing Interface Standard. Version 3.1.
2.  Grama, Gupta, Karypis, Kumar. "Introduction to Parallel Computing". (Раздел о древовидных коммуникациях).