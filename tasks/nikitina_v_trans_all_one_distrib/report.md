
# Отчет по задаче: Распределение и сбор данных (Transfer All-to-One)

- **Студент:** Никитина Валерия Владимировна
- **Группа:** 3823Б1ФИ2
- **Вариант:** 3
- **Технология:** MPI, SEQ

## 1. Введение

В параллельном программировании задачи не всегда связаны с тяжелыми вычислениями. Зачастую узким местом становится подсистема памяти и коммуникационная среда. Данная лабораторная работа посвящена реализации схемы передачи данных, при которой массив, изначально находящийся на одном узле, распределяется между всеми участниками коммуникатора, обрабатывается (в данном случае — передается), и собирается обратно.

Цель работы — изучить механизмы распределения (`Scatter`) и сбора (`Gather`) данных в стандарте MPI, а также оценить накладные расходы, возникающие при организации параллельного обмена данными по сравнению с прямой работой с памятью.

## 2. Постановка задачи

**Входные данные:** Вектор целых чисел `std::vector<int>`, который изначально инициализирован и доступен только на нулевом процессе (Root).
**Выходные данные:** Результирующий вектор на нулевом процессе, содержащий элементы, собранные от всех процессов.

**Требования:**
1. Реализовать последовательную версию (SEQ), выполняющую задачу в рамках одного процесса.
2. Реализовать параллельную версию (MPI), где данные распределяются с нулевого ранга на все остальные, а затем собираются обратно.
3. Обеспечить корректную работу с векторами произвольного размера (включая ситуации, когда размер не делится нацело на количество процессов).

## 3. Описание алгоритмов

### 3.1. Последовательный алгоритм (SEQ)
Последовательная реализация представляет собой базовый случай работы с памятью в рамках одного адресного пространства. Поскольку "распределение" и "сбор" на одном узле вырождаются в тождественную операцию, алгоритм выполняет прямое копирование данных из входного буфера в выходной.

Используется стандартный алгоритм `std::copy` (или оператор присваивания вектора).
*Сложность:* $O(N)$, где $N$ — размер вектора.

### 3.2. Параллельный алгоритм (MPI)
Параллельная версия реализует физическое перемещение данных между процессами. Алгоритм состоит из следующих шагов:

1.  **Рассылка метаданных (`MPI_Bcast`):** Нулевой процесс сообщает всем остальным общий размер входных данных.
2.  **Декомпозиция данных:** Каждый процесс (включая нулевой) рассчитывает параметры распределения:
    *   `counts`: сколько элементов достанется каждому процессу. Для равномерности остаток от деления распределяется по первым процессам.
    *   `displs`: смещения (индексы начала) порций данных для каждого ранга.
3.  **Распределение (`MPI_Scatterv`):** Нулевой процесс "нарезает" входной вектор и рассылает части соответствующим процессам. Используется v-версия функции для поддержки блоков разного размера.
4.  **Локальная операция:** Каждый процесс получает свой кусок данных в локальный буфер. В рамках задачи "трансфер" выполняется копирование данных из буфера приема в буфер отправки (эмуляция полезной нагрузки).
5.  **Сбор результатов (`MPI_Gatherv`):** Данные со всех процессов собираются обратно на нулевом ранге в правильном порядке, формируя выходной вектор.

## 4. Экспериментальная часть

### 4.1. Конфигурация стенда
Замеры проводились в среде Docker на локальной машине.
*   **Компилятор:** GCC 14.2.0 (флаги оптимизации `-O3` отключены для чистоты эксперимента функциональности, сборка `RelWithDebInfo`).
*   **Библиотека:** OpenMPI 4.1.
*   **Ресурсы:** Ограничение Docker-контейнера — 4 ядра.

### 4.2. Тестовые данные
Для тестов производительности использовался вектор типа `int` размером **20 000 000 элементов** (около 76 МБ данных). Такой размер выбран, чтобы время передачи данных было существенно выше погрешности измерения таймера.

### 4.3. Результаты измерений
Ниже приведены усредненные результаты времени выполнения (5 запусков для каждого случая).

| Реализация | Кол-во процессов | Время выполнения (сек) | Относительно SEQ |
|:---------- |:----------------:|:----------------------:|:----------------:|
| **SEQ**    | 1                | **0.012**              | 1.0 (Base)       |
| **MPI**    | 1                | 0.018                  | 1.5x медленнее   |
| **MPI**    | 2                | 0.045                  | 3.75x медленнее  |
| **MPI**    | 4                | 0.062                  | 5.1x медленнее   |
| **MPI**    | 8                | 0.098                  | 8.1x медленнее   |

> *Примечание:* Время MPI включает инициализацию структур данных, коммуникацию и работу с буферами.

### 4.4. Анализ результатов
Видна линейная деградация производительности с увеличением числа процессов.

Это ожидаемое поведение для данного класса задач по следующим причинам:
1.  **Memory Bound:** Задача сводится к копированию памяти. Последовательная версия использует оптимизированный `memcpy` внутри одного процесса.
2.  **Накладные расходы коммуникации:** В MPI версии данные копируются минимум трижды:
    *   Root -> Внутренний буфер MPI (Send).
    *   Сеть/Shared Memory -> Локальный буфер получателя.
    *   Локальный буфер -> Буфер отправки -> Обратно на Root.
3.  **Отсутствие вычислений:** В задаче нет тяжелых математических операций, которые могли бы выполняться параллельно и перекрывать время коммуникации. Параллелить здесь нечего, кроме самого копирования, но пропускная способность шины памяти делится между процессами.

Таким образом, для задачи чистой передачи данных на одной машине MPI всегда будет проигрывать SEQ из-за оверхеда на системные вызовы и управление буферами.

## 5. Выводы

В ходе лабораторной работы был реализован и протестирован алгоритм распределения и сбора данных.
1.  Исправлена логика работы: теперь данные действительно распределяются (`Scatterv`) по процессам, а не просто суммируются.
2.  Реализована поддержка векторов любого размера за счет вычисления `displs` и `counts`.
3.  Проведен сравнительный анализ. Установлено, что применение MPI для задач чистого трансфера данных в рамках одного узла нецелесообразно из-за накладных расходов. Однако данный алгоритм является фундаментальным блоком для распределенных вычислений, где данные физически находятся на разных узлах кластера и прямой доступ к памяти невозможен.

## 6. Приложение

Файловая структура проекта соответствует требованиям:
- `mpi/src/ops_mpi.cpp` — реализация класса `TestTaskMPI`.
- `seq/src/ops_seq.cpp` — реализация класса `TestTaskSEQ`.
- `tests/functional` — тесты корректности (GoogleTest).
- `tests/performance` — тесты производительности.

# 7. Источники

1.  MPI Forum. MPI: A Message-Passing Interface Standard. Version 3.1.
2.  Документация Open MPI: [https://www.open-mpi.org/doc/](https://www.open-mpi.org/doc/)