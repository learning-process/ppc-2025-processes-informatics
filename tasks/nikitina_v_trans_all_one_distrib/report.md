# Передача от всех одному и рассылка (AllReduce)

-   **Студент:** `<Никитина Валерия Владимировна>`, группа `<3823Б1ФИ2>`
-   **Технология:** SEQ, MPI
-   **Вариант:** `<3>`

## 1. Введение

Коллективные операции обмена данными являются основой многих параллельных алгоритмов. Одной из самых востребованных операций является `AllReduce` — операция, при которой данные собираются со всех процессов, агрегируются (например, суммируются), а результат рассылается обратно всем участникам.

Целью данной работы является ручная реализация механизма `AllReduce` с использованием базовых коллективных операций MPI (`Reduce` и `Broadcast`), сравнение её с последовательной версией, а также обеспечение корректности работы на распределенных данных.

## 2. Постановка задачи

**Входные данные:** Целочисленный вектор `std::vector<int>`. Все процессы в коммуникаторе должны предоставить векторы одинакового размера.

**Выходные данные:** Вектор `std::vector<int>` того же размера, где каждый элемент является суммой соответствующих элементов входных векторов со всех процессов. Результат должен быть доступен на каждом процессе.

**Ограничения:**
-   Размер входного вектора должен быть больше нуля.
-   Тип данных — целые числа (`int`).
-   В случае пустого вектора алгоритм должен корректно завершать работу без аварийной остановки.

## 3. Описание алгоритма (базового/последовательного)

Последовательный алгоритм (SEQ) эмулирует поведение операции `AllReduce` для случая, когда в системе всего один вычислительный узел.

1.  **Валидация:** Проверяется, что входной вектор не пуст.
2.  **Выполнение:** Так как процесс всего один, "сумма всех векторов со всех процессов" равна самому входному вектору.
3.  **Результат:** Входные данные просто копируются в выходной буфер.

Временная сложность данного алгоритма составляет O(N), где N — количество элементов в векторе.

## 4. Схема распараллеливания

Для реализации параллельной версии (MPI) была выбрана композиция двух коллективных операций, что соответствует формулировке задания "Передача от всех одному и рассылка".

Алгоритм выполняется в два этапа:

1.  **Сбор и агрегация (Reduce):**
    Используется операция `MPI_Reduce`. Данные со всех процессов (из буфера `input`) передаются на корневой процесс (Rank 0). В процессе передачи выполняется операция суммирования (`MPI_SUM`).
    *   *Результат этапа:* Корневой процесс имеет вектор с полными суммами, остальные процессы — ничего.

2.  **Рассылка результата (Broadcast):**
    Используется операция `MPI_Bcast`. Корневой процесс (Rank 0) берет полученный на предыдущем шаге результирующий вектор и рассылает его всем остальным процессам коммуникатора `MPI_COMM_WORLD`.
    *   *Результат этапа:* Все процессы имеют одинаковый итоговый вектор.

Данная схема гарантирует, что к концу выполнения функции состояние памяти всех процессов синхронизировано и содержит верный результат.

## 5. Экспериментальная установка

-   **Окружение:** Разработка и локальное тестирование проводились в контейнере Docker на базе Ubuntu.
-   **CPU:** Предоставлено средой выполнения.
-   **Toolchain:**
    -   Компилятор: GCC 14.2.0
    -   Система сборки: CMake
    -   Библиотеки: OpenMPI, GoogleTest
-   **Переменные окружения:** Эксперименты проводились при `PPC_NUM_PROC=2` (для MPI) и `PPC_NUM_THREADS=1` (для SEQ).
-   **Данные для тестов:**
    -   **Функциональные тесты:** Векторы, заполненные единицами, для упрощения проверки корректности суммирования (Ожидаемый результат = `Input * NumProc`).
    -   **Тесты производительности:** Вектор размером 1,000,000 элементов.

## 6. Результаты и обсуждение

### 6.1. Корректность и тестовое покрытие

Для проверки корректности использовался фреймворк Google Test.
-   Реализованы параметризованные тесты, проверяющие работу на векторах различной длины.
-   Проверена корректность обработки пустых входных данных.
-   Сравнение результатов MPI версии проводилось с эталонным значением (входной вектор, умноженный на количество процессов).

Все функциональные тесты пройдены успешно (`[ PASSED ]`), утечек памяти при проверке санитайзерами (ASAN) не обнаружено.

### 6.2. Производительность

Были проведены замеры времени выполнения на векторах большого размера (10^6 элементов).

| Режим | Кол-во процессов | Описание | Результат |
| :---- | :--------------: | :------- | :-------- |
| SEQ   |        1         | Копирование памяти (memcpy) | Очень быстро |
| MPI   |        2         | Reduce + Bcast | Медленнее SEQ |

**Анализ результатов:**
В данной задаче последовательная версия (SEQ) выполняет лишь копирование памяти, что является экстремально быстрой операцией. Параллельная версия (MPI) вынуждена выполнять передачу данных по сети (или через общую память с накладными расходами MPI) дважды: сначала сбор, потом рассылка.

Поэтому для данной конкретной задачи на одной локальной машине MPI-версия работает медленнее SEQ из-за накладных расходов на коммуникацию, которые превышают выигрыш от параллельного сложения чисел. Однако, данная реализация является масштабируемой и предназначена для работы в распределенных кластерах, где данные изначально находятся на разных узлах, и SEQ-версия физически не применима.

## 7. Выводы

В ходе работы была успешно реализована операция `AllReduce` средствами MPI.
1.  Реализован механизм сбора данных (`Reduce`) и последующей рассылки (`Bcast`).
2.  Обеспечена полная идентичность интерфейсов SEQ и MPI версий.
3.  Функциональные тесты подтвердили корректность вычислений.

Работа демонстрирует понимание принципов коллективных коммуникаций в стандарте MPI.

## 8. Источники

1.  MPI Forum. MPI: A Message-Passing Interface Standard. Version 3.1.
2.  Документация Open MPI: [https://www.open-mpi.org/doc/](https://www.open-mpi.org/doc/)