# Топология сети "Звезда"
**Студент:** Зенин Антон Алексеевич.
**Группа:** 3823Б1ФИ1
**Технология:** SEQ | MPI
**Вариант:** 8 
---

## 1. Введение
В данной лабораторной работе рассматривается задача реализации виртуальной топологии сети **«Звезда»** с использованием технологии **MPI**.     

Топология «Звезда» характеризуется наличием одного центрального узла (центра), через который осуществляется передача данных между периферийными процессами. Цель работы — реализовать механизм передачи данных **от любого выбранного процесса к любому другому процессу** с соблюдением заданной топологии.    

В рамках работы были реализованы:  
- **MPI-версия** с ручной реализацией маршрутизации сообщений;  
- **SEQ-версия (последовательная)**, реализованная в виде заглушки и используемая для корректного прохождения CI и тестов.  


---

## 2. Постановка задачи

Требуется реализовать виртуальную топологию сети **«Звезда»**, обеспечивающую передачу данных между процессами со следующими условиями:  

- каждый процесс имеет уникальный ранг;  
- процесс с рангом `0` считается **центральным узлом**;  
- передача данных между двумя периферийными процессами осуществляется **через центральный узел**;  
- запрещено использование встроенных MPI-топологий:  
  - `MPI_Cart_create`,  
  - `MPI_Graph_create`.  

**Входные данные**  

- `InType = tuple<size_t src, size_t dst, vector<double> data>`  
- `src` - ранг процесса-источника;    
- `dst` - ранг процесса-получателя;  
- `data` - вектор передаваемых данных;  

**Выходные данные**

- `OutType = vector<double>`  
- на процессе `dst` - полученные данные;  
- на остальных процессах - пустой вектор;

---

## 3. Описание алгоритма SEQ (Sequential)

В SEQ-версии реализована заглушка, которая не выполняет фактической маршрутизации, так как последовательная реализация не имеет смысла для задачи распределенной маршрутизации в топологии "Звезда". SEQ-версия используется только для корректного прохождения CI. 

SEQ-версия не выполняет фактической маршрутизации и просто копирует входные данные `data` в выходной вектор.  


## 4. Описание алгоритма MPI  

В MPI-версии передача данных реализуется вручную с использованием точечных операций `MPI_Send` и `MPI_Recv`.   

Используется следующая логика маршрутизации: 

1. Если src == dst
    Передача данных не требуется, данные копируются локально на процессе-источнике.  
2. Если один из процессов является центральным (rank == 0)  
    Данные передаются напрямую мужду src и dst.  
3. Если оба процесса являются периферийными:  
    Данные передаются по маршруту: src -> center -> dst   

Таким образом, центральный процесс выполняет роль маршрутизатора сообщений.  


## 5. Особенности реализации  

## Структура кода 

common/include/common.hpp — InType, OutType, формат входных и выходных данных  
seq/src/ops_seq.cpp — SEQ-реализация (заглушка)    
mpi/src/ops_mpi.cpp — MPI-реализация топологии «Звезда»   
tests/functional/main.cpp — functional tests   
tests/performance/main.cpp — performance tests  

## Классы

- `ZeninATopologyStarMPI : BaseTask`   
- `ZeninATopologyStarSEQ : BaseTask`   


## 6. Окружение 


### Hardware
- CPU: Intel(R) Core(TM) i5-10400F CPU @ 2.90GHz    
- Cores: 6    
- RAM: 32 GB    
- OS: Windows 10 Pro  

### Toolchain
- Compiler: `C++20`     
- MPI: OpenMPI    
- Build type: Release  

## Данные  
- Используются 15 функциональных тестов с различными наборами данных: (3, 3), (2, 5), (10, 70), (1, 1), (1, 100), (100, 1), 
 (1000, 1000), (10, 2), (5, 3), (4, 5), (4, 3), (10000, 3), (3, 10000), (500, 1), (1, 500).  
- Тест на производительность использует сообщение kMsgSize размером **100000000**.   

---

## Результаты и выводы  

### 7.1 Корректность  
Корректность реализации проверялась с помощью функциональных тестов:  

- использовались 15 различных наборов входных данных;  
- проверялись сценарии:  
- src == dst,  
- передача между центральным и периферийным процессом,  
- передача между двумя периферийными процессами через центр;  

### 7.2 Производительность  

#### Результаты performance-тестов  

SEQ-реализация в измерениях не учитывалась, так как является заглушкой и не выполняет реальных вычислений.  

| Число процессов | pipeline (s) | task_run (s) | 
|-----------------|--------------|--------------|
|        1        | 0.08583624   |   0.07861074 |
|        2        | 0.16763316   |   0.17380862 |
|        4        | 0.17648376   |   0.17356230 |
|        8        | 0.22816498   |   0.25163826 |    


---  

## Анализ результатов  

Результаты измерений показывают, что при увеличении числа процессов время выполнения возрастает.

Это объясняется следующими факторами:  

- отсутствием вычислительно тяжёлых операций;
- доминированием коммуникаций (MPI_Send, MPI_Recv);
- ростом накладных расходов на синхронизацию при увеличении числа процессов.

---

## 8. Выводы  

- Реализована виртуальная топология сети «Звезда» с использованием MPI. 
- Маршрутизация сообщений выполнена вручную без использования встроенных MPI-топологий. 
- Корректность реализации подтверждена функциональными тестами. 
- SEQ-версия реализована в виде заглушки, что обосновано спецификой задачи. 

Реализация алгоритма маршрутизации в топологии "Звезда" с использованием MPI продемонстрировала свою корректность и работоспособность. Алгоритм успешно обрабатывает все базовые сценарии передачи сообщений: src = dst, между центральным и периферийным процессом, между двумя периферийными процессами через центр.  

---

## 9. Источники
- cppreference.com - https://en.cppreference.com  
- Документация по OpenMPI - https://www.open-mpi.org/doc  
- Лекции по параллельному программированию ННГУ им. Лобачевского  
- Практические занятия по параллельному программированию ННГУ им. Лобачевского  

---  

## 10. Приложение. Код MPI реализации  
```cpp 

bool ZeninATopologyStarMPI::RunImpl() {
  int world_rank = 0;
  int world_size = 0;
  MPI_Comm_rank(MPI_COMM_WORLD, &world_rank);
  MPI_Comm_size(MPI_COMM_WORLD, &world_size);

  const auto &in = GetInput();
  const size_t src = std::get<0>(in);
  const size_t dst = std::get<1>(in);
  const auto &data = std::get<2>(in);

  auto &out = GetOutput();
  out.clear();

  const int center = 0;
  const int tag = 0;

  const int src_rank = static_cast<int>(src);
  const int dst_rank = static_cast<int>(dst);
  const int center_rank = center;

  if (src_rank == dst_rank) {
    if (world_rank == src_rank) {
      out = data;
    }
    return true;
  }

  if (src_rank == center_rank || dst_rank == center_rank) {
    if (world_rank == src_rank) {
      MPI_Send(data.data(), static_cast<int>(data.size()), MPI_DOUBLE, dst_rank, tag, MPI_COMM_WORLD);
    } else if (world_rank == dst_rank) {
      out.resize(data.size());
      MPI_Recv(out.data(), static_cast<int>(out.size()), MPI_DOUBLE, src_rank, tag, MPI_COMM_WORLD, MPI_STATUS_IGNORE);
    }
    return true;
  }

  if (world_rank == src_rank) {
    MPI_Send(data.data(), static_cast<int>(data.size()), MPI_DOUBLE, center_rank, tag, MPI_COMM_WORLD);
  } else if (world_rank == center_rank) {
    std::vector<double> buf(data.size());
    MPI_Recv(buf.data(), static_cast<int>(buf.size()), MPI_DOUBLE, src_rank, tag, MPI_COMM_WORLD, MPI_STATUS_IGNORE);

    MPI_Send(buf.data(), static_cast<int>(buf.size()), MPI_DOUBLE, dst_rank, tag, MPI_COMM_WORLD);
  } else if (world_rank == dst_rank) {
    out.resize(data.size());
    MPI_Recv(out.data(), static_cast<int>(out.size()), MPI_DOUBLE, center_rank, tag, MPI_COMM_WORLD, MPI_STATUS_IGNORE);
  }
  

  return true;
}

```

  