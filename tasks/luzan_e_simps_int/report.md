# Вычисление многомерных интегралов с использованием многошаговой схемы (метод Симпсона).

- Student: Лузан Егор Андреевич, group 3823Б1ФИ3
- Technology: SEQ | MPI
- Variant: 9

## 1. Introduction
Цель работы - реализовать и сравнить две версии программы для численного вычисления двойного интеграла:
1. последовательную,
2. параллельную, использующую библиотеку **MPI**.

Вычисление кратных интегралов является вычислительно затратной задачей, часто возникающей в численных методах, математическом моделировании и физических расчётах.  
Использование параллельных вычислений позволяет сократить время выполнения при увеличении числа узлов интегрирования.

Ожидается ускорение MPI-версии по сравнению с последовательной реализацией.


## 2. Problem Statement
Требуется вычислить двойной интеграл вида:
$$
\int_a^b \int_c^d f(x, y)\,dy\,dx
$$

Численное интегрирование выполняется методом Симпсона.

**Входные данные:**
- границы интегрирования: 
    - по $x$: $a \space<\space b$,
    - по $y$: $c \space<\space d$;
- чётное число $n$ - количество малых прямоугольников, на которые будет разбита плоскость при вычислениях;
- подынтегральная функция $f(x, y)$.

**Выходные данные:**
- численное значение двойного интеграла.

**Ограничения:**
- число малых прямоугольников $n$ должно быть чётным;
- область интегрирования является прямоугольником;
- подынтегральная функция определена и непрерывна на всей области интегрирования.


## 3. Baseline Algorithm (Sequential)
Для последовательной версии используется метод Симпсона:
    - область интегрирования разбивается на сетку размером $(n+1) \times (n+1)$;
    - для каждого узла сетки вычисляется значение функции $f(x, y)$;
    - каждому узлу сопоставляется весовой коэффициент Симпсона;
    - итоговая сумма умножается на коэффициент $\frac{h_x h_y}{9}$.


## 4. Parallelization Scheme
### Распределение вычислений
- Параллелизация выполнена по индексу $i$, соответствующему координате $x$.
- Все процессы используют одинаковые параметры сетки и границы интегрирования.
- Каждый MPI-процесс вычисляет частичную сумму для своего диапазона значений $i$.
- Диапазон номеров узлов для каждого процесса вычисляется по следующим формулам:
    ```cpp
    rest = (n + 1) % size; 
    per_proc = (n + 1) / size;
    i_start = (rank * per_proc) + (rest > rank ? rank : rest);
    i_end = i_start + per_proc + (rest > rank ? 1 : 0);
    ```
`size` - количество процессов.
`rank` - ранг текущего процесса.

Выражение `(rest > rank ? rank : rest)` для `i_start` определяет, на сколько нужно сместить начало блока для учёта узлов, которые не удалось равно-распределить между всеми процессами. Если `rank` процесса больше числа таких узлов, то сместить начало на количество таких процессов, иначе сместить на `rank` (именно стольким процессам уже был выдан дополнительный узел для вычислений).

Выражение `(rest > rank ? 1 : 0)` для `i_end` определяет, остались ли ещё нераспределённые узлы. Если да, то выдаём текущему процессу ещё один.

Таким образом, у каждого процесса будет от `per_proc` до `per_proc + 1` узел. Причем `per_proc + 1` узел будет только у первых `rest` процессов.


### Роли процессов
- Root-процесс получает матрицу и её размеры.
- Root-процесс рассылает остальным процессам части матрицы для вычислений.
- Во время вычисления сумм процессы всех рангов имеют одинаковые задачи.
- После вычисления итоговый результат будет записан в процессе `rank = 0` с помощью `MPI_Reduce()`.


## 5. Implementation Details
- Для тестов производительности использован прямоугольник размерами $200 \times 400$, подыинтегральная фукнция с суммированием и различными математическими функциями (`sin`, `cos`, `exp`), количество узлов $(n+1)^2$ равное $401^2$.
- Данные об отрезках, количестве квадратов и подынтегральной функции рассылаются с 0 процесса. Далее каждый процесс сам вычисляет диапазон вычислений.
- Корректность работы программы проверена для малых (площади <$1$) и больших (площади >$100^2$) прямоугольников, для количества квадратов от $2^2$ до $400^2$. Также рассмотрены случаи вытянутых прямоугольников (одна из сторон по длине не менее чем в 10 раз меньше другой).


## 6. Experimental Setup
- Hardware/OS:
  - CPU: Intel Core i7-13620H; P-cores-6, E-cores-4.
  - RAM: 16 GB.
  - OS: Windows 11, x64.
- Toolchain:
  - CMake 3.28.3.
  - Компилятор: gcc (Ubuntu 13.3.0-6ubuntu2~24.04) 13.3.0.
  - Использовался Docker-контейнер с Ubuntu 24.04.2 LTS.
  - Режим сборки: Release.


## 7. Results and Discussion

### 7.1 Correctness
Корректность проверена с использованием GoogleTest на прямоугольниках различной площади, с различным количеством узлов, различными подынтегральными  функциями.

### 7.2 Performance

| Mode | Process count | Time, s | Speedup  | Efficiency |
| -    | -             | -       | -        | -          |
| seq  | 1             | 2.475   | 1.000    | N/A        |
| mpi  | 2             | 1.157   | 2.140    | 107.0%     |
| mpi  | 4             | 0.719   | 3.441    | 86.0%      |
| mpi  | 6             | 0.524   | 4.724    | 78.7%      |
| mpi  | 7             | 0.487   | 5.087    | 72.7%      |
| mpi  | 8             | 0.456   | 5.430    | 67.9%      |

При увеличении числа процессов происходит видимое увеличение производительности. Увеличение производительности однако, как видно из эффективности, не является линейным и уменьшается относительно числа процессов с увеличением числа последних.

## 8. Conclusions
В ходе работы была реализована программа для численного вычисления двойного интеграла методом Симпсона, а также её параллельная версия с использованием MPI.

Параллелизация выполнена путём распределения вычислений по узлам сетки интегрирования между MPI-процессами.  
При увеличении вычислительной нагрузки и числа узлов интегрирования MPI-версия демонстрирует ускорение по сравнению с последовательной реализацией.

Результаты показывают, что метод Симпсона хорошо поддаётся распараллеливанию и может эффективно использоваться в задачах численного интегрирования большой размерности.


## 9. References
1. Курс лекций ННГУ "Параллельное программирование для кластерных систем".
2. Стандарт MPI.