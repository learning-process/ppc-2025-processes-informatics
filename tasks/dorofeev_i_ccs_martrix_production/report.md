# Умножение разреженных матриц в формате CCS

**Student:** Дорофеев Иван Денисович, group 3823Б1ФИ1
**Technology:** SEQ | MPI
**Variant:** 4

---

## 1. Introduction

Умножение матриц является одной из базовых операций линейной алгебры и широко применяется в задачах научных вычислений, моделирования и обработки данных. При работе с большими матрицами часто встречается ситуация, когда большинство элементов равны нулю, что делает использование плотных алгоритмов неэффективным как по времени, так и по памяти.

В данной работе рассматривается умножение **разреженных матриц** с элементами типа `double`, представленных в **столбцовом формате хранения CCS (Compressed Column Storage)**. Этот формат позволяет компактно хранить только ненулевые элементы и эффективно выполнять обход по столбцам.

Целью работы является реализация и сравнение **последовательной (SEQ)** и **параллельной (MPI)** версий алгоритма умножения разреженных матриц, а также анализ их корректности и производительности на матрицах большого размера.

---

## 2. Problem statement

Требуется реализовать умножение двух разреженных матриц в формате CCS.

### Требования:

* элементы матриц имеют тип `double`;
* формат хранения — **CCS (Compressed Column Storage)**;
* размер матриц — **2000 × 2000**;
* структура матриц — **пятидиагональная**;
* реализовать версии:

  * **SEQ** — последовательную,
  * **MPI** — параллельную;
* ограничения на коммуникации отсутствуют;
* необходимо сравнить производительность:

  * SEQ,
  * MPI (2 процесса),
  * MPI (4 процесса).

### Входные данные:

* две разреженные матрицы размера 2000 × 2000 в формате CCS.

### Выходные данные:

* результирующая разреженная матрица в формате CCS, равная произведению входных матриц.

---

## 3. Baseline Algorithm (Sequential)

Последовательная версия реализует стандартное умножение разреженных матриц в формате CCS.

Алгоритм:

1. Матрицы хранятся в формате CCS:

   * массив ненулевых значений,
   * массив индексов строк,
   * массив указателей начала столбцов.
2. Для каждого столбца второй матрицы:

   * перебираются все ненулевые элементы;
   * для каждого такого элемента выполняется умножение на соответствующий столбец первой матрицы.
3. Результаты аккумулируются в структуре результата, также представленной в формате CCS.
4. Нулевые элементы в результирующей матрице не сохраняются.

Последовательная версия используется как базовая для оценки ускорения параллельных реализаций.

---

## 4. Parallelization Scheme (MPI)

MPI-версия использует **распараллеливание по столбцам**, что естественно соответствует формату CCS.

### Схема работы:

1. Столбцы второй матрицы делятся между процессами:

   * каждый процесс получает примерно одинаковое количество столбцов;
2. Каждый процесс:

   * выполняет умножение своей части столбцов второй матрицы на первую матрицу;
   * формирует локальную часть результирующей матрицы в формате CCS.
3. После завершения вычислений:

   * локальные результаты собираются на корневом процессе;
   * выполняется объединение столбцов в итоговую матрицу.

Коммуникация между процессами осуществляется стандартными средствами MPI, без дополнительных ограничений на использование коллективных операций.

---

## 5. Experimental Setup

* **Hardware / OS:**

  * CPU: 13th Gen Intel i5-13420H (12) @ 4.6 GHz
  * RAM: 16 GB
  * OS: Ubuntu 25.10 x86_64
  * Среда выполнения: Docker (Ubuntu noble / trixie)

* **Toolchain:**

  * CMake 3.28.3
  * g++ 13.3.0
  * OpenMPI
  * Тип сборки: `Release`

* **Test configuration:**

  * Размер матриц: 2000 × 2000
  * Тип матриц: пятидиагональные
  * Формат хранения: CCS

* **Modes tested:**

  * SEQ
  * MPI (2 процесса)
  * MPI (4 процесса)

Замеры производительности выполнялись с использованием встроенных performance-тестов (`ppc_perf_tests`) в режимах `pipeline` и `task_run`.

---

## 6. Results and Discussion

### 6.1 Correctness

Корректность реализации подтверждена модульными и функциональными тестами GoogleTest.

Проверяется, что:

* результат умножения совпадает для SEQ и MPI-версий;
* сохраняется корректный формат CCS;
* алгоритм корректно работает на матрицах большого размера (2000 × 2000).

Все тесты завершились успешно.

---

### 6.2 Performance

Для каждого режима использовалось среднее значение между `pipeline` и `task_run`.

#### Усреднённые времена выполнения:

* **SEQ**
  Усреднение двух запусков (при `-n 2` и `-n 4`):

[
T_{seq} = \frac{0.0020515442 + 0.0017208099 + 0.0020211697 + 0.0020744801}{4}
\approx 0.00197 \text{ s}
]

* **MPI (2 процесса)**

[
T_{mpi2} = \frac{0.0043799774 + 0.0059195242}{2}
\approx 0.00515 \text{ s}
]

* **MPI (4 процесса)**

[
T_{mpi4} = \frac{0.0115922458 + 0.0109390654}{2}
\approx 0.01127 \text{ s}
]

#### Таблица производительности:

| Mode    | Processes | Time (s) | Speedup | Efficiency |
| ------- | --------- | -------- | ------- | ---------- |
| **seq** | 1         | 0.00197  | 1.00    | —          |
| **mpi** | 2         | 0.00515  | 0.38    | 19%        |
| **mpi** | 4         | 0.01127  | 0.17    | 4%         |

---

## 7. Discussion

Результаты экспериментов показывают, что для рассматриваемой задачи параллельная версия оказывается медленнее последовательной.

Основные причины этого поведения:

* пятидиагональная матрица содержит очень малое количество ненулевых элементов;
* объём вычислений на каждый процесс невелик;
* накладные расходы на MPI-коммуникации превышают выигрыш от распараллеливания;
* дополнительное время тратится на сбор и объединение локальных CCS-структур.

Таким образом, задача является **память- и коммуникационно-ограниченной**, а не вычислительно интенсивной.

---

## 8. Conclusions

В ходе работы было реализовано умножение разреженных матриц в формате CCS в последовательной и параллельной (MPI) версиях.

Полученные результаты показывают, что:

* SEQ-версия является наиболее эффективной для разреженных пятидиагональных матриц;
* MPI-реализация корректна, но не даёт ускорения при малой плотности ненулевых элементов;
* распараллеливание имеет смысл для более плотных матриц или существенно больших размеров задач.

Работа демонстрирует важность учёта структуры данных и соотношения вычислений и коммуникаций при разработке параллельных алгоритмов.

---

## 9. References

1. "Параллельное программирование для кластерных систем", ИИТММ, ННГУ им. Лобачевского
2. [Open MPI Documentation](https://www.open-mpi.org/doc/)
3. [MPI Reference – Message Passing Interface | Microsoft Learn](https://learn.microsoft.com/en-us/message-passing-interface/mpi-reference)
4. [Matrix Computations, Gene H. Golub, Charles F. Van Loan]

