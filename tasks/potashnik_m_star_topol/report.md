# <Звезда>

- Student: Поташник Максим Ярославович, group 3823Б1ФИ3
- Technology: SEQ | MPI
- Variant: 8

## 1. Introduction
Топология "Звезда" представляет собой способ обмена данными между процессами, когда выбирается один центральный процесс, и все пересылки данных происходят через него.

В лабораторной работе ожидается реализация взаимодействия между процессами, имитирующая работу топологии "Звезда".

## 2. Problem statement
Нужно реализовать программу, в которой выполняется множество операций пересылки данных:
1. Процесс-отправитель отправляет данные на центральный процесс
2. Центральный процесс принимает данные, после чего отправляет их процессу-получателю
3. Процесс-получатель принимает данные от центрального процесса

В случае, если программа запускается на одном процессе, или запускается SEQ-версия программы, запускается код-заглушка, необходимый для прохождения CI-тестов.

### Входные данные:
Вектор целых чисел - множество данных, которые будут рассылаться между процессами.

### Выходные данные:
Одно целое неотрицательное число - сумма всех пересланных данных.

### 3. Baseline Algorithm (Sequential)
В качестве SEQ-версии программы реализована заглушка для прохождения CI-тестов.

### 4. Parallelization Scheme
Имеется вектор рассылаемых данных. Для каждого набора данных определяются два процесса, между которыми будет организовано взаимодействие:

```
inline std::pair<int, int> GetCyclicSrcDst(int world_size, int iter) {
  int num_other_processes = world_size - 1;
  int src = (iter % num_other_processes) + 1;
  int dst = ((iter + 1) % num_other_processes) + 1;
  return {src, dst};
}
```

Если процесс не является центральным, а также не выбран в качестве отправителя/получателя для текущего набора данных, то он ничего не делает.
Процесс-отправитель отправляет данные на центральный процесс с помощью MPI_Send:
```
int data = input[i];
MPI_Send(&data, 1, MPI_INT, star_center, i, MPI_COMM_WORLD);
```
Процесс-получатель принимает данные с центрального процесса с помощью MPI_Recv:
```
int data = 0;
MPI_Recv(&data, 1, MPI_INT, star_center, i, MPI_COMM_WORLD, MPI_STATUS_IGNORE);
```
Центральный процесс принимает данные с процесса-отправителя и отправляет их процессу-получателю:
```
int data = 0;
MPI_Recv(&data, 1, MPI_INT, src, i, MPI_COMM_WORLD, MPI_STATUS_IGNORE);
MPI_Send(&data, 1, MPI_INT, dst, i, MPI_COMM_WORLD);
```

После того, как для всех данных была совершена пересылка, подсчитывается сумма всех пересланных данных со всех процессов для проверки корректности выполнения работы:
```
int local_sum = 0;
for (int value : received_data) {
  local_sum += value;
}

int global_sum = 0;
MPI_Reduce(&local_sum, &global_sum, 1, MPI_INT, MPI_SUM, star_center, MPI_COMM_WORLD);
MPI_Bcast(&global_sum, 1, MPI_INT, star_center, MPI_COMM_WORLD);

GetOutput() = std::make_tuple(global_sum, 1);
```

### 5. Experimental Setup
- Hardware/OS: 12th gen Intel(R) Core(TM) i5-12450H, 8 ядер, 16 GB RAM, Windows 11 x64
- Toolchain: compiler, version, build type (Release/RelWithDebInfo)
    - Cmake 3.28.3
    - Компилятор: g++ (Ubuntu 13.3.0-6ubuntu2~24.04) 13.3.0
    - Использовался Docker-контейнер.
    - Режим сборки: Release.
- Data: Для замера производительности использовался вектор из целочисленных значений, генерируемых детерминированно. 

## 6. Results and Discussion

### 6.1 Correctness
Корректность работы проверена с помощью тестов Google Test на строках размерами: 1, 5, 10, 20, 100.
После выполнения основной части алгоритма (прохода по множеству запросов и их выполнение) подсчитывается сумма всех пересланных данных со всех процессов, после чего проверяется её корректность.

### 6.2 Performance
Подсчет производительности в задании вида "Топология Звезда" не имеет смысла, т.к. отсутствует SEQ-версия для сравнения.
Смысл задания состоит в том, чтобы реализовать взаимодействие между процессами по топологии звезда, а не в параллелизации алгоритма.

## 7. Conclusions
Было реализовано взаимодействие между процессами на основе топологии звезда. Корректность выполнения работы была проверена с помощью подсчета суммы всех пересылаемых данных.

## 8. References
1. "Параллельное программирование для кластерных систем" ННГУ им. Лобачевского, ИИТММ