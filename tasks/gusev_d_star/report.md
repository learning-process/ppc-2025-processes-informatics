# Звезда
- Студент: Гусев Дмитрий Алексеевич, 3823Б1ФИ1
- Технология: SEQ, MPI 
- Вариант: 8

---

## 1. Введение
Топология "Звезда" (Star Topology) — одна из фундаментальных схем организации взаимодействия в распределенных системах. Она характеризуется наличием центрального управляющего узла (Master/Hub), к которому подключены все остальные вычислительные узлы (Workers/Slaves).

В данной работе рассматривается реализация паттерна Master-Worker с использованием технологии MPI. Главная особенность реализации — строгая централизация: рабочие узлы не общаются друг с другом напрямую, а весь поток данных и управления проходит через центральный узел.

---

## 2. Постановка задачи
**Исходные данные**: Целое число $N$, определяющее размерность задачи.

**Цель**: Выполнить ресурсоемкие вычисления, имитирующие нагрузку $O(N^3)$, и получить итоговую контрольную сумму.

**Алгоритм**: Задача представляет собой тройной вложенный цикл, выполняющий арифметические операции. Это классический пример задачи типа "CPU-bound" (ограниченной скоростью процессора), которая идеально подходит для демонстрации эффективности распараллеливания.

**Требования**:
- Реализовать виртуальную топологию "Звезда" средствами MPI (без использования MPI_Cart_Create и MPI_Graph_Create).
- Обеспечить возможность масштабирования на произвольное число процессов.
- Гарантировать корректность сборки результатов от всех вычислительных узлов в центре.

---

## 3. Последовательный алгоритм
Последовательная версия выполняет полный перебор вложенных циклов на одном процессоре. Для имитации полезной нагрузки выполняются операции сложения и вычитания, зависящие от индексов итерации.

Алгоритм:
```cpp
result = 0
for i from 0 to N:
    for j from 0 to N:
        for k from 0 to N:
            // Имитация тяжелой работы
            result += (i + j + k)
            result -= (i + j + k)
```

Временная сложность: $O(N^3)$. При $N=500$ количество итераций составляет 125 миллионов, что создает существенную нагрузку на CPU.

---

## 4. Параллельная схема
- **Роли процессов**:
	- Rank 0 (HUB / Master): Управляющий узел. Не выполняет вычислительную работу во время фазы счета. Его задача — разбить пространство итераций внешнего цикла (по i) на диапазоны и раздать их рабочим.
	- Rank > 0 (SPOKES / Workers): Вычислительные узлы. Ожидают задачу от Мастера, выполняют её и возвращают результат. Прямое общение между Workers запрещено топологией.
- **Распределение данных**:
	- Общий объем работы $N$ делится на $P-1$ рабочих процессов (где $P$ — общее число процессов).
    - Базовый размер куска: chunk = N / workers_count
    - Остаток remainder = N % workers_count распределяется по единице между первыми процессами.
- **Коммуникация**:
	- Рассылка (Scatter phase): HUB отправляет сообщения типа [start_index, count] каждому воркеру индивидуально, используя MPI_Send.
    - Сбор (Gather phase): HUB принимает частичные суммы от каждого воркера через MPI_Recv и агрегирует их в итоговый результат.

---

## 5. Реализация
- **Структура проекта:**
	- `gusev_d_star/seq/include/ops_seq.hpp` и `gusev_d_star/seq/src/ops_seq.cpp` — однопоточная версия.
	- `gusev_d_star/mpi/include/ops_mpi.hpp` и `gusev_d_star/mpi/src/ops_mpi.cpp` — MPI-версия.
	- `gusev_d_star/common/include/common.hpp` — общие типы и интерфейсы.
- **Реализационные детали**:
	- **Декомпозиция.**  
	    Логика разбита на методы RunAsMaster и RunAsWorker, что упрощает чтение кода.
	- **Тэгирование.**  
	    Использованы константы kTagTask и kTagResult для разделения потоков управления.
	- **Оптимизация.**  
	    Внутренние вычисления избавлены от аллокаций памяти (std::vector удален), что перевело задачу из Memory-bound в чистый CPU-bound, обеспечив линейную масштабируемость.
	- **Маршрутизация.**  
		Реализована жесткая проверка рангов. Worker может отправлять данные только на HUB_RANK = 0.

---

## 6. Тестовая конфигурация
- **Оборудование**: 
	Процессор: Intel(R) Core(TM) Ultra 9 185H (16 ядер, 22 логических процессора)  
	ОЗУ: 32 ГБ
- **ОС:** Windows 10 IoT Enterprise Subscription 2009
- **Компилятор:** Microsoft Visual C++ версии 19.44.35220
- **MPI:** Microsoft MPI версии 10.1.12498.18
- **Параметры задачи:** $N = 500$ (примерно 125 млн операций).

---

## 7. Результаты экспериментов
### 7.1 Верификация
Корректность проверялась с помощью Google Test. Сценарии тестирования (всего 21 набор параметров):
- Микро-тесты: $N=1, 2$.
- Простые числа: $N=7, 13, 17, 29, 41, 53, 97$. Это стресс-тест для алгоритма распределения нагрузки (проверка обработки остатков при делении).
- Степени двойки: $N=32, 64, 128, 256$.
- Крупные задачи: $N=150, 200, 300, 400, 500$.

Все 42 функциональных теста (21 для MPI + 21 для SEQ) пройдены успешно. Результаты параллельной версии полностью совпадают с последовательной.

### 7.2 Измерения производительности
Эксперименты по оценке производительности выполнялись на конфигурации, описанной в разделе 6.

Измерения времени выполнения проводились для последовательной (SEQ) и параллельной (MPI) версий при различном количестве процессов. Результаты представлены в таблице ниже.

| Mode | Count | Time, s | Speedup | Efficiency |
|------|-------|---------|---------|------------|
| seq  |   1   | 0.41035 |   1.00  |     N/A    |
| mpi  |   2   | 0.40301 |   1.02  |     51%    |
| mpi  |   4   | 0.14129 |   2.90  |     73%    |
| mpi  |   8   | 0.06589 |   6.22  |     78%    |

**Анализ результатов:**
- 2 процесса (1 Master + 1 Worker): Время выполнения (0.403с) почти идентично последовательной версии (0.410с). Это подтверждает корректность архитектуры: в этом режиме реально работает только один вычислительный узел, а Мастер ожидает завершения.
- 4 процесса (1 Master + 3 Workers): Время сократилось почти в 3 раза (до 0.141с). Три рабочих узла эффективно разделили нагрузку, обеспечив ускорение 2.90x.
- 8 процессов (1 Master + 7 Workers): Время упало до 0.066с, что дает ускорение 6.22x. Эффективность выросла до 78%, что говорит об отличной масштабируемости алгоритма при увеличении числа рабочих узлов, так как накладные расходы на коммуникацию остаются низкими по сравнению с объемом вычислений.

---

## 8. Выводы
- Реализована виртуальная топология "Звезда" без использования графовых коммуникаторов MPI, исключительно за счет логики маршрутизации сообщений.
- Достигнута высокая эффективность параллельных вычислений на задаче $O(N^3)$.
- Подтверждена специфика схемы с выделенным мастером: она наиболее эффективна, когда количество рабочих узлов значительно превышает 1.
- Код успешно оптимизирован: удаление лишних аллокаций памяти позволило обрабатывать задачи размерности $N=500+$ за доли секунды.

---

## 9. Литература
- Материалы курса "Параллельное программирование"
- Справочная документация Open MPI
- Документация Microsoft MPI
- Документация Microsoft Visual C++

---

## 10. Приложение

```cpp
namespace gusev_d_star {

const int kTagTask = 0;
const int kTagResult = 1;

GusevDStarMPI::GusevDStarMPI(const InType &in) {
  SetTypeOfTask(GetStaticTypeOfTask());
  GetInput() = in;
  GetOutput() = 0;
}

bool GusevDStarMPI::ValidationImpl() {
  return (GetInput() > 0);
}

bool GusevDStarMPI::PreProcessingImpl() {
  GetOutput() = 2 * GetInput();
  return true;
}

bool GusevDStarMPI::RunImpl() {
  auto input = GetInput();
  int rank = 0;
  int size = 0;
  
  MPI_Comm_rank(MPI_COMM_WORLD, &rank);
  MPI_Comm_size(MPI_COMM_WORLD, &size);

  if (size == 1) {
    volatile int local_res = 0;
    for (int i = 0; i < input; i++) {
        for (int j = 0; j < input; j++) {
            for (int k = 0; k < input; k++) {
                local_res += (i + j + k);
                local_res -= (i + j + k);
            }
        }
    }
    GetOutput() += local_res;
    return true;
  }

  const int kHubRank = 0;

  if (rank == kHubRank) {
    RunAsMaster(size, input);
  } else {
    RunAsWorker(input);
  }

  return true;
}

bool GusevDStarMPI::PostProcessingImpl() {
  GetOutput() -= GetInput();
  return true;
}

void GusevDStarMPI::RunAsMaster(int size, int input) {
    int workers_count = size - 1;
    int chunk = input / workers_count;
    int remainder = input % workers_count;
    int current_start_index = 0;

    for (int dst_rank = 1; dst_rank < size; dst_rank++) {
        int count = chunk + (dst_rank <= remainder ? 1 : 0);
        int task_data[2] = {current_start_index, count};
        
        MPI_Send(task_data, 2, MPI_INT, dst_rank, kTagTask, MPI_COMM_WORLD);
        current_start_index += count;
    }

    for (int src_rank = 1; src_rank < size; src_rank++) {
        int worker_result = 0;
        MPI_Status status;
        
        MPI_Recv(&worker_result, 1, MPI_INT, src_rank, kTagResult, MPI_COMM_WORLD, &status);
        GetOutput() += worker_result;
    }
}

void GusevDStarMPI::RunAsWorker(int input) {
    int task_data[2];
    MPI_Status status;
    const int kHubRank = 0;

    MPI_Recv(task_data, 2, MPI_INT, kHubRank, kTagTask, MPI_COMM_WORLD, &status);
    
    int start_i = task_data[0];
    int count_i = task_data[1];
    int end_i = start_i + count_i;

    volatile int local_res = 0;
    
    for (int i = start_i; i < end_i; i++) {
        for (int j = 0; j < input; j++) {
            for (int k = 0; k < input; k++) {
                local_res += (i + j + k);
                local_res -= (i + j + k);
            }
        }
    }

    MPI_Send((void*)&local_res, 1, MPI_INT, kHubRank, kTagResult, MPI_COMM_WORLD);
}

}  // namespace gusev_d_star
```
