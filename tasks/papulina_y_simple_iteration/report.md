# Метод простой итерации

- Student: Папулина Юлия Андреевна, group 3823Б1ФИ3
- Technology: SEQ | MPI 
- Variant: 20

## 1. Введение
Метод простой итерации — это численный метод решения систем линейных алгебраических уравнений (СЛАУ). Он является итерационным методом, который преобразует исходную систему уравнений $Ax = b$ к виду $x = Bx + d$, где $B$ — некоторая матрица, построенная на основе $A$, а $d$ — вектор, полученный из $b$. Метод сходится при условии, что спектральный радиус матрицы $B$ меньше единицы.

В данной работе реализованы последовательная (SEQ) и параллельная (MPI) версии метода простой итерации для решения СЛАУ больших размерностей.

## 2. Постановка задачи

**Формальная постановка:** Решить систему линейных уравнений $Ax = b$, где $A$ — квадратная матрица размера $n \times n$, $b$ — вектор правой части длины $n$, $x$ — искомый вектор решения.

**Входные данные:**
- Размерность системы $n \geq 1$
- Матрица коэффициентов $A$ размерности $n \times n$ (вектор размером $n^2$)
- Вектор правой части $b$ размерности $n$

**Выходные данные:**
- Вектор решения $x$ размерности $n$

**Ограничения:**
- Матрица $A$ должна быть невырожденной (определитель $\neq 0$)
- Для гарантированной сходимости требуется диагональное преобладание: $|a_{ii}| > \sum_{j \neq i} |a_{ij}|$ для всех $i$
- Метод сходится при условии $\|B\| <= 1$, где $B = I - D^{-1}A$, $D$ — диагональная часть $A$

## 3. Последовательная версия (Baseline)

Базовый алгоритм реализует классический метод простой итерации, преобразуя исходную систему $Ax = b$ к виду $x = Bx + d$, где:
- $B$ — матрица, элементы которой вычисляются как $b_{ij} = -a_{ij} / a_{ii}$ для $i \neq j$ и $b_{ii} = 0$
- $d$ — вектор, элементы которого вычисляются как $d_i = b_i / a_{ii}$

**Алгоритм:**

1. **Преобразование системы**: $Ax = b \rightarrow x = Bx + d$
2. **Инициализация начального приближения**: $x^{(0)} = d$
3. **Итерационный процесс** (для $k = 0, 1, 2, \dots$):
   - Вычисление нового приближения: $x^{(k+1)} = Bx^{(k)} + d$
   - Проверка условия сходимости: $\|x^{(k+1)} - x^{(k)}\|_2 < \varepsilon$
   - Если условие выполняется — возврат решения $x^{(k+1)}$
   - Иначе продолжение итераций до достижения максимального числа шагов
## 4. Параллельная версия

### 4.1. Разделение данных
Данные распределеяются следующим образом:
- Количество строк на процесс: `rows_for_proc = n / procNum`
- Остаток: `remainder = n % procNum`
- Процессы с рангами `proc_rank < remainder` получают на одну строку больше

Каждый процесс работает со своими локальными данными:
- `local_a_` — блок матрицы A размером `local_rows_count × n`
- `local_b_` — блок вектора b размером `local_rows_count`
- `local_b_matrix` — преобразованная матрица B для локальных строк
- `local_d` — преобразованный вектор d для локальных строк

### 4.2. Взаимодействие процессов
**На этапе подготовки данных (`PreProcessingImpl`):**
1. `MPI_Bcast` — широковещательная рассылка параметров распределения
2. `MPI_Scatterv` — распределение матрицы A и вектора b по процессам

**На каждой итерации (`RunImpl`):**
1. Каждый процесс вычисляет новые значения `local_x_new` для своих строк
2. `MPI_Gatherv` — сбор локальных решений на процессе 0 в глобальный вектор `x_new`
3. `MPI_Bcast` — рассылка обновленного глобального решения всем процессам для следующей итерации
4. `MPI_Allreduce` — вычисление глобальной нормы разности для проверки сходимости (используется `MPI_SUM` для суммирования локальных норм)

**Особенности реализации:**
- Используется `MPI_Gatherv` вместо `MPI_Allgatherv` для сбора результатов только на процессе 0 с последующей рассылкой через `MPI_Bcast`
- Норма вычисляется децентрализовано: каждый процесс вычисляет сумму квадратов для своих строк, затем выполняется глобальное суммирование через `MPI_Allreduce`
- Проверка условия сходимости выполняется на всех процессах одновременно благодаря использованию `MPI_Allreduce`
- Матрица преобразования B вычисляется локально каждым процессом для своих строк


## 5. Детали реализации

### 5.1. Файловая структура проекта
papulina_y_simple_iteration_method/  
├── common/include/common.hpp  
├── seq/include/ops_seq.hpp  
├── seq/src/ops_seq.cpp  
├── mpi/include/ops_mpi.hpp  
├── mpi/src/ops_mpi.cpp  
├── tests/functional/main.cpp  
├── tests/performance/main.cpp  
└── data/ТестовыеДанные

### 5.2. Ключевые классы и функции

**Классы:**

1. **`PapulinaYSimpleIterationSEQ`** — класс последовательной реализации
   - **Вспомогательные статические методы:**
     - `ComputeNewX()` — вычисление нового приближения по формуле $x^{(k+1)} = Bx^{(k)} + d$
     - `CalculateNormB()` — вычисление нормы матрицы B для анализа сходимости
     - `GetDetermCheckingResult()`, `DetermChecking()` — проверка невырожденности матрицы
     - `GetDiagonalDominanceResult()`, `DiagonalDominance()` — проверка диагонального преобладания
     - `FindAndSwapRow()` — перестановка строк матрицы

2. **`PapulinaYSimpleIterationMPI`** — класс параллельной реализации
   - **Вспомогательные методы:**
     - `PrepareLocalMatrices()` — подготовка локальных матриц B и d для каждого процесса
     - `CalculateGatherParameters()` — вычисление параметров для операции `MPI_Gatherv`
     - `CalculateNormB()` — вычисление нормы матрицы B
     - `FindAndSwapRow()` — перестановка строк матрицы
     - `DetermChecking()` — проверка невырожденности
     - `DiagonalDominance()` — проверка диагонального преобладания

**Данные классов:**

- **Общие данные:**
  - `A_` — матрица коэффициентов системы
  - `b_` — вектор правой части
  - `result_` — результирующий вектор решения
  - `n_` — размерность системы
  - `steps_count_` — максимальное число итераций (100000)
  - `eps_` — точность решения (1e-7)

- **MPI-специфичные данные:**
  - `procNum_` — количество процессов MPI
  - `local_a_` — локальный блок матрицы A для текущего процесса
  - `local_b_` — локальный блок вектора b для текущего процесса

**Распределение матриц:** Матрицы A и B распределены по процессам, что снижает локальную память с n² до ~n²/p

## 6. Экспериментальное окружение

**Hardware/OS:**
- **CPU:** Intel Core i5-11400H (6 cores, 12 threads, 2.70 GHz base frequency)
- **RAM:** 16.0 GB DDR4
- **Storage:** SSD 512 GB
- **OS:** Windows 10 

**Toolchain:**
- **Compiler:** Microsoft Visual C++ 2019 (MSVC 19.29.30153)
- **MPI Implementation:** Microsoft MPI Version10.1.12498.52
- **Build System:** CMake 3.30.3
- **Build Type:** Release

**Environment:**
- **PPC_NUM_PROC:** 1, 2, 3, 4, 6 

**Генерация тестовых данных:** Тестовая матрица генерируется по следующему алгоритму:
   - Диагональные элементы: aᵢᵢ = 5.0
   - Элементы на соседних диагоналях (±1): aᵢ,ᵢ±₁ = 2.0
   - Элементы на вторых диагоналях (±2): aᵢ,ᵢ±₂ = 0.5
   - Вектор b вычисляется как A × x, где x = [1, 1, ..., 1]ᵀ, что гарантирует известное точное решение

   Данная матрица обладает слабым диагональным преобладанием, но имеет norm_b = 1, что приводит к предупреждениям о возможных проблемах сходимости, но не препятствует нахождению решения.

**Data:**  
- матрица размера 200 на 200 с слабым диагональным преобладанием(~100000 итераций)

## 7. Результаты

### 7.1 Корректность
Корректность реализации подтверждается следующими аспектами:
1. **Всестороннее тестирование:** Реализация протестирована на разнообразных тестовых наборах, включающих:
   - **Малые системы** (1×1, 2×2, 3×3, 4×4) — проверка базовой функциональности
   - **Средние системы** (5×5, 8×8, 10×10) — проверка масштабируемости
   - **Специальные матрицы:**
     - матрица с диагональным преобладанием
     - трехдиагональная матрица
     - матрица с чередующимися знаками
     - матрица с отрицательными элементами
     - несимметричная матрица
     - матрица с большими диагональными элементами
     - матрица с быстрой сходимостью
     - матрица со средней скоростью сходимости

2. **Сравнение с эталонным решением:** все результаты сравнены с эталонными решениями с точностью 1e-7.
### 7.2 Производительность
Время выполнения для матрицы с плохой сходимостью, n=200 (~100000 итераций)

| Mode | Count | Time, s | Speedup | Efficiency |
|------|-------|---------|---------|------------|
| seq  | 1     | 17.20   | 1.00    | N/A        |
| mpi  | 2     | 12.80   | 1.34    | 67.2%      |
| mpi  | 3     | 10.36   | 1.66    | 55.3%      |
| mpi  | 4     | 9.79    | 1.76    | 43.9%      |
| mpi  | 6     | 10.49   | 1.64    | 27.3%      |

**Анализ результатов:**
**Положительное ускорение:** MPI версия демонстрирует ускорение относительно последовательной реализации для всех конфигураций. Максимальное ускорение 1.76× достигается при использовании 4 процессов.
**Анализ узких мест:**
1. **Коммуникационные накладные расходы:** Основное узкое место — операция `MPI_Gatherv` + `MPI_Bcast` на каждой итерации. Сбор локальных результатов со всех процессов и их рассылка требует O(n) коммуникаций на итерацию, что становится существенным фактором при увеличении числа процессов.

2. **Глобальная синхронизация:** Итерационный характер алгоритма требует полной синхронизации всех процессов после каждой итерации для обновления вектора решения. Это ограничивает возможности асинхронного выполнения.

## 8. Заключение
В ходе выполнения работы были успешно реализованы и проанализированы последовательная (SEQ) и параллельная (MPI) версии метода простой итерации для решения систем линейных алгебраических уравнений.
**Основные результаты:**

1. **Функциональная корректность:** обе реализации прошли всестороннее тестирование на разнообразных тестовых наборах, демонстрируя правильность работы для матриц различных типов и размерностей. SEQ и MPI версии дают идентичные результаты с точностью до заданного ε.

2. **Параллельная реализация:** разработана эффективная схема распараллеливания с использованием MPI, включающая:
   - эффективное распределение строк матрицы между процессами
   - минимизацию коммуникаций через локальное преобразование матриц

3. **Производительность:** параллельная версия демонстрирует положительное ускорение до 1.76× при использовании 4 процессов для матрицы размерности n=200. 
## 9. Источники
1. Microsoft MPI : документация [Электронный ресурс] // Microsoft Learn. – URL: https://learn.microsoft.com/ru-ru/message-passing-interface/microsoft-mpi (дата обращения: 03.11.2025).
2. Сысоев А. В. Курс лекций по параллельному программированию
