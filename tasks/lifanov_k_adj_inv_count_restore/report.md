# Нахождение числа нарушений упорядоченности соседних элементов вектора (SEQ + MPI)

**Студент:** Лифанов Кирилл Максимович, группа 3823Б1ФИ2  
**Технология:** SEQ + MPI  
**Вариант:** №6

---

## 1. Введение

Цель работы — разработать алгоритм подсчёта количества *соседних инверсий* в массиве целых чисел и исследовать его параллельную версию на основе MPI.

Определение: **Соседняя инверсия** — пара элементов `a[i], a[i+1]`, нарушающая порядок неубывания:

```
если a[i] > a[i+1], то существует adjacent inversion
```
---

## 2. Постановка задачи

Задано: массив целых чисел `v` длины `n`.

Требуется вычислить:

```
count = |{ i | v[i] > v[i+1], 0 ≤ i < n−1 }|
```

### Ограничения

- `n ≥ 2`;
- массив не должен быть пустым;
- элементы — тип `int`;
- результаты SEQ и MPI реализаций должны совпадать.

---

## 3. Последовательная реализация (SEQ)

```cpp
result = 0
for i in 0..n-2:
    if a[i] > a[i+1]:
        result++
```

Алгоритм выполняет линейный проход по массиву и имеет сложность **O(n)**.

---

## 4. Параллельная реализация (MPI)

MPI-версия распределяет *пары соседних элементов* между процессами.

### 4.1 Декомпозиция данных

Всего пар:

```
total_pairs = n - 1
```

При `p` процессах:

```
base = total_pairs / p
rem  = total_pairs % p
local_pairs(rank) = base + (rank < rem ? 1 : 0)
```

Для корректного анализа границ каждый процесс получает:

```
sendcounts[rank] = local_pairs + 1
```

То есть передаётся массив длиной на один элемент больше числа пар.

### 4.2 Рассылка данных

Используется `MPI_Scatterv`.  
Смещения рассчитываются как:

```
displs[0] = 0
displs[r] = displs[r-1] + sendcounts[r-1] - 1
```

Такая схема даёт **перекрытие на один элемент**, что позволяет корректно учитывать инверсии на стыках блоков.

### 4.3 Локальный подсчёт

Каждый процесс считает инверсии в своём блоке:

```cpp
for (i = 0; i + 1 < local_size; ++i)
    if (local[i] > local[i+1]):
        local_inv++
```

### 4.4 Сбор результата

Используются:

- `MPI_Reduce` — суммирование локальных результатов,
- `MPI_Bcast` — рассылка результата всем процессам.

---

## 5. Корректность и тестирование

Функциональные тесты проверяли:

- возрастающие массивы;
- убывающие массивы;
- повторяющиеся значения;
- отрицательные числа;
- корректный учёт стыков блоков;
- совпадение SEQ и MPI.

Все тесты успешно пройдены.

---

## 6. Производительность (100 млн элементов)

Фактические результаты, полученные в perf-тестах:

| Режим | Процессы | Время (с)    | Ускорение S(p) | Эффективность E(p)  |
|-------|----------|--------------|----------------|---------------------|
| SEQ   | 1        | 0.0232829    | 1.00           | —                   |
| MPI   | 2        | 0.1465371    | 0.159          | 7.95%               |
| MPI   | 4        | 0.1147471    | 0.203          | 5.08%               |

### Интерпретация результатов

Распараллеливание даёт малый процент эффективности.

Причины:

- алгоритм обладает **низкой вычислительной плотностью** (одно сравнение на элемент);
- `MPI_Scatterv` имеет высокие коммуникационные затраты;
- объём данных очень большой, а вычислений мало;
- Высокие накладные расходы.

Коммуникации и синхронизации значительно дороже, чем линейный проход по массиву, поэтому MPI-версия неизбежно медленнее SEQ.

---

## 7. Заключение

В ходе работы:

- разработаны SEQ и MPI реализации подсчёта соседних инверсий;
- использована корректная схема параллельного разбиения с перекрытием одного элемента;
- функциональная корректность подтверждена тестами;
- произведён анализ производительности, показавший, что для задач с низкой вычислительной плотностью MPI не обеспечивает ускорение.

---

## 8. Источники

1. P. Pacheco — *Parallel Programming with MPI*.  
2. Kumar V. — *Introduction to Parallel Computing*.
