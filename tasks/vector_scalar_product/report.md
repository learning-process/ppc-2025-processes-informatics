# Отчет по лабораторной работе №1
## "Скалярное произведение векторов"

**Студент:** Шеленкова Мария Сергеевна  
**Группа:** 3823Б1ФИ1  
**Вариант:** 9

### 1. Введение
**Мотивация.** Скалярное произведение лежит в основе множества алгоритмов линейной алгебры и анализа данных, поэтому важно исследовать его последовательную и параллельную реализации.  
**Проблематика.** Операция элементарна и содержит мало вычислений на одну итерацию. Коммуникационные накладные расходы могут превысить выигрыш от распараллеливания.  
**Ожидаемый результат.** Предполагалось, что параллельная версия окажется эффективной только для очень длинных векторов; на малых размерах преимущество сохранит последовательный алгоритм.

### 2. Постановка задачи
**Задано:** два вещественных вектора одинаковой длины.  
**Требуется:** вычислить их скалярное произведение.  
**Входные данные:** массивы `double a[i]`, `double b[i]`, `size_t n`.  
**Выходные данные:** значение `double result`.

### 3. Последовательный алгоритм
1. Проверяются корректность входных данных и равенство размеров векторов.
2. Используется стандартная функция `std::inner_product`, последовательно перемножающая пары элементов и суммирующая результат.
3. Алгоритм имеет линейную сложность `O(n)` и использует константный объём дополнительной памяти.

### 4. Схема распараллеливания
**Декомпозиция данных.** Индексный диапазон разделяется на блоки по числу процессов. Первые `n % P` процессов получают на один элемент больше.  
**Коммуникация.** Корневой процесс рассылает длину векторов и параметры распределения с помощью `MPI_Bcast`. После локальных вычислений частичные суммы сводятся к корню через `MPI_Reduce` с операцией `MPI_SUM`; при необходимости результат снова транслируется `MPI_Bcast`. Дополнительных синхронизаций не требуется.

### 5. Экспериментальная установка
* Процессор: 13th Gen Intel(R) Core(TM) i9-13980HX (24 физических ядра, 32 логических потока).
* Оперативная память: 32 GB.
* Операционная система: Microsoft Windows 11 Pro.
* Компилятор: MSVC 19.40, конфигурация Release x64.
* MPI-библиотека: Microsoft MPI 10.1.12498.52.

### 6. Результаты и обсуждение
#### 6.1. Корректность
Проверка выполнена командой `ppc_func_tests.exe --gtest_filter=*vector_scalar_product*`. Все тесты завершились успешно; значения параллельной версии совпадают с последовательным эталоном.

#### 6.2. Производительность
Измерения проводились с помощью `ppc_perf_tests.exe`. Последовательный вариант запускался на одном процессе, параллельный - на восьми (`mpiexec -n 8`).

| Реализация | Процессов | Режим теста | Время, сек |
|-----------:|----------:|-------------|-----------:|
| seq        | 1         | pipeline    | 0.0000927 |
| seq        | 1         | task_only   | 0.0000899 |
| mpi        | 8         | pipeline    | 0.0006886 |
| mpi        | 8         | task_only   | 0.0000111 |

В режиме `task_only` MPI-ядро отрабатывает быстрее благодаря распределённой сумме, однако полная конвейерная цепочка медленнее из-за затрат на передачу данных и синхронизацию. Итоговое ускорение для заданного размера отсутствует.

### 7. Выводы
Последовательный алгоритм остаётся предпочтительным для небольших векторов, где коммуникационные затраты доминируют. Реализация MPI корректна и готова к масштабированию на большие объёмы данных, при которых вычислительная нагрузка превысит стоимость обменов.
