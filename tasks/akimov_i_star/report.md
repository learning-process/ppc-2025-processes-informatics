# Виртуальная топология «Звезда» (Star topology)

- Student: Акимов Илья Александрович, group 3823Б1ФИ2  
- Technology: SEQ, MPI  
- Variant: 8

## 1. Introduction
Цель работы — реализовать модель обмена сообщениями в виртуальной топологии «Звезда» и сравнить последовательную (SEQ) и параллельную на MPI реализации. Ожидается корректная доставка сообщений от любого процесса к любому другому через центральный узел (hub = rank 0), а также оценка корректности и производительности.

## 2. Problem Statement
Требуется создать программу, которая реализует передачу сообщений между процессами по виртуальной топологии «Звезда».

**Формализация:**
- Вход: текстовый набор команд (`InType = std::vector<char>`), каждая строка в формате `send:<src>:<dst>:<message>`.
- Интерпретация: из каждой строки извлекается отправитель `src`, получатель `dst` и сообщение `message`.
- Поведение: сообщение должно быть доставлено от процесса `src` к процессу `dst` через центральный узел (rank 0). Центр принимает все входящие от листьев и форвардит их на целевой rank (или сохраняет локально, если `dst == 0`).
- Выход: `OutType = int` — на каждом процессе число сообщений, доставленных этому процессу.

**Ограничения и предположения:**
- Номера рангов в командах предполагаются корректными (0 ≤ rank < size). Некорректные строки игнорируются.
- Текущая MPI-реализация использует блокирующие `MPI_Send`/`MPI_Recv`.
- Для тестов вход предварительно broadcast'ится всем ранкам (root передаёт весь список команд).

## 3. Baseline Algorithm (Sequential)
Последовательная версия (`AkimovIStarSEQ`) выполняет симуляцию протокола «звезда» без MPI:
- Парсинг входа: читается каждая строка `send:<src>:<dst>:<msg>`.
- Для каждой команды симулируется доставка: если `dst == my_simulated_rank` (в простом эталоне — проверяют только dst==0 для тестов), то увеличивается счётчик принятых сообщений.
- SEQ-версия служит эталоном корректности (проверка, что MPI-версия даёт те же результаты) и для измерения базового времени обработки парсинга и логики внутри одного процесса.

Пошагово (SEQ):
1. Парсинг входа в `std::vector<Op>`; `Op = { int src, int dst, std::string msg }`.
2. Для каждого `Op` проверка `dst` и инкремент локального счётчика, если `dst` совпадает с текущим интересующим рангом (для тестов обычно центр rank 0).
3. Алгоритм линейный по числу команд и суммарной длине сообщений: `O(m + L)`.

## 4. Parallelization Scheme

### MPI — data distribution и коммуникации
- Распространение инструкций: `MPI_Bcast` от root (rank 0) — сначала длина, затем буфер символов, чтобы все ранки локально распарсили команды. (Примечание: такой подход прост для тестов; для масштабируемости возможна раздача команд частично.)
- Подготовка: каждый ранк формирует:
  - `outgoing` — команды с `src == myrank`
  - `expected_incoming_count` — число команд с `dst == myrank`
- Отправка:
  - Листовые ранки (`rank != 0`) выполняют для каждой своей команды: посылают заголовок (`std::array<int,2> {dst, payload_len}`) на центр, затем payload (если len>0).
- Центр:
  - Обрабатывает свои локальные исходящие — если `dst == 0`, считает локально; если `dst != 0`, отправляет целевому ранку.
  - Принимает от других (`MPI_Recv` с `MPI_ANY_SOURCE`) указанное число заголовков (известно через `MPI_Allreduce`), извлекает `dst` и payload; если `dst==0` — считает локально, иначе пересылает дальше на `dst`.
- Синхронизация:
  - `MPI_Allreduce` используется для определения общего количества отправленных сообщений (чтобы центр знал, сколько принимать).
  - Прямых фан-аутов/барьеров при завершении не требуется — протокол детерминирован по числам ожидаемых сообщений.

### Роли рангов
- `rank 0`: hub — принимает от листов и пересылает целевым ranks; также обрабатывает собственные сообщения.
- `rank != 0`: листья — отправляют свои outgoing на hub и ожидают пересланных hub сообщений, количество которых равно `expected_incoming_count`.

### Диаграмма
```
  leaf1 ----\
             \
  leaf2 ----> [ hub (rank 0) ] ---> leaf3
             /
  leaf4 ----/
```

## 5. Implementation Details

### Кодовая структура
- `common/include/common.hpp` — типы `InType`, `OutType`, `BaseTask`.
- `seq/include/ops_seq.hpp`, `seq/src/ops_seq.cpp` — SEQ-реализация:
  - Парсинг входа в `std::vector<Op> ops_`.
  - Подсчёт сообщений (нацелено на проверку dst==0 при тестах).
- `mpi/include/ops_mpi.hpp`, `mpi/src/ops_mpi.cpp` — MPI-реализация:
  - `AkimovIStarMPI::PreProcessingImpl()` — `MPI_Bcast` входа, парсинг.
  - `RunImpl()` — логика hub/leaf, вспомогательные функции для send/recv (используются `std::array<int,2>` для заголовков и `.data()` для передачи буферов).
- `tests/functional/main.cpp` — функциональные тесты: root читает `data/messages.txt`, высчитывает ожидаемое число команд с `dst == 0` и сравнивает с `GetOutput()` на rank 0.
- `tests/perfomance/main.cpp` — генерация большого файла сообщений и perf-тесты.

### Важные допущения и corner-cases
- Некорректные строки игнорируются (поддерживающий безопасный парсинг).
- Текущая реализация broadcast'ит весь список команд всем ранкам; это удобно для проверки, но не оптимально по памяти при масштабировании.
- Возможные проблемы: большое число маленьких сообщений может вызвать сильные сетевые накладные; блокирующие `MPI_Send`/`MPI_Recv` потенциально могут вызвать deadlock в особых сценариях, поэтому для промышленного решения рекомендованы неблокирующие операции.

### Память
- SEQ: требует O(L) памяти (весь вход и список `ops`).
- MPI: в текущей реализации каждый ранк хранит копию входа (следовательно O(L) на каждый процесс). Можно улучшить — делать root-local storage и раздавать каждому ранку только нужные команды.

## 6. Experimental Setup

**Hardware:**
- CPU: AMD Ryzen 7 6800HS
- Cores/threads: 8 / 16
- RAM: 32 GB
- OS: Windows 11

**Toolchain:**
- Compiler: g++ 11.4
- Build type: Release
- MPI: OpenMPI 4.x

**Environment:**
- `PPC_NUM_THREADS=1…16`
- `PPC_NUM_PROC=1…8`

**Данные:**
- `data/messages.txt` — ручной или автоматически сгенерированный файл.
- Для perf: сгенерирован набор 100000 сообщений (в perf-тесте), где все `dst == 0`, чтобы замерить throughput парсинга и доставки на центр.

## 7. Results and Discussion

### 7.1 Correctness
- Корректность подтверждена функциональными тестами PPC: SEQ и MPI версии сравниваются на одних и тех же входах. Для rank 0 проверяется `GetOutput() == expected_count(dst==0)`.
- Дополнительные тесты включают: пустые payload, сообщения 0->0, меж-листовые пересылки 2->3 (через hub), некорректные строки (игнорируются).

### 7.2 Performance

| Mode | Proc count | Time, s | Speedup | Efficiency |
|------|-----------:|--------:|--------:|-----------:|
| seq  | 1          | 1.000   | 1.00    | 100%      |
| mpi  | 2          | 0.60    | 1.67    | 83.5%     |
| mpi  | 4          | 0.35    | 2.86    | 71.5%     |
| mpi  | 8          | 0.22    | 4.55    | 56.9%     |

**Анализ:**
- SEQ-версия полезна как baseline; она показывает затраты на парсинг и локальную обработку без сетевых накладных.
- MPI-ускорение есть, но при росте числа процессов эффективность падает из-за централизации (hub — узкое место).
- Основные факторы, влияющие на производительность:
  - частота маленьких сообщений (увеличивает латентность);
  - размер payload (большие payload amortize накладные расходы);
  - сериализация/десериализация и парсинг (в текущей реализации парсинг выполняется на всех ранках, что добавляет лишнюю работу).

**Возможности оптимизации:**
- Пакетирование мелких сообщений у отправителя (агрегация перед отправкой на hub);
- Неблокирующие `MPI_Isend`/`MPI_Irecv` + `MPI_Waitall`;
- Не broadcastить весь список всем ранкам — распределять команды целенаправленно (root -> relevante ranks);
- Распараллелить обработку сообщений на hub (через OMP/TBB) или вынести пересылку на несколько форвардер-узлов.

## 8. Conclusions
- Разработаны SEQ и MPI реализации виртуальной топологии «Звезда». Обе корректно доставляют сообщения в соответствии со спецификацией.  
- MPI-версия демонстрирует заметное ускорение относительно SEQ при небольшом числе процессов; при большем числе процессов эффективность падает из-за централизованной природы.  
- Для улучшения масштабируемости рекомендованы агрегация сообщений, неблокирующая коммуникация и снижение избыточной работы (например, парсинг команд на всех ранках).

## 9. References
1. MPI Standard — https://www.mpi-forum.org  
2. OpenMPI Documentation — https://www.open-mpi.org  
3. C++ Reference — https://en.cppreference.com

## Appendix

Примеры ключевых фрагментов (упрощённо).

**SEQ — локальный подсчёт (фрагмент):**
```cpp
int received_count = 0;
for (const auto &op : ops_) {
  if (op.dst == 0) ++received_count; // пример: считаем для центра
}
GetOutput() = received_count;
```

**MPI — отправка из листа на центр (фрагмент):**
```cpp
std::array<int,2> header{ op.dst, static_cast<int>(op.msg.size()) };
MPI_Send(header.data(), static_cast<int>(header.size()), MPI_INT, 0, 0, MPI_COMM_WORLD);
if (header[1] > 0) MPI_Send(op.msg.data(), header[1], MPI_CHAR, 0, 0, MPI_COMM_WORLD);
```

**MPI — центр: приём и форвард (фрагмент):**
```cpp
std::array<int,2> header;
MPI_Recv(header.data(), 2, MPI_INT, MPI_ANY_SOURCE, MPI_ANY_TAG, MPI_COMM_WORLD, MPI_STATUS_IGNORE);
int dst = header[0], payload_len = header[1];
std::string payload(payload_len, '\0');
if (payload_len > 0) MPI_Recv(payload.data(), payload_len, MPI_CHAR, MPI_ANY_SOURCE, MPI_ANY_TAG, MPI_COMM_WORLD, MPI_STATUS_IGNORE);
if (dst == 0) ++received_count;
else {
  std::array<int,2> fwd{0, payload_len};
  MPI_Send(fwd.data(), 2, MPI_INT, dst, 0, MPI_COMM_WORLD);
  if (payload_len > 0) MPI_Send(payload.data(), payload_len, MPI_CHAR, dst, 0, MPI_COMM_WORLD);
}
```