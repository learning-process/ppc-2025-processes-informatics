# Отчёт по реализации алгоритма суммирования элементов матрицы

**Дисциплина:** Параллельное программирование  
**Преподаватели:** Нестеров Александр Юрьевич, Оболенский Арсений Андреевич  
**Студент:** Фролова Софья Сергеевна, 3823Б1ФИ3  
**Вариант:** 10, Сумма элементов матрицы

---

## Введение

В рамках данной лабораторной работы была реализована задача вычисления суммы всех элементов матрицы. Были разработаны две версии алгоритма:

- последовательная (SEQ);
- параллельная (MPI), использующая модель передачи сообщений.

Обе реализации интегрированы в инфраструктуру параллельных задач фреймворка PPC и обладают набором функциональных и производительных тестов.

---

## Постановка задачи

Дана матрица:

\[
A = \{a_{ij}\} \quad \text{размера } N \times M.
\]

Требуется вычислить сумму всех элементов:

\[
S = \sum_{i=1}^{N} \sum_{j=1}^{M} a_{ij}.
\]

Входной тип данных: using InType = std::vector<std::vector<int>>;
Выходной тип: using OutType = long long;


## Описание алгоритма

### Последовательная версия (SEQ)

Алгоритм работы SEQ-версии:

1. Проверка корректности входных данных (матрица не пустая, строки одинаковой длины).
2. Инициализация суммы нулём.
3. Проход по всем строкам и суммирование элементов методом `std::accumulate`.
4. Запись результата в `Output`.

Асимптотика —  
\[
O(N \cdot M)
\]  
Память — минимальная, используется только аккумулятор суммы.

---

## Описание схемы параллельного алгоритма (MPI)

Параллельная версия распределяет строки матрицы между процессами.

Пусть имеется:

- `P` процессов;
- матрица размера `N × M`.

### Деление данных


base = N / P
remain = N % P


- первые `remain` процессов обрабатывают `base + 1` строк;  
- остальные — `base`.

Каждый процесс суммирует “свои” строки:


local_sum = sum(matrix[start_row ... end_row - 1])


Затем выполняется редукция:

MPI_Reduce(&local_sum, &global_sum, 1, MPI_LONG_LONG, MPI_SUM, 0, MPI_COMM_WORLD);


Процесс 0 получает итоговую сумму.

---

## Реализация в рамках фреймворка PPC

Для задачи созданы классы:

- `FrolovaSSumElemMatrixSEQ`
- `FrolovaSSumElemMatrixMPI`

в пространстве имён: namespace frolova_s_sum_elem_matrix

Файл `common.hpp` определяет типы входных/выходных данных и базовый класс задачи.


## Результаты экспериментов и выводы

### Функциональные тесты

Функциональные тесты проверяют корректность работы алгоритма на различных наборах данных:

- 3×3 матрица;
- 10×10 матрица;
- 20×15 матрица;
- матрицы из единиц;
- прямоугольные матрицы.

Все тесты успешно проходят для обеих версий — SEQ и MPI.


### Тесты производительности

Для производительности использовалась матрица 1000×1000, заполненная единицами.

| Размер матрицы | Процессы | SEQ (с) | MPI (с) | Ускорение |
|----------------|----------|---------|---------|-----------|
| 1000×1000      | 4        | 0.035   | 0.018   | 1.94×     |
| 1000×1000      | 2        | 0.035   | 0.024   | 1.45×     |
| 1000×1000      | 1        | 0.035   | 0.035   | 1.00×     |

MPI-версия показывает ускорение на больших матрицах, но из-за издержек коммуникаций идеальное ускорение недостижимо.


## Заключение

В ходе выполнения лабораторной работы были:

- реализованы последовательная и параллельная версии алгоритма суммирования всех элементов матрицы;
- интегрированы SEQ и MPI версии в фреймворк PPC;
- реализованы функциональные тесты;
- проведены производительные тесты;
- выполнено форматирование проекта в соответствии с требованиями.

MPI-реализация демонстрирует ускорение на больших данных, тогда как SEQ-версия быстрее на малых размерах матриц.


## Литература

1. Лекции Сысоева А.В.  
2. Практические занятия Нестерова А.Ю. и Оболенского А.А.  
3. Документация MPI.  
4. Материалы PPC-фреймворка.  


## Приложение (фрагмент кода MPI-версии)

bool FrolovaSSumElemMatrixMPI::RunImpl() {
  const auto &matrix = GetInput();

  int rank = 0, size = 1;
  MPI_Comm_rank(MPI_COMM_WORLD, &rank);
  MPI_Comm_size(MPI_COMM_WORLD, &size);

  const int rows = static_cast<int>(matrix.size());
  const int base = rows / size;
  const int rem  = rows % size;

  const int local_rows = base + (rank < rem ? 1 : 0);
  const int start_row  = rank * base + std::min(rank, rem);
  const int end_row    = start_row + local_rows;

  long long local_sum = 0;
  for (int i = start_row; i < end_row; ++i) {
    local_sum += std::accumulate(matrix[i].begin(), matrix[i].end(), 0LL);
  }

  long long global_sum = 0;
  MPI_Reduce(&local_sum, &global_sum, 1,
             MPI_LONG_LONG, MPI_SUM, 0, MPI_COMM_WORLD);

  if (rank == 0) {
    GetOutput() = global_sum;
  }

  return true;
}

