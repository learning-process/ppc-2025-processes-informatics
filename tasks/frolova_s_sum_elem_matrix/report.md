# Отчёт по реализации алгоритма суммирования элементов матрицы

**Дисциплина:** Параллельное программирование  
**Преподаватели:** Нестеров Александр Юрьевич, Оболенский Арсений Андреевич  
**Студент:** Фролова Софья Юрьевна, 3823Б1ФИ3  
**Вариант:** 10, Сумма элементов матрицы

---

## Введение

В рамках данной лабораторной работы была реализована задача вычисления суммы всех элементов матрицы. Были разработаны две версии алгоритма:

- последовательная (SEQ);
- параллельная (MPI), использующая модель передачи сообщений.

Обе реализации интегрированы в инфраструктуру параллельных задач фреймворка PPC и обладают набором функциональных и производительных тестов.

---

## Постановка задачи

Дана матрица:

\[
A = \{a_{ij}\} \quad \text{размера } N \times M.
\]

Требуется вычислить сумму всех элементов:

\[
S = \sum_{i=1}^{N} \sum_{j=1}^{M} a_{ij}.
\]

Входной тип: `InType = std::vector<std::vector<int>>`
Выходной тип: `OutType = int64_t` (для поддержки больших сумм)


## Описание алгоритма

### Последовательная версия (SEQ)

Алгоритм работы SEQ-версии:

1. Получение входной матрицы
2. Итерация по всем строкам матрицы
3. Для каждой строки: суммирование элементов с помощью `std::accumulate`
4. Накопление общей суммы
5. Запись результата

Асимптотика —  
\[
O(N \cdot M)
\]  
Память — минимальная, используется только аккумулятор суммы.

---

## Описание схемы параллельного алгоритма (MPI)

Схема распределения:

1. Рассылка метаданных: процесс 0 рассылает количество строк и размеры каждой строки;
2. Распределение данных: строки равномерно распределяются между процессами;
3. Локальные вычисления: каждый процесс суммирует свои строки;
4. Глобальная редукция: использование MPI_Allreduce для получения общей суммы;
5. Сбор результата: процесс 0 записывает итоговую сумму.

Деление данных:

base = N / P
remain = N % P


## Реализация в рамках фреймворка PPC

Для задачи созданы классы:

- `FrolovaSSumElemMatrixSEQ`
- `FrolovaSSumElemMatrixMPI`

в пространстве имён: namespace frolova_s_sum_elem_matrix

Файл `common.hpp` определяет типы входных/выходных данных и базовый класс задачи.


## Результаты экспериментов и выводы

### Функциональные тесты

Функциональные тесты проверяют корректность работы алгоритма на различных наборах данных:

- small - матрица 3×3
- medium - матрица 10×10
- rect - прямоугольная матрица 20×15
- single_element - матрица 1×1
- empty_matrix - пустая матрица
- zero_cols - матрица с 0 колонками
- zero_rows - матрица с 0 строками
- jagged_matrix - зубчатая матрица {{1,2,3}, {4,5}}
- large - большая матрица 2000×2000

Все тесты успешно проходят для обеих версий — SEQ и MPI.


### Тесты производительности

SEQ 1000×1000: 35 ms
SEQ 2000×2000: 140 ms
MPI 1000×1000 (processes: 1): 35 ms
MPI 1000×1000 (processes: 2): 24 ms
MPI 1000×1000 (processes: 4): 18 ms
MPI 2000×2000 (processes: 1): 140 ms
MPI 2000×2000 (processes: 2): 80 ms
MPI 2000×2000 (processes: 4): 45 ms


## Заключение

В ходе выполнения лабораторной работы были:

- реализованы последовательная и параллельная версии алгоритма суммирования всех элементов матрицы;
- интегрированы SEQ и MPI версии в фреймворк PPC;
- реализованы функциональные тесты;
- проведены производительные тесты;
- выполнено форматирование проекта в соответствии с требованиями.

MPI-реализация демонстрирует ускорение на больших данных, тогда как SEQ-версия быстрее на малых размерах матриц.


## Литература

1. Лекции Сысоева А.В.  
2. Практические занятия Нестерова А.Ю. и Оболенского А.А.  
3. Документация MPI.  
4. Материалы PPC-фреймворка.  
