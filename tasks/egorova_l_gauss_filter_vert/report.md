# Вертикальный фильтр Гаусса (3x3)
- **Студент:** Егорова Лариса Алексеевна
- **Группа:** 3823Б1ФИ1
- **Технология:** MPI, SEQ
- **Вариант:** 27

## 1. Введение
Фильтрация изображений является одной из ключевых задач компьютерного зрения и цифровой обработки сигналов. Фильтр Гаусса применяется для снижения уровня цветового шума и сглаживания деталей путем низкочастотной фильтрации. В условиях работы с изображениями высокого разрешения последовательная обработка становится узким местом, что обуславливает необходимость использования технологий параллельного программирования, таких как MPI, для распределения вычислительной нагрузки между ядрами процессора.

## 2. Постановка задачи
Необходимо реализовать параллельный алгоритм фильтрации изображения с использованием ядра Гаусса размером $3 \times 3$.
- **Вход:** Структура данных, содержащая количество строк, столбцов, каналов и одномерный вектор пикселей `uint8_t`.
- **Выход:** Преобразованный вектор пикселей того же размера.
- **Особенности:** Реализация должна корректно обрабатывать границы изображения (используя зажатие координат `clamp`) и обеспечивать синхронизацию "призрачных зон" (halo-зоны) для непрерывности фильтрации на стыках данных разных процессов.

## 3. Последовательный алгоритм
Алгоритм реализует математическую операцию свертки изображения с дискретным аналогом функции Гаусса.
1. Для каждого целевого пикселя $(x, y)$ анализируется окрестность $3 \times 3$.
2. Используется ядро с весовыми коэффициентами и последующей нормализацией ($\frac{1}{16}$):
   $$K = \frac{1}{16} \begin{bmatrix} 1 & 2 & 1 \\ 2 & 4 & 2 \\ 1 & 2 & 1 \end{bmatrix}$$
3. Для пикселей на краях изображения применяется стратегия **Clamping**: если индекс выходит за пределы, используется значение ближайшего существующего пикселя, что предотвращает появление артефактов на границах.

## 4. Схема распараллеливания (MPI)
Для минимизации межпроцессных взаимодействий выбрана одномерная вертикальная декомпозиция (разделение изображения на полосы по столбцам).

1. **Распределение нагрузки:** Изображение делится на $N$ вертикальных полос, где $N$ — число процессов. Каждый процесс отвечает за вычисление своей части столбцов.
2. **Организация Halo-зоны:** Так как фильтр имеет радиус 1, каждому процессу для расчета крайних пикселей своей полосы требуется один соседний столбец от предыдущего и последующего процессов.
3. **Коллективные операции:** - `MPI_Bcast` используется для передачи метаданных и исходного массива данных.
   - `MPI_Allgatherv` выполняет сборку обработанных полос обратно в единое изображение на всех узлах.

## 5. Детали реализации
- **Управление памятью:** Использование `std::vector` и `std::array` обеспечивает безопасность работы с памятью.
- **Оптимизация:** Прямой доступ к коэффициентам ядра через метод `.data()` минимизирует накладные расходы внутри вложенных циклов свертки. Промежуточные вычисления ведутся в типе `float` для сохранения точности.
- **Статический анализ:** Код соответствует стандартам `cppcoreguidelines`, в частности, заменены все C-style массивы и исправлены потенциальные ошибки приведения типов (`narrowing conversion`).

## 6. Настройка экспериментов
- **Процессор:** 13th Gen Intel(R) Core(TM) i5-13420H (2.10 GHz, 8 ядер / 12 потоков)
- **Оперативная память:** 16,0 ГБ
- **ОС:** Windows 10 (WSL2 / Docker Desktop)
- **Инструментарий:** Clang-21, MPI, CMake, Release build.

## 7. Результаты и обсуждение

### 7.1 Корректность
Корректность алгоритма верифицирована с помощью модульных тестов:
- Сравнение с последовательной версией на различных форматах данных.
- Проверка многоканальных (RGB) и полутоновых изображений.
- Валидация работы на прямоугольных (неквадратных) матрицах.

### 7.2 Производительность
Результаты замеров времени выполнения (режим `task_run`) для изображения **2000x2000**:

| Режим | Кол-во процессов | Время, с | Ускорение | Эффективность |
|-------|:----------------:|:--------:|:---------:|:-------------:|
| seq   | 1                | 0.03016  | 1.00      | N/A           |
| mpi   | 1                | 0.03017  | 1.00      | 100.0%        |
| mpi   | 2                | 0.01386  | 2.17      | 108.5%        |
| mpi   | 4                | 0.01189  | 2.54      | 63.5%         |

**Анализ результатов:**
- **Суперлинейное ускорение (2 процесса):** Ускорение 2.17 (эффективность > 100%) связано с тем, что при разделении данных на части они начинают лучше утилизировать локальную кэш-память процессора ($L2/L3$), что сокращает задержки при доступе к памяти.
- **Масштабируемость (4 процесса):** Время выполнения достигает минимума (0.01189 с), однако эффективность падает до 63.5%. Это обусловлено тем, что на данном объеме данных затраты на коллективные операции обмена (`MPI_Allgatherv`) становятся сопоставимы со временем самих вычислений.

## 8. Заключение
В ходе выполнения работы была реализована параллельная версия вертикального фильтра Гаусса. Использование MPI позволило сократить время обработки в 2.54 раза на четырех процессах. Дальнейшее повышение эффективности масштабирования возможно при обработке изображений сверхвысокого разрешения (4K и выше), где вычислительная нагрузка будет преобладать над коммуникационной.

## 9. Источники
1. Open MPI Documentation: [https://www.open-mpi.org/doc/](https://www.open-mpi.org/doc/)
2. MPI Standard: [https://www.mpi-forum.org/docs/](https://www.mpi-forum.org/docs/)
3. Introduction to MPI: [https://mpitutorial.com/](https://mpitutorial.com/)
4. Материалы курса «Параллельное программирование».

## Приложение (код)

```cpp
bool EgorovaLGaussFilterVertMPI::RunImpl() {
  int rank = 0;
  int size = 0;
  MPI_Comm_rank(MPI_COMM_WORLD, &rank);
  MPI_Comm_size(MPI_COMM_WORLD, &size);

  const auto &full_img = GetInput();
  int rr = 0;
  int cc = 0;
  int ch = 0;

  if (rank == 0) {
    rr = full_img.rows;
    cc = full_img.cols;
    ch = full_img.channels;
  }

  MPI_Bcast(&rr, 1, MPI_INT, 0, MPI_COMM_WORLD);
  MPI_Bcast(&cc, 1, MPI_INT, 0, MPI_COMM_WORLD);
  MPI_Bcast(&ch, 1, MPI_INT, 0, MPI_COMM_WORLD);

  int qq = (cc / size);
  int rem = (cc % size);
  int local_cols = (qq + (rank < rem ? 1 : 0));
  int offset = ((rank * qq) + std::min(rank, rem));

  int left_h = ((rank > 0) ? 1 : 0);
  int right_h = ((rank < size - 1) ? 1 : 0);
  int total_lc = (local_cols + left_h + right_h);

  std::vector<uint8_t> local_in(static_cast<std::size_t>(rr) * total_lc * ch);
  std::vector<uint8_t> local_out(static_cast<std::size_t>(rr) * local_cols * ch);

  std::vector<uint8_t> shared_data;
  if (rank == 0) {
    shared_data = full_img.data;
  } else {
    shared_data.resize(static_cast<std::size_t>(rr) * cc * ch);
  }
  MPI_Bcast(shared_data.data(), static_cast<int>(shared_data.size()), MPI_BYTE, 0, MPI_COMM_WORLD);

  for (int ii = 0; ii < rr; ++ii) {
    std::size_t src_start = (static_cast<std::size_t>(ii) * cc + (offset - left_h)) * ch;
    std::size_t dest_start = (static_cast<std::size_t>(ii) * total_lc) * ch;
    std::size_t count = static_cast<std::size_t>(total_lc) * ch;

    std::copy(std::next(shared_data.begin(), static_cast<std::ptrdiff_t>(src_start)),
              std::next(shared_data.begin(), static_cast<std::ptrdiff_t>(src_start + count)),
              std::next(local_in.begin(), static_cast<std::ptrdiff_t>(dest_start)));
  }

  ApplyFilter(local_in, local_out, rr, local_cols, total_lc, ch);

  std::vector<int> counts(size);
  std::vector<int> displs(size);
  for (int ii = 0; ii < size; ++ii) {
    counts[ii] = (qq + (ii < rem ? 1 : 0)) * ch;
    displs[ii] = (ii == 0) ? 0 : displs[ii - 1] + counts[ii - 1];
  }

  auto &output = GetOutput();
  output.rows = rr;
  output.cols = cc;
  output.channels = ch;
  output.data.resize(static_cast<std::size_t>(rr) * cc * ch);

  for (int ii = 0; ii < rr; ++ii) {
    std::size_t send_offset = (static_cast<std::size_t>(ii) * local_cols * ch);
    std::size_t recv_offset = (static_cast<std::size_t>(ii) * cc * ch);

    MPI_Allgatherv(local_out.data() + send_offset, counts[rank], MPI_BYTE, output.data.data() + recv_offset,
                   counts.data(), displs.data(), MPI_BYTE, MPI_COMM_WORLD);
  }

  return true;
}
