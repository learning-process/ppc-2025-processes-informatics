# Сумма элементов вектора
- Студент: Гонозов Леонид Андреевич, 3823Б1ФИ3
- Технология: SEQ, MPI 
- Вариант: 1

---

## 1. Введение
Многие дисциплины, например, физику, геометрию и компьютерную графику невозможно представить без такого математического объекта, как вектор. Но сами по себе объекты были бы бессмысленны, если бы над ними нельзя было бы проводить различных опреаций. Одной из базовых операций над векторами является суммирование его элементов, что и будет рассмотрено в данном проекте. 

Цель работы - реализовать параллельную версию алгоритма суммирования элементов вектора с использованием технологии MPI, оценить корректность и исследовать ускорение относительно последовательного варианта.

---

## 2. Постановка задачи
**Определение**: Вектор длины большей нуля, заполненный числовыми значениями по всем своим координатам. Числовые значения также произвольны. Над ним должны быть совершены операции, приводящие к получению суммы элементов вектора. 

**Входные данные**: вектор.
**Выходные данные:** число с плавающей запятой - количество слов.

**Ограничения**:
- длина вектора должна быть больше нуля;
- элементы вектора должны быть представлены конечными значениями;
- для элементов вектора нужно использовать целочисленный тип, чтобы работать с как можно большим объёмом данных в тестах на производительность
- различное количество процессов не должно приводить к непредвиденным последствиям.

---

## 3. Описание базового алгоритма 
Последовательная версия реализована с помощью std::accumulate из стандартной библиотеки c++. Данная функция суммирует значения в заданном диапазоне элементов, которые хранятся в каком-либо контейнере, в нашем случае в std::vector.

```cpp
	OutType sum = std::accumulate(GetInput().begin(), GetInput().end(), 0LL);
	GetOutput() = sum;
```

---

## 4.  Схема распараллеливания
- **Декомпозиция данных**:
	- Исходный вектор разбивается на части по числу процессов.
	- Каждый процесс получает свою часть вектора через `MPI_Scatterv`.
- **Вычисления на процессе**:
	Каждый процесс высчитывает сумму элементов, исходя из той части входного вектора, которая досталась ему для подсчёта.
- **Коммуникация**:
	После локальных подсчётов выполняется `MPI_Allreduce` с операцией суммы, с помощью которой мы получаем сумму элементов вектора.
- **Топология и обмен**:
	Используется коммуникатор `MPI_COMM_WORLD` без явной топологии (все процессы равноправны).  
	Тип обмена — коллективные операции (`MPI_Scatterv`, `MPI_Allreduce`).

 
---
## 5.  Детали реализации
- **Ключевые файлы:**
	- `tasks/gonozov_l_elem_vec_sum/seq/include/ops_seq.hpp` и `tasks/gonozov_l_elem_vec_sum/seq/src/ops_seq.cpp` - последовательная реализация.
	- `tasks/gonozov_l_elem_vec_sum/mpi/include/ops_mpi.hpp` и `tasks/gonozov_l_elem_vec_sum/mpi/src/ops_mpi.cpp` - параллельная реализация.
	- `tasks/gonozov_l_elem_vec_sum/common/include/common.hpp` - общие определения типов и интерфейсов.
- **Особенности**:
- **Обработка случая, когда процессов больше, чем элементов вектора**
	Последовательная обработка вектора в случае, когда процессов больше, чем элементов вектора, осуществляется с целью того, чтобы уменьшить накладные расходы от распределения данных по векторам. 
- **Разбиение вектора на части по числу процессов**
	В том случае, если существует остаток от деления вектора на число процессов, процессы, чей ранг меньше, чем это остаток, получают в распоряжение на один элемент больше. Это иллюстрирует следующий псевдокод:
```cpp
  	int remainder = размер_вектора % ко-во_процессов;
  	int local_size = размер_вектора / ко-во_процессов + (procRank < remainder ? 1 : 0);
```
- **Передача процессам данных, требуемых для вычислений**
	С помощью `MPI_Scatterv` каждый процесс получает *local_size* элементов (так как он уникален для каждого процесса, они хранятся в *sendcounts*), расположенных в векторе, поступившим на вход, в котором смещение характеризуется величиной *displs* (массив, в котором хранится смещение для каждого отдельно взятого процесса)
- **Вычисления на каждом отдельно взятом процессе**
	```cpp
  	OutType local_sum = std::accumulate(subvector.begin(), subvector.end(), 0LL);
	```
	Используется std::accumulate, данная функция суммирует значения в заданном диапазоне элементов
	subvector - буффер, в который каждый процесс складывает вычисленные суммы
- **Получение конечного результата**
	Получаем результат благодаря `MPI_Allreduce` с операцией суммы
---

## 6. Экспериментальная установка
- **Аппаратное обеспечение**: 
	12th Gen Intel(R) Core(TM) i7-12650H (2.30 GHz) (6 производительных ядер и 4 энергоэффективных)  
	ОЗУ — 16,0 ГБ
- **Операционная система:** Windows 11 Pro
- **Инструменты:**
	- Компилятор: g++ (Ubuntu 13.3.0-6ubuntu2~24.04) 13.3.0 
	- CMake 4.1.1
	- Режим сборки: Release.
	- Был использован Docker-container
- **Данные:**
	Вектора на 1 000 000, 10 000 000 и 100 000 000 элементов, заполнены значениями от 0 до 999 999, от 0 до 9 999 999 и от 0 до 99 999 999 соответственно

---

## 7.  Результаты и обсуждение

### 7.1 Корректность
Корректность проверялась сравнением результатов полученных в ходе алгоритма и заранее заготовленных значений. При этом на вход подавались вектора различной длины заполненных единицами.

### 7.2 Производительность

Тест 1. Вектор из 1 000 000 элементов.

| Mode | Count | Time,  s | Speedup | Efficiency |
| ---- | ----- | -------- | ------- | ---------- |
| seq  | 1     | 0.000241 | 1.00    | N/A        |
| mpi  | 2     | 0.000421 | 0.572   | 28.6%      |
| mpi  | 4     | 0.000505 | 0.477   | 11.925%    |
| mpi  | 8     | 0.000638 | 0.377   | 4.712%     |

Тест 2. Вектор из 10 000 000 элементов.

| Mode | Count | Time, s | Speedup | Efficiency |
| ---- | ----- | ------- | ------- | ---------- |
| seq  | 1     | 0.00251 | 1.00    | N/A        |
| mpi  | 2     | 0.00557 | 0.45    | 22.5%      |
| mpi  | 4     | 0.00588 | 0.426   | 10.65%     |
| mpi  | 8     | 0.00645 | 0.389   | 4.862%     |

Тест 3. Вектор из 100 000 000 элементов.

| Mode | Count | Time, s | Speedup | Efficiency |
| ---- | ----- | ------- | ------- | ---------- |
| seq  | 1     | 0.02377 | 1.00    | N/A        |
| mpi  | 2     | 0.15131 | 0.157   | 7.854%     | 
| mpi  | 4     | 0.16886 | 0.1407  | 3.519%     |
| mpi  | 8     |    -    |   -     |   -        |

- Накладные расходы коммуникации полностью перекрывают выигрыш от распараллеливания
- В тесте 3 при 8 процессах накладных расходов настолько много, что это приводит к падению программы 

---

## 8. Заключение
Реализованный в зоде работы параллельный алгоритм действительно выполняет свою задачу - суммирование элементов вектора, но из-за накладных расходов на обмен данными между процессами наблюдается полная несостоятельность применения такого варианта алгоритма на практике. Причём стоит отметить, что с ростом данных происходит лишь усугубление этой проблемы.

---

## 9. Источники
- Лекции и практики курса "Параллельное программирование", https://cppreference.com/
