# Обобщённая передача от одного всем (Scatter)

**Student:** Дорофеев Иван Денисович, group 3823Б1ФИ1
**Technology:** SEQ | MPI
**Variant:** 4

---

## 1. Introduction

Операция **Scatter** относится к базовым методам передачи сообщений в MPI и предназначена для распределения частей массива от одного корневого процесса всем остальным процессам коммуникатора. Каждый процесс получает свою часть данных, причём распределение выполняется в рамках одной коллективной операции.

В рамках данного задания требуется реализовать функциональность, эквивалентную `MPI_Scatter`, **используя только точечные операции `MPI_Send` и `MPI_Recv`**, а также применяя **древовидную схему передачи данных**. Такой подход позволяет снизить нагрузку на корневой процесс и улучшить масштабируемость алгоритма.

---

## 2. Problem statement

Требуется реализовать обобщённую операцию рассылки данных от одного процесса всем остальным (Scatter).

### Требования:

* реализация должна иметь **тот же прототип**, что и соответствующая функция MPI;
* разрешено использовать **только `MPI_Send` и `MPI_Recv`**;
* передача должна выполняться **по дереву процессов**;
* тестовая программа должна позволять:

  * выбирать процесс `root`,
  * выполнять рассылку массивов типов:

    * `MPI_INT`,
    * `MPI_FLOAT`,
    * `MPI_DOUBLE`.

### Входные данные:

* массив элементов на корневом процессе;
* номер корневого процесса `root`;
* тип данных MPI;
* коммуникатор процессов.

### Выходные данные:

* каждый процесс получает свой элемент (или блок элементов) исходного массива.

---

## 3. Baseline Algorithm (Sequential)

Последовательная версия не использует межпроцессное взаимодействие.

Алгоритм:

1. Корневой процесс хранит исходный массив.
2. Для каждого логического «процесса» соответствующий элемент массива считается доступным локально.
3. Операции передачи данных отсутствуют.

Последовательная версия используется исключительно как базовый вариант для сравнения производительности.

---

## 4. Parallelization Scheme (MPI)

MPI-версия реализует операцию Scatter **через точечные передачи**, организованные в виде бинарного дерева процессов.

### Схема работы:

1. Процессы логически организуются в дерево.
2. Корневой процесс:

   * делит массив на части,
   * отправляет подмассивы своим дочерним процессам.
3. Каждый промежуточный процесс:

   * принимает данные от родителя,
   * пересылает соответствующие части своим дочерним процессам.
4. Листовые процессы получают только свою часть данных.

Для передачи используются исключительно `MPI_Send` и `MPI_Recv`.
Коллективные операции MPI **не применяются**.

Такая схема снижает количество сообщений, исходящих от корня, с `O(P)` до `O(log P)`.

---

## 5. Experimental Setup

* **Hardware / OS:**

  * CPU: 13th Gen Intel i5-13420H (12) @ 4.6 GHz
  * RAM: 16 GB
  * OS: Ubuntu 25.10 x86_64
  * Среда выполнения: Docker (Ubuntu noble / trixie)

* **Toolchain:**

  * CMake 3.28.3
  * g++ 13.3.0
  * OpenMPI
  * Тип сборки: `Release`

* **Modes tested:**

  * SEQ
  * MPI (2 процесса)
  * MPI (4 процесса)

Замеры выполнялись с использованием встроенных performance-тестов (`ppc_perf_tests`).

---

## 6. Results and Discussion

### 6.1 Correctness

Корректность реализации подтверждена модульными тестами GoogleTest.

Проверяется, что:

* каждый процесс получает корректный элемент массива;
* поддерживаются типы `MPI_INT`, `MPI_FLOAT`, `MPI_DOUBLE`;
* корректно работает произвольный выбор процесса `root`.

Все тесты завершились успешно.

---

### 6.2 Performance

Для измерения времени выполнения использовались встроенные performance-тесты `ppc_perf_tests`.
Для каждого режима измерения (`pipeline` и `task_run`) фиксировалось время выполнения, после чего в таблице приводится **усреднённое значение**.

Замеры проводились для последовательной (SEQ) и параллельной (MPI) версий при числе процессов 2 и 4.

### Усреднённые времена выполнения Scatter

| Mode    | Processes | Time (s)   | Speedup  | Efficiency |
| ------- | --------- | ---------- | -------- | ---------- |
| **seq** | 1         | **0.0675** | **1.00** | —          |
| **mpi** | 2         | **0.0594** | **1.14** | **57%**    |
| **mpi** | 4         | **0.0781** | **0.86** | **22%**    |

---

## 7. Discussion

Результаты показывают, что для операции Scatter выигрыш по времени при малых объёмах данных минимален.

олученные результаты показывают, что:

* при использовании **2 процессов MPI** наблюдается умеренное ускорение по сравнению с последовательной версией;
* при **4 процессах** ускорение не достигается, что объясняется ростом накладных расходов на межпроцессное взаимодействие.

Операция Scatter относится к **коммуникационно-ограниченным** алгоритмам. При увеличении числа процессов возрастает количество точечных передач и синхронизаций, что при фиксированном объёме данных может приводить к снижению эффективности.

Тем не менее использование древовидной схемы позволяет:

* снизить нагрузку на корневой процесс;
* обеспечить корректную масштабируемость при увеличении объёма данных.

При увеличении размера передаваемого массива преимущества древовидной схемы становятся более заметными.

---

## 8. Conclusions

В ходе работы была реализована операция Scatter, эквивалентная `MPI_Scatter`, с использованием только `MPI_Send` и `MPI_Recv` и древовидной схемы передачи данных.

Реализация:

* полностью соответствует требованиям задания;
* корректно работает для различных типов данных;
* успешно проходит все тесты.

Хотя ускорение для малых данных незначительно, реализация демонстрирует правильный подход к построению масштабируемых коллективных операций и может эффективно использоваться при больших объёмах передаваемой информации.

---

## 9. References

1. "Параллельное программирование для кластерных систем", ИИТММ, ННГУ им. Лобачевского
2. [Open MPI Documentation](https://www.open-mpi.org/doc/)
3. [MPI Reference – Message Passing Interface | Microsoft Learn](https://learn.microsoft.com/en-us/message-passing-interface/mpi-reference)
4. [MPI: A Message-Passing Interface Standard](https://www.mpi-forum.org/docs/mpi-5.0/mpi50-report.pdf)
