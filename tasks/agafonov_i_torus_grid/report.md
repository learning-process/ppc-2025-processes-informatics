# Решетка-тор

- Student: Агафонов Илья Дмитриевич, group 3823Б1ФИ1
- Technology: SEQ | MPI
- Variant: 9

## 1. Introduction
Целью данной работы является реализация алгоритма обмена данными в вычислительной сети с топологией «двумерный тор» (Torus Grid). Топология тора обеспечивает высокую степень связности и отказоустойчивость, что делает её актуальной для распределенных вычислений. Ожидаемый результат — корректная передача сообщения от заданного узла-источника к узлу-приемнику с использованием интерфейса MPI.

## 2. Problem Statement
Необходимо реализовать передачу целочисленного значения (`int`) между двумя произвольными узлами (процессами) в логической сетке размера $N \times N$.
- **Входные данные:** Значение для передачи, ранг отправителя (`source_rank`) и ранг получателя (`dest_rank`).
- **Выходные данные:** После завершения работы значение должно быть доступно на всех процессах.
- **Ограничения:** Количество запущенных процессов $P$ должно быть полным квадратом ($P = N^2$).

## 3. Baseline Algorithm (Sequential)
Последовательный алгоритм имитирует передачу данных путем прямого копирования значения из входной структуры в выходную переменную. Логика маршрутизации отсутствует, так как в последовательном режиме существует только один процесс, который одновременно является и источником, и приемником.

## 4. Parallelization Scheme
Для параллельной реализации используется библиотека MPI и следующая схема взаимодействия:
1. **Топология:** Процессы интерпретируются как узлы сетки. Проверка `ValidationImpl` гарантирует, что количество процессов позволяет сформировать квадратную структуру.
2. **Точечная передача (Point-to-Point):** Процесс-источник (`source_rank`) использует `MPI_Send` для отправки данных. Процесс-приемник (`dest_rank`) использует `MPI_Recv` для их получения.
3. **Широковещательная рассылка:** Чтобы результат стал доступен всем узлам (согласно требованиям), используется `MPI_Bcast` от ранга-получателя.

## 5. Implementation Details
- **Основные функции:**
  - `ValidationImpl`: Проверяет границы рангов и квадратность сетки.
  - `RunImpl`: Реализует логику `Send` -> `Recv` -> `Bcast`.
- **Линтер:** Код прошел проверку `clang-tidy-21`, исправлены замечания по упрощению логических выражений (законы Де Моргана) и чистоте заголовочных файлов.

## 6. Experimental Setup
**Hardware/OS:**
  - **Процессор:** Процессор	AMD Ryzen 5 5500U, ядер: 6, логических процессоров: 12
  - **Оперативная память:** 16 ГБ DDR4
  - **Операционная система:** Windows 10 Pro 22H2
- **Toolchain:**
  - **Компилятор:** g++ 13.3.0
  - **Тип сборки:** Release (-O3 )
  - **MPI:** Open MPI 4.1.6

## 7. Results and Discussion

### 7.1 Correctness
Функциональные тесты успешно пройдены для различных сценариев передачи (между узлами, самопередача). Проверка проводилась на 4 процессах с использованием `mpirun`.

### 7.2 Performance
Результаты замера времени выполнения (`task_run`) на 4 процессах:

| Mode        | Count | Time, s      | Speedup | Efficiency |
|-------------|-------|--------------|---------|------------|
| seq         | 1     | 0.00000014   | 1.00    | N/A        |
| mpi         | 4     | 0.0000073408 | 0.00001 | 0.45%      |
| mpi         | 9     | 0.00155104   | 0.00009 | 0.001%      |

Низкая эффективность объясняется спецификой задачи — передача одного числа `int` происходит слишком быстро по сравнению с накладными расходами на вызов функций MPI и синхронизацию процессов в Docker-контейнере.

## 8. Conclusions
Алгоритм обмена в топологии «Тор» реализован и верифицирован. Программа демонстрирует стабильную работу и проходит статический анализ кода. Опытным путем подтверждено, что для микро-задач коммуникационные затраты MPI значительно превышают время полезных вычислений.

## 9. Список литературы
1. Материлы и документация по курсу
2. Документация стандарта MPI: https://www.mpi-forum.org/
