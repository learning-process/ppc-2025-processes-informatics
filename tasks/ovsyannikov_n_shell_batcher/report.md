# Сортировка Шелла с четно-нечетным слиянием Бэтчера

**Студент:** Овсянников Никита Владимирович  
**Группа:** 3823Б1ФИ2  
**Технология:** SEQ, MPI  
**Вариант:** 17  

---

## 1. Introduction

Целью данной работы является изучение технологии **MPI** и принципов распараллеливания алгоритмов обработки данных на системах с распределённой памятью. В рамках работы реализована последовательная и параллельная версии алгоритма сортировки массива целых чисел.

В качестве базового алгоритма сортировки выбран **Shell sort**, так как он является улучшением сортировки вставками и демонстрирует более высокую производительность на практических данных. Для объединения частично отсортированных массивов в параллельной версии используется **сеть сортировки Batcher Odd–Even Merge**, что позволяет корректно сливать результаты работы отдельных MPI-процессов.

---

## 2. Problem Statement

Дан массив целых чисел произвольной длины.  
Необходимо отсортировать его по неубыванию.

### Входные данные
- `std::vector<int>` — массив целых чисел.

### Выходные данные
- `std::vector<int>` — отсортированный массив.

---

## 3. Baseline Algorithm (Sequential)

В последовательной версии используется алгоритм **Shell sort**.  
Алгоритм работает следующим образом:

1. Выбирается начальный шаг `gap = n / 2`.
2. Выполняется сортировка вставками для элементов, отстоящих друг от друга на расстояние `gap`.
3. Значение шага постепенно уменьшается вдвое до 1.
4. На последнем шаге алгоритм эквивалентен обычной сортировке вставками.

Пример реализации:

```cpp
for (std::size_t gap = n / 2; gap > 0; gap /= 2) {
  for (std::size_t i = gap; i < n; ++i) {
    int tmp = a[i];
    std::size_t j = i;
    while (j >= gap && a[j - gap] > tmp) {
      a[j] = a[j - gap];
      j -= gap;
    }
    a[j] = tmp;
  }
}
```
Средняя вычислительная сложность Shell sort составляет
O(n · log² n) (в зависимости от последовательности шагов).

## 4. Parallelization Scheme (MPI)

Параллельная версия алгоритма реализована с использованием технологии **MPI** и ориентирована на распределённые вычисления. Основная идея заключается в том, что исходный массив делится между процессами, каждый из которых выполняет локальную сортировку, после чего частично отсортированные данные объединяются в единый результат.

Алгоритм работы параллельной версии включает следующие этапы:

1. **Определение размера входных данных**  
   Процесс с рангом 0 определяет размер входного массива и рассылает его всем остальным процессам с помощью `MPI_Bcast`.

2. **Распределение данных между процессами**  
   Исходный массив делится на части пропорционально количеству MPI-процессов.  
   Для передачи данных используется функция `MPI_Scatterv`, позволяющая распределять фрагменты разного размера.

3. **Локальная сортировка**  
   Каждый процесс независимо сортирует полученную часть массива с использованием алгоритма **Shell sort**.

4. **Слияние частично отсортированных массивов**  
   Для объединения результатов используется алгоритм **Batcher Odd–Even Merge**.  
   Процессы обмениваются отсортированными фрагментами попарно, с увеличением шага в степени двойки, формируя дерево слияния.

5. **Формирование итогового результата**  
   После завершения всех этапов слияния итоговый отсортированный массив формируется на процессе с рангом 0 и рассылается всем остальным процессам.

---

## 5. Implementation Details

Реализация задачи организована модульно и состоит из следующих компонентов:

- **common**  
  Содержит общие типы данных и вспомогательные алгоритмы:
  - реализация алгоритма Shell sort;
  - реализация сети сортировки Batcher Odd–Even Merge;
  - функции обмена векторами между MPI-процессами.

- **seq**  
  Последовательная реализация задачи, использующая Shell sort для сортировки всего массива.

- **mpi**  
  Параллельная реализация задачи:
  - распределение данных с помощью `MPI_Scatterv`;
  - локальная сортировка на каждом процессе;
  - поэтапное слияние отсортированных частей.

- **tests**  
  Содержит функциональные тесты для проверки корректности работы алгоритма, а также тесты производительности для сравнения последовательной и параллельной реализаций.

---

## 6. Experimental Setup

Экспериментальные исследования проводились на следующей вычислительной системе:

- **Процессор:** 13th Gen Intel Core i5-13500H (12 ядер: 4P + 8E)
- **Оперативная память:** 16 ГБ
- **Операционная система:** Windows 11
- **Компилятор:** MSVC
- **Тип сборки:** Release
- **MPI-библиотека:** MS-MPI

---

### 7. Performance Results

Результаты измерений производительности приведены в таблице ниже. Время выполнения усреднено по нескольким запускам. В качестве базового значения использовалась последовательная реализация алгоритма.

| Mode | Processes | Time (s) | Speedup | Efficiency |
|------|-----------|----------|---------|------------|
| seq  | 1         | 0.00461  | 1.00    | N/A        |
| mpi  | 1         | 0.00418  | 1.10    | 110%       |
| mpi  | 2         | 0.11230  | 0.041   | 2.05%      |
| mpi  | 4         | 0.38290  | 0.012   | 0.30%      |
| mpi  | 8         | 1.02170  | 0.0045  | 0.056%     |

### 7.1 Analysis of Results

Из таблицы видно, что при увеличении числа MPI-процессов производительность параллельной версии существенно ухудшается. Уже при двух процессах время выполнения возрастает более чем в 20 раз по сравнению с последовательной реализацией.

Незначительное формальное ускорение при использовании одного MPI-процесса обусловлено погрешностями измерений и отсутствием реального параллелизма и не имеет практического значения.

Основной причиной наблюдаемого отрицательного ускорения являются накладные расходы на инициализацию MPI, передачу данных и синхронизацию процессов. Для рассматриваемой задачи объём вычислений слишком мал по сравнению со стоимостью межпроцессного взаимодействия.

Таким образом, применение технологии MPI для данной задачи на однопроцессорной системе с общей памятью является неэффективным и приводит к выраженному эффекту slowdown.


---

## 8. Conclusions

В ходе выполнения работы были сделаны следующие выводы:

1. Алгоритм **Shell sort** эффективно работает в последовательной реализации.
2. Использование **MPI** для данной задачи не всегда приводит к ускорению из-за высоких накладных расходов на коммуникации.
3. Алгоритм **Batcher Odd–Even Merge** обеспечивает корректное слияние отсортированных частей массива, но увеличивает объём межпроцессного взаимодействия.
4. Для задач сортировки на многопроцессорных системах с общей памятью более целесообразно использовать технологии многопоточности (OpenMP, TBB).

---

## 9. References

1. Лекции и практические занятия курса «Параллельное программирование».
2. 
