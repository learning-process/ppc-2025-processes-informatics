# <Алгоритм Кэннона. Блочная схема>

- Student: Лукин Иван Антонович, group 3823Б1ФИ3
- Technology: SEQ | MPI
- Variant: 1

## 1. Introduction
Мотивация: Исследовать эффективность распараллеливания алгоритма умножения матриц с использованием алгоритма Кэннона через MPI

Проблема: Умножение матриц имеет вычислительную сложность `O(n³)`, что делает его подходящим кандидатом для параллелизации, однако коммуникационные затраты могут ограничивать масштабируемость

Ожидаемый результат: MPI-версия должна демонстрировать ускорение при достаточно больших размерах матриц, где выигрыш от параллельных вычислений превышает накладные расходы на коммуникацию

## 2. Problem Statement
На вход поступают две квадратные матрицы размером `N×N` (тип `double`) и их размер `N`. 
Задача - вычислить произведение матриц `C = A × B` с использованием последовательного и параллельного (алгоритм Кэннона) подходов для сравнения производительности.

## 3. Baseline Algorithm (Sequential)
Базовый (последовательный) алгоритм реализует классическое умножение матриц с оптимизированным порядком циклов `ikj` для лучшей кэш-локальности.

## 4. Parallelization Scheme
Параллельный алгоритм реализует классический алгоритм Кэннона на квадратной решетке процессов. Перед распределением данных корневой процесс выполняет ручную упаковку матриц в линейные буферы: исходные матрицы `A` и `B` делятся на блоки, согласно топологии решетки, которые упаковываются в массивы в порядке, соответствующем рангам процессов-получателей. Полученные буферы распределяются по процессам с помощью `MPI_Scatter` - каждый процесс получает ровно один блок матрицы `A` и один блок матрицы `B`, соответствующий его позиции в решетке.

Перед началом вычислений выполняется начальный сдвиг: блоки матрицы `A` циклически сдвигаются влево на количество позиций, равное номеру строки процесса, а блоки матрицы `B` циклически сдвигаются вверх на количество позиций, равное номеру столбца процесса.

Основной цикл выполняется `grid_size` раз. На каждой итерации:

- Процессы перемножают локальные блоки `A` и `B`, накапливая результат в блоке `C`

- Блоки матрицы `A` сдвигаются на одну позицию влево по строке

- Блоки матрицы `B` сдвигаются на одну позицию вверх по столбцу

Сдвиги реализуются через `MPI_Sendrecv_replace`, чтобы не создавать дополнительных буфферов. После завершения цикла частичные результаты собираются на корневом процессе с помощью `MPI_Gather`, где происходит обратная распаковка блоков в итоговую матрицу. Результат рассылается всем процессам через `MPI_Bcast`.

Для неквадратного числа процессов используется максимально возможная квадратная решетка (первые `floor(√p)²` процессов). Если процессов меньше 4, выполняется последовательная версия.

## 5. Experimental Setup
- Hardware/OS: Intel I5-13420H, 8 cores, 16GB RAM, Win10 OS
- Toolchain: Microsoft Visual C++ (MSVC), MSBuild 17.14.23, Release
- Environment: PPC_NUM_PROC
- Data: Матрица `1000х1000`, состоящая из элементов `double`. 

## 6. Results and Discussion

### 6.1 Correctness
Корректность реализаций проверена функциональными тестами для матриц размером `2×2`, `4×4`, `8×8` и `12×12`. Тесты используют `epsilon`-сравнение с точностью `1e-9` для учета ошибок округления при операциях с плавающей точкой. Алгоритм корректно обрабатывает различные конфигурации процессов.

### 6.2 Performance
Это тесты производительности на матрице размером `100x100` элементов. Параллельная версия демонстрирует отрицательное ускорение: эффективность при 4 процессах составляет лишь `11.1%`, а при 16 процессах падает ниже `1%`. Это связано с тем, что вычислительная нагрузка недостаточно велика для перекрытия коммуникационных затрат.

| Mode        | Count | Time, s | Speedup | Efficiency |
|-------------|-------|---------|---------|------------|
| seq         | 1     | 0.0004  | 1.00    | N/A        |
| mpi         | 4     | 0.0009  | 0.44    | 11.1%      |
| mpi         | 16    | 0.026   | 0.02    | < 1.0%     |

Это тесты производительности на матрице размером `1000x1000` элементов. При 4 процессах достигается хорошее ускорение `3.5×` с высокой эффективностью `87.5%`, однако при масштабировании до 16 процессов эффективность резко падает до `17.5%`, что свидетельствует о росте коммуникационных затрат.

| Mode        | Count | Time, s | Speedup | Efficiency |
|-------------|-------|---------|---------|------------|
| seq         | 1     | 0.042   | 1.00    | N/A        |
| mpi         | 4     | 0.012   | 3.5     | 87.5%      |
| mpi         | 16    | 0.015   | 2.8     | 17.5%      |


## 7. Conclusions
Алгоритм Кэннона доказал свою принципиальную применимость для эффективного распараллеливания умножения матриц с использованием MPI на матрицах значительного размера. При использовании 4 процессов на матрице `1000×1000` удалось достичь высокого ускорения `3.5×` и эффективности `87.5%`. Однако масштабирование алгоритма до 16 процессов на той же матрице показало существенное падение эффективности до `17.5%`, что указывает на доминирование коммуникационных затрат при увеличении числа процессов.

Результаты на малых матрицах (`100×100`) демонстрируют полную неэффективность параллельной версии из-за абсолютного преобладания накладных расходов на организацию решетки процессов и пересылку данных над полезными вычислениями. 

Важно отметить, что классический алгоритм Кэннона имеет ряд существенных ограничений: он работает только с квадратными матрицами, требует квадратной решетки процессов и предполагает равномерное распределение блоков одинакового размера. Эти ограничения снижают его практическую универсальность и затрудняют применение для произвольных размеров матриц.

Таким образом, алгоритм Кэннона эффективен для крупных вычислительных задач с квадратными матрицами, но требует тщательного выбора количества процессов для достижения приемлемой эффективности.
## 8. References
1. Лекции по параллельному программированию ННГУ
2. Стандарт MPI