# Умножение разреженных матриц. Элементы типа double. Формат хранения матрицы – строковый (CRS).

- Студент: Романов Артем Сергеевич, группа 3823Б1ФИ3
- Технологии: SEQ | MPI
- Вариант: 4

## 1. Введение

Умножение матриц является одной из важнейших вычислительных задач, которая встречается в множестве различных областей: машинном обучении, компьютерной графике, решении систем линейных уравнений, вычислительной физике и многих других. Тривиальный алгоритм умножения матриц имеет сложность $O(n^3)$, что не позволяет применять его на матрицах больших размеров.

Однако, в реальных задачах часто встречаются разреженные матрицы – матрицы, в которых большинство элементов равны нулю. Они возникают при работе с графами, в машинном обучении и во многих других областях. Хранение разреженных матриц в полном формате (со всеми нулями) крайне неэффективно как с точки зрения использования памяти, так и с точки зрения дальнейших вычислений.

Для эффективной работы с разреженными матрицами разработаны специальные форматы хранения, наиболее популярным из которых является CRS (Compressed Row Storage). Этот формат позволяет хранить только ненулевые элементы матрицы, значительно экономя память и ускоряя доступ к данным. Но умножение матриц в формате CRS требует специальных алгоритмов, учитывающих разреженную структуру данных, – однако благодаря этому подходу сама операция умножения выполняется значительно быстрее, так как вычисления производятся только над существенными элементами, а не над всей плотной матрицей.

Целью данной работы является реализации алгоритма умножения разреженных матриц, хранящихся в формате CRS, с элементами типа double, а также его распараллеливание с помощью технологии MPI, – комбинация эффективного формата хранения и параллельных вычислений может позволить достичь большого прироста производительности при работе с данными больших объёмов.

## 2. Постановка задачи

Пусть даны две разреженные квадратные матрицы  $A, B \in \mathbb{R}^{n \times n}$, хранящиеся в формате CRS. Требуется вычислить их произведение $C = A \times B$, результат которого также должен быть представлен в формате CRS.

**Входные данные:**
- $A$ – разреженная квадратная матрица размерности $n \times n$, представленная в формате CRS, с элементами типа `double`;
- $B$ – разреженная квадратная матрица размерности $n \times n$, представленная в формате CRS, с элементами типа `double`.

**Выходные данные:**
- $C$ – разреженная квадратная матрица-результат размерности $n \times n$, представляющая собой произведение $C = A \times B$ и хранящаяся в формате CRS.

## 3. Базовый алгоритм (последовательный)

В последовательной реализации умножения разреженных матриц в формате CRS используется классический алгоритм матричного умножения, но вычисления выполняются только для ненулевых (существующих) элементов. Формат CRS хранит для каждой строки список ненулевых значений и их столбцов, что позволяет алгоритму перебирать только необходимые элементы, избегая операции с нулями.

Пусть заданы матрицы $A$ и $B$ размера $n \times n$ в формате CRS. Для удобства матрица $B$ предварительно транспонируется для ускорения поиска совпадающих столбцов при перемножении строк $A$ и столбцов $B$.

Последовательный алгоритм работает следующим образом:

1. Для каждой строки $i$ матрицы $A$ выбираются все её ненулевые элементы;
2. Для каждого столбца $j$ матрицы $B$ (фактически – строки транcпонированной матрицы $B^T$) выполняется "слияние" двух отсортированных списков ненулевых элементов:
   * ненулевых элементов строки $A_i$;
   * ненулевых элементов столбца $B_j$.
3. Если индексы ненулевых элементов совпадают, их произведение суммируется;
4. Если итоговая сумма не равна нулю, в результирующую матрицу $C$ добавляется элемент $C_{ij}$.

Таким образом, вычисляются значения только тех элементов $C_{ij}$, которые потенциально могут быть ненулевыми. Это позволяет значительно сократить количество операций по сравнению с плотным алгоритмом.

Данный алгоритм может быть описан следующим псевдокодом:

```
for i in 0..n-1:
    for j in 0..n-1:
        sum = 0
        ptrA = элементы строки i в A
        ptrB = элементы столбца j в B
        while ptrA и ptrB не закончились:
            если ptrA.col == ptrB.col:
                sum += ptrA.val * ptrB.val
                ptrA++, ptrB++
            иначе сдвигаем меньший индекс
        если sum != 0:
            C.add(i, j, sum)
```


Результат также записывается в формате CRS: для каждой строки запоминаются индексы столбцов ненулевых элементов и соответствующие значения.


## 4. Параллелизация

Параллельная версия кода основана на том, что строки матрицы $A$ можно умножать на матрицу $B$ независимо друг от друга, поэтому матрицу $A$ удобно разрезать на блоки строк (матрицы меньшего размера) и распределить между процессами, а матрицу $B$ – разослать всем один раз.

В начале работы корневой процесс (нулевой) считывает обе матрицы, выполняет широковещательную передачу матрицы $B$ (с помощью специальной написанной функции `BroadcastCRS`) и вычисляет число строк, которое достаётся каждому процессу: $cnt = \left\lceil\frac{n}{p}\right\rceil$

Далее корневой процесс отправляет каждому процессу его блок строк матрицы $A$ (с помощью `SendCRS`). Если строк на всех не хватает, некоторым процессам передаётся пустая матрица. Каждый процесс получает свой блок (благодаря `RecvCRS`) и локально умножает его на матрицу $B$.

После завершения вычислений корневой процесс собирает частичные результаты от всех процессов и объединяет их в одну матрицу. Полученная результирующая матрица $C$ в конце рассылается всем процессам (из-за особенностей фреймворка).

Данную схему работы можно описать следующим псевдокодом:

```
if rank == 0:
    read A, B

РазослатьВсем(B)

if rank == 0:
    Разделить A на блоки A0, A1, ..., Ap-1
    for i in 1..n-1:
        Отправить Ai Процессу i
else:
    Получить Ai

if Ai не empty:
    Ci = Ai * B
else:
    Ci = empty

if rank == 0:
    Собрать все Ci от всех процессов
    C = Объединить(C0 ... Cp-1)
else:
    Отправить Ci на процесс 0

РазослатьВсем(C)

```

## 5. Детали реализации

В рамках работы были реализованы как структуры данных для плотных и разреженных матриц, так и вспомогательные функции, необходимые для удобной передачи данных между MPI-процессами.

### 5.1 Структуры данных

#### **Плотные матрицы**

Для задания функциональных тестов используется вспомогательная структура `Dense`, представляющая плотную матрицу. Она хранит:

- количество строк `n` и столбцов `m`;
- одномерный вектор `data` длины `n * m` с элементами типа `double`.

В вычислениях данный тип матрицы не используется, и в тестах конвертируется в CRS формат.

#### **Разреженные матрицы**

Структура `CRS` реализует строковый формат хранения разреженной матрицы. Она содержит следующие поля:

- `n`, `m` – число строк и столбцов;
- `value` – список ненулевых значений;
- `column` – соответствующие индексы столбцов;
- `row_index` – массив размеров `n + 1`, содержащий указатели на начало соответствующих строк в массивах `value`/`column`.

Для удобной работы с ней реализованы функции и методы, которые позволяют делать следующее:
- Cравнивать матрицы с учётом погрешности;
- Транспонировать матрицу;
- Заполнять случайными данными;
- Извлекать блоки строк;
- Конкатенировать несколько блоков строк в одну большую матрицу.

Также реализован оператор умножения, реализующий последовательный алгоритм умножения CRS матриц.


### 5.2 Вспомогательные функции для MPI

Поскольку структура CRS не является тривиальным объектом и содержит динамические массивы, для корректного обмена данными между MPI-процессами были реализованы три вспомогательные функции:

- **`BroadcastCRS`** - выполняет широковещательную передачу матрицы CRS от корневого процесса ко всем остальным;
- **`SendCRS`** - отправляет CRS матрицу на указанный процесс;
- **`RecvCRS`** - принимает CRS матрицу с указанного процесса.


## 6. Условия проведения экспериментов
- **Аппаратное обеспечение/ОС:** Intel Core i5-10400f, 6 ядер/12 логических процессоров, 32GB RAM DDR4 2667 Mhz, Ubuntu 24.04.2/Docker (под Windows 10 Home 22H2);
- **Инструменты сборки:** GCC 13.3.0, Release, Cmake 3.28.3;
- **Переменные окружения:** Не использовались (запуск тестов происходил с флагом `-np <x>`);
- **Данные:** Тестовые данные генерировались вручную.

## 7. Результаты

### 7.1 Корректность

Код последовательной и параллельной версии алгоритма проверялся на некоторой группе тестов, включающей в себя различные матрицы, для которых находилось их произведение.

В каждом тесте проверялось, что в результирующей матрицы элементы отличаются от элемнтов в матрице-ответе не более чем на $\epsilon = 10^{-6}$.

### 7.2 Производительность

Время, ускорение и эффективность измерялись на следующем тесте:
- Размер матриц: $1000 \times 1000$;
- Процент заполнения: ~ $10$ %, т.е. примерно $10^5$ элементов;
- Матрицы генерируются псведослучайно на основании заранее прописанного сида генерации.

В результате запуска последовательной версии алгоритма и параллельной на различном числе процессов были получены следующие данные:

| Режим | Число процессов | Время, с | Ускорение | Эффективность |
| ----- | --------------- | -------- | --------- | ------------- |
| SEQ   | 1               | 0.76     | 1.00      | N/A           |
| MPI   | 2               | 0.42     | 1.81      | 90.5%         |
| MPI   | 3               | 0.30     | 2.53      | 84.3%         |
| MPI   | 4               | 0.24     | 3.16      | 79.0%         |
| MPI   | 5               | 0.20     | 3.80      | 76.0%         |
| MPI   | 6               | 0.19     | 4.00      | 66.7%         |
| MPI   | 7               | 0.18     | 4.22      | 60.3%         |
| MPI   | 8               | 0.17     | 4.47      | 55.6%         |
| MPI   | 9               | 0.15     | 5.06      | 56.2%         |
| MPI   | 10              | 0.15     | 5.06      | 50.6%         |
| MPI   | 11              | 0.14     | 5.42      | 49.3%         |
| MPI   | 12              | 0.16     | 4.75      | 39.6%         |
| MPI   | 16              | 0.42     | 1.81      | 11.3%         |
| MPI   | 24              | 0.67     | 1.13      | 4.7%          |

При использовании не более **6 процессов** (число физических ядер) наблюдается достаточно эффективное ускорение, что соответствует ожиданиям – каждый процесс выполняется на отдельном физическом ядре, выполняя требуемые вычисления.

При увеличении числа процессов начинают использоваться логические (виртуальные) ядра. Рост производительности замедляется, а эффективность падает. Это связано с тем, что логические ядра не дают реального удвоения вычислительных ресурсов: процессы начинают конкурировать за разделяемый между ними кэш, ALU, ...

После **11–12 процессов** ускорение начинает уменьшается. Это может быть, например, связано с:
- увеличением накладных расходов, связанных с обменом данными (возможно, из-за недостаточно эффективной схеме обмена);
- увеличением числа переключений контекста.

## 8. Заключение

В данной работе был реализован алгоритм умножения разреженных матриц в формате CRS и его параллельная версия с использованием технологии MPI. Эксперименты показали, что распараллеливание действительно ускоряет вычисления: на физических ядрах прирост производительности заметный и стабильный. Однако, при дальнейшем увеличении числа процессов эффективность падает из-за накладных расходов MPI и особенностей работы логических ядер. Параллельный вариант показывает хорошую масштабируемость в пределах возможностей оборудования.

## 9. Список литературы
1. Мееров И.Б., Сысоев А.В. (при участии Сафоновой Я.) Лабораторная работа №7: Разреженное матричное умножение: учеб.–метод. пособие по курсу «Параллельные численные методы» / Нижегородский государственный университет, Фак. вычисл. математики и кибернетики; при поддержке компании Intel. – Нижний Новгород, 2011. – 82 с;
2. Мееров И.Б. (при участии Лебедева С.А., Пировой А.Ю.) Оптимизация структур данных при работе с разреженными матрицами. Введение в анализ производительности и оптимизацию программ [Электронный ресурс]. – Нижегородский государственный университет. – URL: https://hpc-education.unn.ru/files/courses/optimization/2_3_SparseDS_Lect.pdf (дата обращения: 24.11.2025);
3. Оболенский А., Нестеров А. Parallel Programming course. MPI (detailed API overview) [Электронный ресурс]. – Нижегородский государственный университет. – URL:  https://learning-process.github.io/parallel_programming_slides/slides/03-mpi-api.pdf (дата обращения: 24.12.2025);
4. Оболенский А., Нестеров А. Parallel Programming course. Introduction [Электронный ресурс]. – Нижегородский государственный университет. – URL:  https://learning-process.github.io/parallel_programming_slides/slides/01-intro.pdf (дата обращения: 24.12.2025).