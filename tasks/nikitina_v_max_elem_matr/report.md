# Поиск максимального элемента в целочисленной матрице

-   **Студент:** `<Никитина Валерия Владимировна>`, группа `<3823Б1ФИ2>`
-   **Технология:** SEQ, MPI
-   **Вариант:** `<13>`

## 1. Введение

Поиск экстремальных значений является одной из фундаментальных операций в анализе данных. При работе с большими матрицами последовательное выполнение этой задачи может занимать значительное время. Распараллеливание алгоритма с использованием технологии Message Passing Interface (MPI) позволяет сократить время вычислений за счет одновременной обработки разных частей данных на нескольких вычислительных узлах.

Целью данной работы является реализация и сравнение производительности последовательного и параллельного (MPI) алгоритмов для поиска максимального элемента в целочисленной матрице, а также обеспечение полной корректности и тестового покрытия реализованного решения.

## 2. Постановка задачи

**Входные данные:** Целочисленная матрица, представленная в виде одномерного вектора `std::vector<int>`. Первые два элемента вектора задают размеры матрицы: количество строк `rows` и столбцов `cols`. Остальные `rows * cols` элементов представляют собой данные матрицы.

**Выходные данные:** Одно целое число — максимальное значение среди всех элементов матрицы.

**Ограничения:**
-   Элементы матрицы — целые числа.
-   Матрица с нулевыми размерами (например, `10x0` или `0x10`) является валидным входным данным. Для таких матриц результат равен `INT_MIN`.
-   Входные данные с отрицательными размерами или некорректным количеством элементов считаются невалидными.

## 3. Описание алгоритма (базового/последовательного)

Последовательный алгоритм представляет собой простой линейный обход всех элементов матрицы.

1.  Проводится валидация входных данных: проверяется, что размеры матрицы неотрицательные и соответствуют фактическому количеству переданных элементов.
2.  Инициализируется переменная `global_max` минимально возможным значением для типа `int` (`INT_MIN`).
3.  Если матрица не пуста, организуется цикл, который итерируется по каждому элементу.
4.  На каждой итерации текущий элемент матрицы сравнивается со значением `global_max`. Если текущий элемент больше, `global_max` обновляется.
5.  После завершения обхода переменная `global_max` содержит искомое максимальное значение.

Временная сложность данного алгоритма составляет O(N × M), где N и M — размеры матрицы.

## 4. Схема распараллеливания

Для распараллеливания задачи с использованием технологии MPI была выбрана модель "Мастер-Рабочий" (Master-Worker). Декомпозиция данных выполняется путем разделения исходной матрицы на примерно равные непрерывные части.

-   **Валидация:** Каждый MPI-процесс самостоятельно выполняет проверку входных данных для достижения 100% тестового покрытия ветвей кода.

-   **Схема обменов данными:**
    1.  **Процесс с рангом 0 (Мастер):** Хранит полную исходную матрицу.
    2.  **`MPI_Scatterv`:** Мастер распределяет данные. Он вычисляет, сколько элементов будет обрабатывать каждый процесс, и рассылает соответствующие части матрицы всем процессам (включая себя).
    3.  **Локальные вычисления:** Каждый процесс находит максимальный элемент в своей локальной части (`local_max`).
    4.  **`MPI_Reduce`:** Все процессы участвуют в коллективной операции редукции, в результате которой на 0-м процессе вычисляется глобальный максимум (`global_max`).
    5.  **`MPI_Bcast`:** Процесс 0 рассылает финальный `global_max` всем остальным процессам. Этот шаг критически важен, так как тестовый фреймворк проверяет корректность результата на каждом процессе.

Эта схема позволяет эффективно распределить вычислительную нагрузку, минимизируя коммуникации.

## 5. Экспериментальная установка

-   **Окружение:** Разработка и локальное тестирование проводились в контейнере Docker на базе Ubuntu.
-   **CPU:** Предоставлено средой выполнения (2 виртуальных ядра).
-   **Toolchain:**
    -   Компилятор: GCC 14.2.0
    -   Система сборки: CMake
    -   Тип сборки: `Release`
-   **Переменные окружения:** Эксперименты проводились при `PPC_NUM_PROC=2`.
-   **Данные для тестов:**
    -   **Функциональные тесты:** Матрицы различных размеров, включая граничные и невалидные случаи.
    -   **Тесты производительности:** Матрица размером **12000x12000**, заполненная случайными целыми числами.

## 6. Результаты и обсуждение

### 6.1. Корректность и тестовое покрытие

Корректность работы алгоритмов проверялась с помощью исчерпывающего набора GTest'ов, включающего как тесты на валидных данных, так и тесты, проверяющие обработку некорректных входных данных. Благодаря этому было достигнуто **100% покрытие строк и ветвей кода**, что гарантирует надежность реализованных функций.

### 6.2. Производительность

Измерения времени выполнения проводились на матрице 12000x12000. Ускорение (Speedup) вычислялось как S(p) = T(1) / T(p), а эффективность (Efficiency) как E(p) = S(p) / p.

| Режим | Кол-во процессов (p) | Время, с | Ускорение (S) | Эффективность (E) |
| :---- | :------------------: | :------- | :------------ | :---------------- |
| seq   |           1          | **0.415**| 1.00          | 100%              |
| mpi   |           2          | **0.224**| 1.85          | 92.5%             |

**Анализ результатов:**

Параллельная MPI-реализация на двух процессах показала значительное **ускорение в 1.85 раза** по сравнению с последовательной версией. Это близко к идеальному двукратному ускорению, что свидетельствует о высокой эффективности распараллеливания.

Эффективность в `92.5%` означает, что только 7.5% времени было потрачено на накладные расходы, связанные с работой MPI (раздача данных и сбор результатов). Такой высокий показатель подтверждает, что для данной задачи на двух процессах вычислительная нагрузка хорошо распределяется, а коммуникационные издержки минимальны.

## 7. Выводы

В ходе выполнения работы были успешно реализованы, протестированы и отлажены последовательный и параллельный (MPI) алгоритмы для поиска максимального элемента в матрице.

Экспериментальные замеры показали, что MPI-реализация на двух процессах достигает ускорения `1.85x` при эффективности `92.5%`. Это подтверждает, что выбранная схема распараллеливания является высокоэффективной для данной задачи и позволяет значительно сократить время выполнения по сравнению с последовательным подходом.

## 8. Источники

1.  Open MPI Documentation — [https://www.open-mpi.org/doc/](https://www.open-mpi.org/doc/)
2.  Peter S. Pacheco. Parallel Programming with MPI. Morgan Kaufmann, 1996.