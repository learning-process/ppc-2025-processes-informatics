# Поиск максимального элемента в целочисленной матрице

-   **Студент:** `<Никитина Валерия Владимировна>`, группа `<3823Б1ФИ2>`
-   **Технология:** SEQ, MPI
-   **Вариант:** `<13>`

## 1. Введение

Поиск экстремальных значений является одной из фундаментальных операций в анализе данных. При работе с большими матрицами последовательное выполнение этой задачи может занимать значительное время. Распараллеливание алгоритма с использованием технологии MPI позволяет сократить время вычислений за счет одновременной обработки разных частей данных на нескольких вычислительных узлах.

Целью данной работы является реализация и сравнение производительности последовательного и параллельного (MPI) алгоритмов для поиска максимального элемента в целочисленной матрице.

## 2. Постановка задачи

**Входные данные:** Целочисленная матрица, представленная в виде одномерного вектора `std::vector<int>`. Первые два элемента вектора задают размеры матрицы: количество строк `rows` и столбцов `cols`. Остальные `rows * cols` элементов представляют собой данные матрицы.

**Выходные данные:** Одно целое число — максимальное значение среди всех элементов матрицы.

**Ограничения:**
-   Элементы матрицы — целые числа.
-   Матрица может иметь нулевые размеры. В этом случае максимальный элемент считается равным `INT_MIN`.

## 3. Описание алгоритма (базового/последовательного)

Последовательный алгоритм представляет собой простой линейный обход всех элементов матрицы.

1.  Инициализируется переменная `global_max` минимально возможным значением для типа `int` (`INT_MIN`).
2.  Организуется цикл, который итерируется по каждому элементу входной матрицы.
3.  На каждой итерации текущий элемент матрицы сравнивается со значением `global_max`.
4.  Если текущий элемент больше, чем `global_max`, то `global_max` обновляется значением этого элемента.
5.  После завершения обхода всех элементов переменная `global_max` содержит искомое максимальное значение.

Временная сложность данного алгоритма составляет O(N × M), где N и M — размеры матрицы.

## 4. Схема распараллеливания

Для распараллеливания задачи с использованием технологии MPI была выбрана модель "Мастер-Рабочий" (Master-Worker). Декомпозиция данных выполняется путем разделения исходной матрицы (представленной как одномерный массив) на примерно равные непрерывные части.

-   **Процесс с рангом 0 (Мастер):**
    1.  Хранит полную исходную матрицу.
    2.  Вычисляет, сколько элементов будет обрабатывать каждый процесс.
    3.  Распределяет части матрицы между всеми процессами с помощью коллективной операции `MPI_Scatterv`.
    4.  Собирает локальные максимумы от всех процессов с помощью `MPI_Reduce` с операцией `MPI_MAX`, получая итоговый `global_max`.
    5.  Рассылает итоговый результат всем остальным процессам с помощью `MPI_Bcast`.

-   **Процессы с рангом > 0 (Рабочие):**
    1.  Получают свою часть данных от Мастера.
    2.  Находят максимальный элемент в своей локальной части (`local_max`).
    3.  Отправляют свой `local_max` Мастеру в рамках операции `MPI_Reduce`.
    4.  Получают итоговый `global_max` от Мастера.

Эта схема позволяет эффективно распределить вычислительную нагрузку и минимизировать объем передаваемых данных на этапе сбора результатов.

## 5. Экспериментальная установка

-   **Окружение:** Разработка велась в контейнере Docker на базе Ubuntu.
-   **CPU:** Предоставлено средой выполнения (например, 2 виртуальных ядра).
-   **Toolchain:**
    -   Компилятор: GCC 14.2.0
    -   Система сборки: CMake
    -   Тип сборки: `Release`
-   **Переменные окружения:** Эксперименты проводились при `PPC_NUM_PROC=2`.
-   **Данные для тестов:**
    -   Тесты производительности проводились на матрице размером **6000x6000**, заполненной случайными целыми числами в диапазоне [-1000, 1000].

## 6. Результаты и обсуждение

### 6.1. Корректность

Корректность работы параллельного алгоритма проверялась путем сравнения его результатов с результатами эталонной последовательной реализации. Функциональные тесты показали идентичные результаты для SEQ и MPI версий. Критически важным для прохождения тестов оказалось добавление `MPI_Bcast` для финального результата, чтобы все процессы получили корректный ответ для проверки.

### 6.2. Производительность

Измерения времени выполнения проводились на матрице 6000x6000. Ускорение (Speedup) вычислялось как S(p) = T(1) / T(p), а эффективность (Efficiency) как E(p) = S(p) / p, где T(1) — время последовательной версии, T(p) — время MPI-версии на `p` процессах.

| Режим | Кол-во процессов (p) | Время, с | Ускорение (S) | Эффективность (E) |
| :---- | :------------------: | :------- | :------------ | :---------------- |
| seq   |           1          | **0.240**| 1.00          | 100%              |
| mpi   |           2          | **0.128**| 1.88          | 94.0%             |

**Анализ результатов:**

Параллельная MPI-реализация на двух процессах показала значительное **ускорение в 1.88 раза** по сравнению с последовательной версией. Это близко к идеальному двукратному ускорению, что свидетельствует о высокой эффективности распараллеливания.

Эффективность в `94.0%` означает, что только 6% времени было потрачено на накладные расходы, связанные с работой MPI (раздача данных через `MPI_Scatterv` и сбор результатов через `MPI_Reduce`). Такой высокий показатель подтверждает, что для данной задачи и на двух процессах вычислительная нагрузка хорошо распределяется, а коммуникационные издержки минимальны.

## 7. Выводы

В ходе выполнения работы были успешно реализованы и протестированы последовательный и параллельный (MPI) алгоритмы для поиска максимального элемента в матрице.

Экспериментальные замеры показали, что MPI-реализация на двух процессах достигает ускорения `1.88x` при эффективности `94.0%`. Это подтверждает, что выбранная схема распараллеливания с декомпозицией данных является эффективной для данной задачи и позволяет значительно сократить время выполнения по сравнению с последовательным подходом.

## 8. Источники

1.  Open MPI Documentation — [https://www.open-mpi.org/doc/](https://www.open-mpi.org/doc/)
2.  Peter S. Pacheco. Parallel Programming with MPI. Morgan Kaufmann, 1996.