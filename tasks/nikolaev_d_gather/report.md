# Обобщённая передача данных от всех процессов одному (MPI_Gather)

- Студент: Николаев Денис Андреевич, группа 3823Б1ФИ1
- Технология: SEQ | MPI
- Вариант: 5

## 1. Введение 

В рамках стандарта MPI операция Gather является фундаментальным примитивом для сбора данных со всех процессов в буфер процесса-получателя. Цель лабораторной работы направлена на разработку корректной и масштабируемой реализации этой операции и анализ её быстродействия в сравнении с последовательным методом.

## 2. Постановка задачи

- Задача: Реализовать обобщённую передачу данных от всех процессов одному.
- Входные данные: массив data типа std::vector<char>, count - количество элементов на процесс, datatype  тип данных (MPI_INT, MPI_FLOAT,MPI_DOUBLE), root - номер корневого процесса.
- Выходные данные: на корневом процессе - массив, содержащий данные всех процессов, упорядоченные по рангу, на остальных процессах - пустой результат.

## 3. Базовый алгоритм (последовательный) 

- В последовательном варианте задача реализуется копирование входного буфера в выходной.

Алгоритм:
- Проверка корректности входных данных.
- Копирование входного массива в выходной.
- Возвращение результата.

## Сложность алгоритма:
- Временная: O(n).
- Пространственная: O(n).

## 4. Схема параллелизации
Для реализации операции `MPI_Gather` применена древовидная схема (tree-based gather). Её суть заключается в следующем:
- процессы перенумеровываются относительно корня;
- сбор данных происходит по этапам с пронумерованным шагом;
- на каждом этапе часть процессов отправляет накопленные данные процессам-получателям;
- корневой процесс в конце получает данные от всех процессов.

## Коммуникации выполняются с помощью:
- `MPI_Send`
- `MPI_Recv`
- Для восстановления порядка данных хранится дополнительный массив рангов, который соответствует каждому блоку данных.

## 5. Детали реализации

## Структура проекта
- `common.hpp` — общие типы данных
- `ops_seq.hpp / ops_seq.cpp` — последовательная версия
- `ops_mpi.hpp / ops_mpi.cpp` — MPI версия
- `functional_tests.cpp` — функциональные тесты
- `performance_tests.cpp` — тесты производительности

## Основные классы
- `NikolaevDGatherSEQ` — последовательная реализация
- `NikolaevDGatherMPI` — MPI-реализация 

## Основные методы
- `ValidationImpl()` — проверка корректности входных данных
- `PreProcessingImpl()` — подготовка
- `RunImpl()` — основной алгоритм
- `PostProcessingImpl()` — завершение работы

## Особенности реализации
- Поддержка типов int, float, double
- Корректная работа при size = 1
- Корректная работа при count = 1
- Использование собственного алгоритма TreeGatherImpl
- Отсутствие вызовов MPI_Gather

## Использование памяти
- **SEQ**: O(N)
- **MPI**: O(N) на корне

## Граничные случаи
- Count <= 0 - ошибка валидации
- Некорректный datatype - ошибка
- Root >= size - ошибка
- Один процесс
- Средний корень 
- Большие объёмы данных

## 6. Экспериментальная установка

### Оборудование
- CPU: AMD Ryzen 5 2600 (6 cores, 12 threads, 3.4 GHz)
- RAM: 16 GB
- OS: Windows 10, сама программа была запущена через WSL2 на Ubuntu 24.04.2 LTS.
- Toolchain: GCC 13.3.0, Release build

## Параметры тестирования

**Разные сценарии**: 
- Сбор данных при root = 0
- Минимальный размер данных (count = 1)
- Большие объёмы данных
- Различные типы данных (int, float, double)
- Проверка корректного порядка данных в выходном буфере

**Функциональные тесты**:
- Размер блока данных от 1 до 1000 элементов
- Типы данных: MPI_INT, MPI_FLOAT, MPI_DOUBLE
- Корневой процесс: 0

## Тесты производительности
- Длина: 50'000'000 элементов
- Типы данных: MPI_INT, MPI_FLOAT, MPI_DOUBLE
- Количество процессов MPI: 1, 2, 3, 4
- Режимы выполнения: task_run, pipeline

## 7. Результаты и обсуждение

## 7.1 Корректность

## Все функциональные тесты успешно пройдены
Тесты учитывают: 
- Базовый gather
- Неверные входные данные
- Большие массивы
- SEQ и MPI версии


### 7.2 Производительность

Тестирование проводилось с вектором длиной в 50'000'000 элементов 

- task_run:

| Режим | Процессов | Время, с | Ускорение | Эффективность |
| ----- | --------: | -------: | --------: | ------------: |
| seq   |         1 |  0.02262 |      1.00 |           N/A |
| mpi   |         1 |  0.28752 |      0.08 |          7.9% |
| mpi   |         2 |  0.57230 |      0.04 |          2.0% |
| mpi   |         3 |  0.96400 |      0.02 |          0.8% |
| mpi   |         4 |  1.29530 |      0.02 |          0.4% |

- task_pipeline:

| Режим | Процессов | Время, с | Ускорение | Эффективность |
| ----- | --------: | -------: | --------: | ------------: |
| seq   |         1 |  0.02231 |      1.00 |           N/A |
| mpi   |         1 |  0.29649 |      0.08 |          7.5% |
| mpi   |         2 |  0.82388 |      0.03 |          1.4% |
| mpi   |         3 |  1.43997 |      0.02 |          0.5% |
| mpi   |         4 |  2.01992 |      0.01 |          0.3% |

## Анализ производительности

Расчёт метрик:
- Ускорение = T_seq / T_mpi
- Эффективность = (Speedup / P) × 100%

## Анализ результатов

**task_run**:
- На одном процессе производительность MPI-версии значительно ниже, чем у последовательной реализации, из-за затрат на инициализацию MPI, организацию обменов и фактического алгоритма SEQ.
- При увеличении количества процессов до 2, 3 и 4 время выполнения продолжает увеличиваться, поскольку коммуникационные издержки преобладают над полезными вычислениями.

**task_pipeline**:
- На одном процессе MPI-реализация также демонстрирует худшую производительность по сравнению с последовательной версией, так как накладные расходы не перекрываются за счет конвейеризации.
- С ростом числа процессов время выполнения увеличивается ещё сильнее из-за возрастающих затрат на коммуникацию и синхронизацию между этапами.

## Вывод:
- Режим task_run показывает более стабильное поведение по сравнению с 
pipeline, однако не обеспечивает ускорения относительно последовательной 
реализации.
- Режим pipeline оказывается наименее эффективным для данной задачи из-за 
высоких накладных расходов на коммуникацию.
- Для операции gather, ориентированной на передачу данных к одному процессу, 
коммуникационные издержки существенно превышают выигрыш от параллелизма.

## 8. Выводы
- Режим task_run отличается более предсказуемой производительностью, чем pipeline, однако он также не превосходит по скорости последовательное выполнение.
- Режим task_pipeline оказывается наименее эффективным для данной задачи из-за особенно высоких накладных расходов.
- Для операции gather, в рамках которой все данные собираются на одном процессе, коммуникационные затраты значительно превышают выигрыш от параллельной обработки.

## 9. Список литературы

1. MPI Forum. **MPI: A Message-Passing Interface Standard. Version 3.1** [Электронный ресурс]. — Режим доступа: https://www.mpi-forum.org/docs/
2. Лекции по параллельному программированию
3. Практические занятия по параллельному программированию
