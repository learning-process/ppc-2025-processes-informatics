# Поиск максимального значения в каждой строке матрицы

- Студент: Чащин Владимир Александрович, группа 3823Б1ФИ3
- Технология: SEQ, MPI
- Вариант: 15

## 1. Введение

В данной работе рассматривается задача передачи данных между процессами с использованием MPI и реализации broadcast с деревообразной схемой передачи сообщений.

## 2. Постановка задачи

**Дано:**

* Набор данных, который должен быть доступен всем процессам.
* Число процессов p и схема деревообразной передачи данных для broadcast.

**Требуется:**

* Реализовать передачу данных от корневого процесса к остальным процессам с использованием MPI.
* Измерить время выполнения операции broadcast для различного числа процессов.
* Рассчитать ускорение (Speedup) и эффективность (Efficiency) параллельной реализации.

**Входные данные:**

* Данные на корневом процессе (например, массив или вектор).
* Количество процессов, участвующих в broadcast.

**Выходные данные:**

* Копии данных на всех процессах после выполнения broadcast.
* Время выполнения broadcast для каждого числа процессов.
* Метрики ускорения и эффективности.

**Типы данных:**

```cpp
using InTypeI = std::vector<int>;
using OutTypeI = std::vector<int>;

using InTypeF = std::vector<float>;
using OutTypeF = std::vector<float>;

using InTypeD = std::vector<double>;
using OutTypeD = std::vector<double>;
```

Эти типы данных используются для передачи массивов или векторов различных типов в MPI broadcast.

## 3. Последовательный алгоритм

Последовательный алгоритм предлагает заглушку, которая копирует данные из входа на выход 

## 4. Схема распараллеливания

Код реализует деревообразный алгоритм MPI broadcast с использованием битовой маски для определения источника и назначения передачи данных:

* Определяются ранг текущего процесса и общее число процессов (`MPI_Comm_rank`, `MPI_Comm_size`).
* В зависимости от типа данных `T` выбирается соответствующий `MPI_Datatype` (int, float, double).
* Используется деревообразная схема передачи с виртуальными рангами (`virtual_rank`) для корректной адресации данных относительно корневого процесса.
* Цикл с маской `mask` организует передачу по дереву: процессы с `virtual_rank & mask == 0` отправляют данные, остальные получают и прекращают дальнейшее участие в цикле.
* Передача данных реализована через `MPI_Send` и `MPI_Recv`, что обеспечивает log(p) шагов для доставки данных всем процессам.
* После завершения цикла процесс копирует полученные данные в выходной вектор.

## 4. Экспериментальная установка

	* CPU: Intel Core i5-12500H
	* 4 производительных ядра
	* 8 энергоэффективных ядер
	* RAM: 16 GB
	* OS: Windows 11 Pro 24H2
	* Компилятор: Intel C++ Compiler 2025
	* Матрица: детерминированная, квадратная, 20000×20000
	* Время — среднее по 8 повторениям.

## 5. Результаты и обсуждение

### 5.1 Корректность

Функциональные тесты покрывают 97% кода, включая:
	* матрицы разных размеров;
	* строку длины 0;
	* матрицу из одной строки;
	* большие матрицы;
	* соответствие MPI-версии последовательному алгоритму.

Все тесты пройдены, расхождений нет.
Корректность параллельной реализации подтверждена.

### 5.2 Производительность

Для оценки производительности измерялось чистое время выполнения алгоритма без учета создания тестовых данных. На основе полученных данных были рассчитаны метрики ускорения (Speedup) и эффективности (Efficiency).

| Режим | Число процессов | Время, ms | Ускорение | Эффективность |
| ----- | --------------- | --------- | --------- | ------------- |
| seq   | 1               | 7.27      | 1.00      | —             |
| mpi   | 1               | 0.019     | 38        | 3800%         |
| mpi   | 2               | 6.59      | 1.10      | 55%           |
| mpi   | 4               | 19.51     | 0.37      | 9%            |
| mpi   | 8               | 36.55     | 0.20      | 2.5%          |
| mpi   | 12              | 60.60     | 0.12      | 1.0%          |
| mpi   | 16              | 109.27    | 0.07      | 0.44%         |
| mpi   | 24              | 200.72    | 0.04      | 0.17%         |
| mpi   | 32              | 306.16    | 0.02      | 0.06%         |


**Анализ результатов:**

* На 1 процессе MPI реализация не выполняет копирование и считает, что данные уже есть у процесса 0. Это объясняет экстремально низкое время (0.019 мс) и неестественно высокое ускорение (38x) и эффективность (3800%).
* На 2 процессах время выполнения (6.59 мс) почти совпадает с SEQ (7.27 мс). Здесь происходит одно реальное копирование, и эффективность составляет 55%.
* При увеличении числа процессов выше 2 наблюдается ухудшение производительности: время растет, ускорение падает ниже 1, а эффективность стремится к нулю.
* Реализация использует деревообразный паттерн передачи данных (0→1, 0→2, 1→3, 0→4, 1→5, 2→6, 3→7 и т.д.), что теоретически должно обеспечивать сложность O(log(p)), однако на практике накладные расходы на организацию MPI-сообщений и синхронизацию процессов приводят к росту времени выполнения с увеличением числа процессов.

## 6. Выводы

* MPI broadcast с деревообразной схемой передачи данных масштабируется лучше, чем последовательные send/receive ко всем процессам, но накладные расходы на коммуникацию все еще могут снижать эффективность.
* Для небольшого числа процессов (p=2) накладные расходы MPI умеренные, но начиная с 4 процессов эффективность заметно падает.
* Для реального масштабирования необходимо учитывать оптимизацию MPI-сообщений и минимизацию синхронизационных задержек.

## 7. Источники

1.  Parallel Programming Course - [https://learning-process.github.io/parallel_programming_course/ru/](https://learning-process.github.io/parallel_programming_course/ru/)
2.  Parallel Programming 2025-2026 Video-Records - [https://disk.yandex.ru/d/NvHFyhOJCQU65w](https://disk.yandex.ru/d/NvHFyhOJCQU65w)
3.  Open MPI: Documentation — [https://www.open-mpi.org/doc/](https://www.open-mpi.org/doc/)