# Обобщённая передача от одного всем (scatter)
- Студент: Балдин Андрей Леонидович, 3823Б1ФИ3
- Технология: SEQ, MPI 
- Вариант: 4

---

## 1. Введение
Коллективные операции обмена данными являются фундаментальной частью стандарта MPI. Операция `MPI_Scatter` используется для рассылки частей единого буфера с корневого процесса всем остальным процессам в коммуникаторе. 
**Цель работы** — разработать собственную реализацию функции Scatter, используя только базовые операции  MPI_Send, MPI_Recv, и сравнить её производительность со стандартной реализацией из библиотеки MPI.

---

## 2. Постановка задачи
**Задано**:
- `sendbuf`: буфер с данными на корневом процессе (root).
- `sendcount`: количество элементов, отправляемых каждому процессу.
- `sendtype`: тип данных отправляемых элементов.
- `recvbuf`: буфер для приема данных на каждом процессе.
- `recvcount`: количество принимаемых элементов.
- `recvtype`: тип принимаемых элементов.
- `root`: ранг корневого процесса.
- `comm`: коммуникатор.

**Требуется**:
Реализовать рассылку данных так, чтобы i-й процесс получил i-ю порцию данных из `sendbuf` (размером `sendcount`).

Реализация должна:
1. Иметь сигнатуру, аналогичную `MPI_Scatter`.
2. Поддерживать произвольные типы данных (`MPI_INT`, `MPI_FLOAT`, `MPI_DOUBLE`).
3. Поддерживать произвольный корневой процесс (`root`).
4. Использовать топологию "дерево" для оптимизации коммуникаций (O(log N) шагов).

**Ограничения**:
- Использовать только блокирующие `MPI_Send` и `MPI_Recv`.
- Корректная работа при любом числе процессов и размере данных.

---

## 3. Базовый алгоритм
В силу ограничения фраемворка в качестве последовательной версии будет использоваться "заглушка".
1.  Определяем размер типа данных в байтах.
2.  Если буфер приема (`recvbuf`) не совпадает с буфером отправки (`sendbuf`), то в цикле выполняем копирование первых `sendcount` элементов из `sendbuf` в `recvbuf` .

---

## 4. Схема распараллеливания
### 4.1. Топология и обмен
Стандартный алгоритм биномиального дерева предполагает, что рассылка начинается с процесса 0. Чтобы поддержать произвольный корневой процесс (`root`), вводится понятие **виртуального ранга** (`v_rank`). Это логическое отображение, при котором физический `root` становится виртуальным `0`.
   
Формула преобразования: `v_rank = (rank - root + size) % size`

**Пример:** Пусть общее число процессов `size = 4`, а корневой процесс `root = 1`.
`0 -> 3`
`1 -> 0`
`2 -> 1`
`3 -> 2`
   
### 4.2. Подговка данных
Поскольку виртуальная нумерация меняет логический порядок процессов, данные в исходном буфере `sendbuf` (где они лежат линейно: данные для 0, данные для 1...) перестают соответствовать получателям в дереве.  
Перед началом рассылки корневой процесс формирует временный буфер, выполняя циклический сдвиг данных, чтобы они соответствовали порядку виртуальных рангов:
1. Блок данных для физических рангов `[root ... size-1]` копируется в начало временного буфера.
2. Блок данных для физических рангов `[0 ... root-1]` копируется в конец временного буфера.
**Визуализация (size=4, root=1):**
*Исходный буфер:* `[Chunk 0] [Chunk 1] [Chunk 2] [Chunk 3]`
*Целевой порядок (по v_rank):*
- v_rank 0 (Rank 1) должен получить `Chunk 1`
- v_rank 1 (Rank 2) должен получить `Chunk 2`
- v_rank 2 (Rank 3) должен получить `Chunk 3`
- v_rank 3 (Rank 0) должен получить `Chunk 0`

*Временный буфер:* `[Chunk 1] [Chunk 2] [Chunk 3] [Chunk 0]`
   
### 4.3 Алгоритм рассылки
Используется маска `mask`, которая инициализируется старшей степенью двойки, меньшей размера коммуникатора.
На каждом шаге (пока `mask > 0`):
1.  **Определение роли:** Процесс проверяет свой `v_rank`.
    - Если `v_rank % (2 * mask) == 0`: Процесс является **отправителем** на этом уровне. Он уже имеет данные и должен передать часть их потомку.
    - Если `v_rank % (2 * mask) == mask`: Процесс является **получателем**. Он ждет данные от родителя.
2.  **Обмен:**
    - Отправитель вычисляет виртуальный ранг получателя: `v_dest = v_rank + mask`.
    - Если `v_dest` находится в пределах коммуникатора (`v_dest < size`), происходит передача.
    - Объем передаваемых данных рассчитывается как сумма данных для всего поддерева получателя

Cхема обмена (для 4 процессов, root=0, v_rank == rank):
*   **Шаг 1 (mask=2):**
    *   Процесс 0 (имеет данные для 0,1,2,3) отправляет данные для {2,3} процессу 2.
    *   Теперь: Процесс 0 имеет {0,1}, Процесс 2 имеет {2,3}.
*   **Шаг 2 (mask=1):**
    *   Процесс 0 отправляет данные для {1} процессу 1.
    *   Процесс 2 отправляет данные для {3} процессу 3.
    *   Все процессы имеют свои данные.

### 4.4 Примеры топологий:
Ниже приведены схемы биномиальных деревьев для различного количества процессов (на основе виртуальных рангов, где 0 — это Root).

**Случай 1: Степень двойки (Size = 8)**
Идеальное биномиальное дерево.
```
      0
    / | \
   4  2  1
 / |  |
6  5  3
|
7
```

**Случай 2: Произвольное число (Size = 10)**
Если `v_dest >= size`, передача не выполняется. Ветви дерева "обрубаются".
Например, процесс 8 попытался бы отправить данные процессу 12 (при mask=4) и процессу 10 (при mask=2), но так как их нет, он отправляет только процессу 9 (mask=1).
```
	     0
     /  / | \
    8  4  2  1
  /   / |  |
 9   6  5  3
     |
     7
```

**Случай 3: Произвольное число (Size = 12)**
```
          0
      /  / \  \
     8   4  2  1
   / |  / | |
 10  9 6  5 3
  |    |
 11    7
```

---
## 5. Детали реализации
**Ключевые файлы:**
- `baldin_a_my_scatter/seq/include/ops_seq.hpp` и `baldin_a_my_scatter/seq/src/ops_seq.cpp` - последовательная реализация.
- `baldin_a_my_scatter/mpi/include/ops_mpi.hpp` и `baldin_a_my_scatter/mpi/src/ops_mpi.cpp` - параллельная реализация.
- `baldin_a_my_scatter/common/include/common.hpp` - общие определения типов и интерфейсов.

**Ocновные функции**:
- `GetDataTypeExtent(MPI_Datatype type)` - возвращает размер одного элемента данных в байтах. Это позволяет алгоритму работать универсально с `MPI_INT`, `MPI_FLOAT` и `MPI_DOUBLE`.
- `VirtualToRealRank(int v_rank, int root, int size)` - преобразует виртуальный ранг обратно в физический ранг MPI-процесса.
- `CalculateInitialMask(int size)` - вычисляет начальное значение битовой маски для алгоритма.
- `CalculateSubtreeSize(int v_dest, int mask, int size)` - определяет, какой объем данных (в "порциях" для процессов) необходимо отправить потомку на текущем шаге. Внутри сделана дополнительная проверка `std::min(v_dest + mask, size)`, которая позволяет автоматически "обрубать" несуществующие ветви дерева, выходящие за пределы `size`.
- `PrepareRootBuffer(...)`  - отвечает за предварительную подготовку данных на корневом процессе.

 **Особенности**:
- При вызове MPI функций в последовательной версии или функциональных тестах в CI падает этап `Run tests (threads)`, поэтому была написана функция `IsSeqTest()`, которая в зависимости от названия теста определяет, можно ли использовать MPI функции.
	- Если `IsSeqTest()` возвращает `true` (последовательный запуск), тесты принудительно устанавливают `rank = 0`, `size = 1` и не вызывают функции MPI.
	- Если `IsSeqTest()` возвращает `false` (параллельный запуск), используются реальные функции MPI для получения ранга и размера коммуникатора.

**Стадии обработки задачи:**
1.  **Валидация (`ValidationImpl`):**
    Проверяется корректность входных параметров в соответствии со стандартом MPI:
    *   `sendcount` и `recvcount` должны быть положительными и равными друг другу.
    *   `sendtype` должен совпадать с `recvtype`.
    *   Поддерживаются только типы `MPI_INT`, `MPI_FLOAT`, `MPI_DOUBLE` (минимальный вариант по тз).
    *   В последовательной версии дополнительно проверяется валидность указателей на буферы.
2.  **Предобработка (`PreProcessingImpl`):**
    Выполняется нормализация параметра `root`. Так как тестовая система может передавать параметры `root`, превышающие размер текущего коммуникатора (например, тест с `root=3` запускается на 2 процессах), выполняется корректировка:
    `input.root = input.root % world_size`.
3.  **Входные и выходные данные:**
    *   **Вход (`InType`):** Кортеж (`std::tuple`), полностью повторяющий сигнатуру `MPI_Scatter`: указатели на буферы, счетчики, типы данных, ранг корня и коммуникатор.
    *   **Выход (`OutType`):** Указатель `void*` на `recvbuf`.
    Поскольку `Scatter` модифицирует память по переданному указателю, задача возвращает этот же указатель как результат. Это позволяет в тестах через метод `GetOutput()` получить доступ к локальному буферу каждого процесса и проверить его содержимое.

**Генерация данных:**
Для упрощения верификации буфер отправки (`sendbuf`) заполняется последовательными значениями: 0, 1, 2, ... , N.
Это позволяет каждому процессу легко проверить корректность полученных данных. Процесс с рангом `R`, принимающий `C` элементов, должен получить последовательность чисел, начинающуюся со значения `R * C`. Проверка выполняется простым циклом сравнения.

---

## 6. Экспериментальная установка
- **Аппаратное обеспечение**: 
	Apple M1 Pro (8 ядер CPU: 6 производительных + 2 эффективных, 10 ядер GPU)  
	ОЗУ — 16 ГБ LPDDR5
- **Операционная система:** macOS Sonoma 14.5
- **Компилятор:** Apple Clang 21.1.4 (Homebrew)
- **MPI-библиотека:** Open MPI 5.0.8 (Homebrew)
- **Тип сборки:** Release (`-O3`)

---

## 7.  Результаты и обсуждение
### 7.1 Корректность
1. Проверена работа с основными типами MPI (`MPI_INT`, `MPI_FLOAT`, `MPI_DOUBLE`).
2. Проверена корректность работы при различных корневых процессах (`root = 0`, `root = 1` и т.д.) с использованием механизма виртуальных рангов.
3. Рассмотрены граничные случаи:
   - Передача малого количества элементов (1 элемент).
   - Передача больших объемов данных (10 млн элементов).
   - Число процессов, не являющееся степенью двойки (проверка корректности отсечения ветвей биномиального дерева).

Для верификации данных использовался метод сравнения содержимого буферов приема (`recvbuf`) с ожидаемой арифметической прогрессией, генерируемой на этапе инициализации. Все тесты пройдены успешно.

### 7.2 Производительность
Так как последовательная версия является "заглушкой" (локальное копирование памяти), она используется только как базовая точка отсчета. Основной интерес представляет динамика времени выполнения параллельной версии при увеличении числа процессов.

Тест 1. Количество элементов - 100'008

| Mode | Count | Time, s |
| ---- | ----- | ------- |
| omp  | 1     | 0.0002  |
| omp  | 2     | 0.0003  |
| omp  | 3     | 0.0004  |
| omp  | 4     | 0.0006  |
| omp  | 8     | 0.0011  |

Тест 2. Количество элементов - 1'000'008

| Mode | Count | Time, s |
| ---- | ----- | ------- |
| omp  | 1     | 0.0018  |
| omp  | 2     | 0.0035  |
| omp  | 3     | 0.0055  |
| omp  | 4     | 0.0075  |
| omp  | 8     | 0.0138  |

Тест 3. Количество элементов - 10'000'008

| Mode | Count | Time, s |
| ---- | ----- | ------- |
| omp  | 1     | 0.0172  |
| omp  | 2     | 0.0374  |
| omp  | 3     | 0.0582  |
| omp  | 4     | 0.0836  |
| omp  | 8     | 0.1743  |


Из представленных данных видно увеличение времени выполнения при росте числа процессов. Это обусловлено совокупностью факторов реализации и среды запуска:
1.  **Накладные расходы на топологию:** С ростом числа процессов увеличивается высота биномиального дерева, что требует большего количества шагов коммуникации (этапов `Send`/`Recv`).
2.  **Управление памятью:** Пользовательская реализация использует `std::vector` для промежуточных буферов. На каждом этапе коммуникации происходит выделение памяти (`resize`) и копирование данных (`memcpy`), что создает существенную нагрузку.

---

## 8. Заключение

- Реализованный алгоритм корректно выполняет рассылку данных между процессами коммуникатора.  
- Наблюдается замедление при увеличении количества процессов.  
- Основные ограничения — большое количество копирований при обмене данными между процессами и выстраивание сложной топологии.

---

## 9. Источники
- Лекции и практики курса "Параллельное программирование"
- Open MPI: https://github.com/open-mpi/ompi

---

## 10. Приложение

```cpp
namespace {

MPI_Aint GetDataTypeExtent(MPI_Datatype type) {
  MPI_Aint lb = 0;
  MPI_Aint extent = 0;
  MPI_Type_get_extent(type, &lb, &extent);
  return extent;
}

int VirtualToRealRank(int v_rank, int root, int size) {
  return (v_rank + root) % size;
}

int CalculateSubtreeSize(int v_dest, int mask, int size) {
  return std::min(v_dest + mask, size);
}

int CalculateInitialMask(int size) {
  int mask = 1;
  while (mask < size) {
    mask <<= 1;
  }
  return mask >> 1;
}

void PrepareRootBuffer(const void *sendbuf, int size, int root, int count, MPI_Aint extent, std::vector<char> &buffer) {
  size_t total_bytes = static_cast<size_t>(size) * count * extent;
  size_t chunk_bytes = static_cast<size_t>(count) * extent;

  buffer.resize(total_bytes);

  const char *send_ptr = static_cast<const char *>(sendbuf);
  char *tmp_ptr = buffer.data();

  // Логика сдвига: [root...end] -> начало, [0...root] -> конец
  size_t first_part_bytes = (size - root) * chunk_bytes;
  size_t second_part_bytes = root * chunk_bytes;

  std::memcpy(tmp_ptr, send_ptr + second_part_bytes, first_part_bytes);
  std::memcpy(tmp_ptr + first_part_bytes, send_ptr, second_part_bytes);
}

}  // namespace

bool BaldinAMyScatterMPI::RunImpl() {
  auto &input = GetInput();
  const auto &[sendbuf, sendcount, sendtype, recvbuf, recvcount, recvtype, root, comm] = input;

  int rank = 0;
  int size = 0;
  MPI_Comm_rank(comm, &rank);
  MPI_Comm_size(comm, &size);

  MPI_Aint extent = GetDataTypeExtent(rank == root ? sendtype : recvtype);

  std::vector<char> temp_buffer;
  const char *curr_buf_ptr = nullptr;

  // --- ЭТАП 1: Подготовка (только на root) ---
  if (rank == root) {
    PrepareRootBuffer(sendbuf, size, root, sendcount, extent, temp_buffer);
    curr_buf_ptr = temp_buffer.data();
  }

  int v_rank = (rank - root + size) % size;
  int mask = CalculateInitialMask(size);

  // --- ЭТАП 2: Рассылка по дереву ---
  while (mask > 0) {
    // Если процесс - отправитель на этом уровне
    if (v_rank % (2 * mask) == 0) {
      int v_dest = v_rank + mask;

      if (v_dest < size) {
        int subtree_size = CalculateSubtreeSize(v_dest, mask, size);
        int count_to_send = (subtree_size - v_dest) * recvcount;

        size_t offset_bytes = static_cast<size_t>(v_dest - v_rank) * recvcount * extent;
        int real_dest = VirtualToRealRank(v_dest, root, size);

        MPI_Send(curr_buf_ptr + offset_bytes, count_to_send, (rank == root ? sendtype : recvtype), real_dest, 0, comm);
      }
    }

    // Если процесс - получатель
    else if (v_rank % (2 * mask) == mask) {
      int v_source = v_rank - mask;
      int real_source = VirtualToRealRank(v_source, root, size);

      int subtree_end = CalculateSubtreeSize(v_rank, mask, size);
      int count_to_recv = (subtree_end - v_rank) * recvcount;

      size_t bytes_to_recv = static_cast<size_t>(count_to_recv) * extent;
      temp_buffer.resize(bytes_to_recv);

      MPI_Recv(temp_buffer.data(), count_to_recv, recvtype, real_source, 0, comm, MPI_STATUS_IGNORE);

      // Теперь работаем с полученным буфером
      curr_buf_ptr = temp_buffer.data();
    }

    mask >>= 1;
  }

  // --- ЭТАП 3: Копирование в пользовательский буфер ---
  if (recvbuf != MPI_IN_PLACE && curr_buf_ptr != nullptr) {
    // Копируем только свою долю (recvcount)
    std::memcpy(recvbuf, curr_buf_ptr, recvcount * extent);
  }
  GetOutput() = recvbuf;
  return true;
}
```