#Собственная реализация коллективной функции MPI_Allreduce(SEQ + MPI)

**Студент : **Лифанов Кирилл Максимович,
    группа 3823Б1ФИ2 * *Технология : **SEQ + MPI **Вариант : ** №3

-- -

##1. Введение

Цель работы — разработать коллективную функцию *MPI_allreduce * используя MPI_Send и MPI_Recv.

Определение: **MPI_Allreduce * * -Коллективная операция между процессами, использующая сбор данных и рассылку результата

-- -

##2. Постановка задачи

Задано : массив целых чисел `v` длины `n`.

Требуется вычислить :

``` count = sum(v);
```

## #Ограничения

- `n ≥ 1`;
- массив не должен быть пустым;
- элементы — тип `int`;
- результаты SEQ и MPI реализаций должны совпадать.

-- -

##3. Последовательная реализация(SEQ)

```cpp std::vector<int> current_values = GetInput();
GetOutput().resize(1);
int global_sum = 0;
for (std::size_t i = 0; i < current_values.size(); i++) {
  global_sum += current_values[i];
}
GetOutput()[0] = global_sum;
```

Алгоритм выполняет линейный проход по массиву и имеет сложность **O(n) **.

-- -

##4. Параллельная реализация(MPI)

MPI
- версия распределяет *вектор *между процессами.

## #4.1 Декомпозиция данных

Всего частей :

При `p` процессах :

```
base = total_pairs / p rem = total_pairs % p
local_pairs(rank) = base + (rank < rem ? 1 : 0)
```

## #4.2 Рассылка данных

Используется `MPI_Scatterv`.Смещения рассчитываются как :

```
offset+= sendCounts[i];
```

## #4.3 Локальный подсчёт

Каждый процесс считает сумму в своей части :

```
cpp for (i = 0; i + 1 < local_size; ++i) if (local[i] > local[i + 1]) : local_sum++
```

## #4.4 Сбор результата

Используются :

- `MPI_Recv` — сбор локальных результатов,
    Затем суммирование и получение итоговой суммы

  -- -

  ##5. Корректность и тестирование

  Функциональные тесты проверяли :

-разные размеры массивов;
- разные данные в массивах;
- повторяющиеся значения;
- отрицательные числа;
- пустые массивы;
- совпадение SEQ и MPI.

Все тесты успешно пройдены.

-- -

##6. Производительность(20 млн элементов)

Фактические результаты, полученные в perf-тестах:

| Режим | Процессы | Время(с) | Ускорение S(p) | Эффективность E(p) | 
| SEQ | 1 | 0.1061624 | 1.00  | —      | 
| MPI | 2 | 0.0452578 | 2.35  | 117 %  | 
| MPI | 4 | 0.0322029 | 3.312 | 82.8 % | 
| MPI | 8 | 0.0283682 | 3.79  | 47.3 % |

## #Интерпретация результатов

Распараллеливание даёт достаточный процент эффективности.

Возможные причины:

-алгоритм обладает **низкой вычислительной плотностью **(одна операция);
- объём данных очень большой, а вычислений мало;
- Высокие накладные расходы.

-- -

##7. Заключение

В ходе работы:

-разработаны SEQ и MPI реализации MPI_Allreduce;
- функциональная корректность подтверждена тестами;
- произведён анализ производительности, показавший, что MPI - версия на огромных массивах имеет значительное ускорение.

  -- -

  ##8. Источники

  1. P.Pacheco — *Parallel Programming with MPI *.
  2. 2. Kumar V. — *Introduction to Parallel Computing *.
