# Обобщенная передача от одного всем (scatter)

- Студент: Романов Артем Сергеевич, группа 3823Б1ФИ3
- Технологии: SEQ | MPI
- Вариант: 4 

## 1. Введение

Параллельные вычисления широко применяются для ускорения обработки больших объёмов данных и решения вычислительно сложных задач. Одним из наиболее распространённых стандартов для разработки параллельных программ является MPI (Message Passing Interface), который предоставляет различные средства для взаимодействия процессов.

При написании параллельного кода часто возникает необходимость распределять исходные данные между процессами для их последующей независимой обработки. Для решения этой задачи в MPI предусмотрены коллективные операции обмена данными, одной из которых является MPI_Scatter. 

Целью данной работы является реализация собственной функции MyMPI_Scatter с использованием базовых функций передачи сообщений MPI_Send и MPI_Recv.

## 2. Постановка задачи

Пусть задан коммуникатор `MPI_Comm`, объединяющий *p* параллельных процессов, и один из процессов с рангом `root` содержит массив данных, состоящий из элементов некоторого типа. Требуется реализовать функцию `MyMPI_Scatter`, осуществляющую распределение данных от процесса `root` ко всем процессам коммуникатора таким образом, что каждому процессу передаётся непрерывный блок элементов одинакового размера.

Реализуемая функция должна иметь интерфейс, аналогичный стандартной функции `MPI_Scatter`, и использовать для обмена данными только базовые операции передачи сообщений `MPI_Send` и `MPI_Recv`.

### Входные данные:

- указатель `void* sendbuf` на массив исходных данных, расположенный на процессе `root`;
- целое число `int sendcount` — количество элементов, передаваемых каждому процессу;
- тип данных `MPI_Datatype sendtype` элементов массива;
- указатель `void* recvbuf` на буфер для приёма данных;
- целое число `int recvcount` — количество элементов, принимаемых каждым процессом;
- тип данных `MPI_Datatype recvtype` принимаемых элементов;
- целое число `int root` — ранг процесса-источника данных;
- коммуникатор `MPI_Comm MPI_Comm`.

### Выходные данные:

- массив `void* recvbuf`, содержащий часть исходных данных, соответствующую рангу текущего процесса (возврат массива осуществляется посредством заполнения данных по переданному указателю).


## 3. Базовый алгоритм

В базовой версии кода используется стандартная функция MPI_Scatter, которая выполняет распределение данных от процесса с рангом root ко всем процессам коммуникатора MPI_COMM_WORLD. Данная реализация используется как эталон для сравнения со временем выполнения и производительностью со собственной реализацией функции `MyMPI_Scatter`.

```cpp
  int rank = 0;
  MPI_Comm_rank(MPI_COMM_WORLD, &rank);

  int num_processes = 0;
  MPI_Comm_size(MPI_COMM_WORLD, &num_processes);

  int root = std::get<2>(GetInput());
  int sendcount = std::get<1>(GetInput());

  std::vector<int> sendbuf;
  if (rank == root) {
    sendbuf = std::get<0>(GetInput());
    sendbuf.resize(num_processes * sendcount);
  }

  std::vector<int> recvbuf(sendcount);

  MPI_Scatter(
    sendbuf.data(),
    sendcount,
    MPI_INT,
    recvbuf.data(),
    sendcount,
    MPI_INT,
    root,
    MPI_COMM_WORLD
  );

  GetOutput() = recvbuf;

  return true;
```

Также из-за особенностей фреймворка отправляемые данные дополняются нулями (либо отбрасываются) таким образом, чтобы каждый процесс получил одно и то же количество данных. 

## 4. Параллелизация


## 5. Детали реализации


## 6. Условия проведения экспериментов
- **Аппаратное обеспечение/ОС:** Intel Core i5-10400f, 6 ядер/12 логических процессоров, 32GB RAM DDR4 2667 Mhz, Ubuntu 24.04.2/Docker (под Windows 10 Home 22H2);
- **Инструменты сборки:** GCC 13.3.0, Release, Cmake 3.28.3;
- **Переменные окружения:** Не использовались (запуск тестов происходил с флагом `-np <x>`);
- **Данные:** Тестовые данные генерировались вручную.

## 7. Результаты

### 7.1 Корректность


### 7.2 Производительность


| Число процессов | Время MPI версии, с | Время собественной версии, с | Ускорение |
| --------------- | ------------------- | ---------------------------- | --------- |
| 1               | 0.77                | 0.70                         | 1.10      |
| 2               | 0.79                | 0.78                         | 1.01      |
| 3               | 0.78                | 0.74                         | 1.05      |
| 4               | 0.76                | 0.86                         | 0.88      |
| 5               | 0.77                | 0.84                         | 0.92      |
| 6               | 0.75                | 0.89                         | 0.84      |
| 7               | 0.76                | 0.95                         | 0.80      |
| 8               | 0.78                | 0.99                         | 0.79      |
| 9               | 0.79                | 0.96                         | 0.82      |
| 10              | 0.79                | 1.05                         | 0.75      |
| 11              | 0.78                | 1.06                         | 0.74      |
| 12              | 1.05                | 1.26                         | 0.83      |
| 16              | 1.02                | 1.34                         | 0.76      |
| 24              | 1.41                | 2.03                         | 0.69      |
| 32              | 1.53                | 2.73                         | 0.56      |
| 50              | 2.33                | 4.18                         | 0.55      |
| 100             | 4.17                | 7.17                         | 0.58      |

## 8. Заключение


## 9. Список литературы
1. Microsoft. MPI_Scatter function [Электронный ресурс]. – URL: https://learn.microsoft.com/ru-ru/message-passing-interface/mpi-scatter-function (дата обращения: 12.12.2025);
2. Оболенский А., Нестеров А. Parallel Programming course. MPI (detailed API overview) [Электронный ресурс]. – Нижегородский государственный университет. – URL:  https://learning-process.github.io/parallel_programming_slides/slides/03-mpi-api.pdf (дата обращения: 18.11.2025);
3. Оболенский А., Нестеров А. Parallel Programming course. Introduction [Электронный ресурс]. – Нижегородский государственный университет. – URL:  https://learning-process.github.io/parallel_programming_slides/slides/01-intro.pdf (дата обращения: 18.11.2025).
