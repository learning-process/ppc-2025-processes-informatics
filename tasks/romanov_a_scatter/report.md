# Обобщенная передача от одного всем (scatter)
- Студент: Романов Артем Сергеевич, группа 3823Б1ФИ3
- Технологии: SEQ | MPI
- Вариант: 4
## 1. Введение
Параллельные вычисления широко применяются для ускорения обработки больших объёмов данных и решения вычислительно сложных задач. Одним из наиболее распространённых стандартов для разработки параллельных программ является MPI (Message Passing Interface), который предоставляет различные средства для взаимодействия процессов.

При написании параллельного кода часто возникает необходимость распределять исходные данные между процессами для их последующей независимой обработки. Для решения этой задачи в MPI предусмотрены коллективные операции обмена данными, одной из которых является `MPI_Scatter`.

Целью данной работы является реализация собственной функции для рассылки данных `MyMPIScatter` с использованием базовых функций передачи сообщений `MPI_Send` и `MPI_Recv`.

## 2. Постановка задачи
Пусть задан коммуникатор `MPI_Comm`, объединяющий *p* параллельных процессов, и один из процессов с рангом `root` содержит массив данных, состоящий из элементов некоторого типа. Требуется реализовать функцию `MyMPIScatter`, осуществляющую распределение данных от процесса `root` ко всем процессам коммуникатора таким образом, что каждому процессу передаётся непрерывный блок элементов одинакового размера.

Реализуемая функция должна иметь интерфейс, аналогичный стандартной функции `MPI_Scatter`, и использовать для обмена данными только базовые операции передачи сообщений `MPI_Send` и `MPI_Recv`.

### Входные данные:
- указатель `void* sendbuf` на массив исходных данных, расположенный на процессе `root`;
- целое число `int sendcount` — количество элементов, передаваемых каждому процессу;
- тип данных `MPI_Datatype sendtype` элементов массива;
- указатель `void* recvbuf` на буфер для приёма данных;
- целое число `int recvcount` — количество элементов, принимаемых каждым процессом;
- тип данных `MPI_Datatype recvtype` принимаемых элементов;
- целое число `int root` — ранг процесса-источника данных;
- коммуникатор `MPI_Comm MPI_Comm`.
### Выходные данные:
- массив `void* recvbuf`, содержащий часть исходных данных, соответствующую рангу текущего процесса (возврат массива осуществляется посредством заполнения данных по переданному указателю).

## 3. Базовый алгоритм
Из-за особенностей фреймворка и CI в последовательной реализации стоит "заглушка", которая циклом копирует данные в ответ. Однако локально она была изменена так, чтобы благодаря ней можно было измерять производительность собственной реализации функции (данный код сохранён и закомментирован).

В изменённой версии кода используется стандартная функция `MPI_Scatter`, которая выполняет распределение данных от процесса с рангом root ко всем процессам коммуникатора MPI_COMM_WORLD. Данная реализация используется как эталон для сравнения времени выполнения и производительности собственной реализации `MPI_Scatter` и стандартной.

```cpp
  int rank = 0;
  MPI_Comm_rank(MPI_COMM_WORLD, &rank);

  int num_processes = 0;
  MPI_Comm_size(MPI_COMM_WORLD, &num_processes);

  int root = std::get<2>(GetInput());
  int sendcount = std::get<1>(GetInput());

  std::vector<int> sendbuf;
  if (rank == root) {
    sendbuf = std::get<0>(GetInput());
    sendbuf.resize(num_processes * sendcount);
  }

  std::vector<int> recvbuf(sendcount);

  MPI_Scatter(
    sendbuf.data(),
    sendcount,
    MPI_INT,
    recvbuf.data(),
    sendcount,
    MPI_INT,
    root,
    MPI_COMM_WORLD
  );

  GetOutput() = recvbuf;

  return true;
```
## 4. Параллелизация
Параллельная версия содержит код аналогичный локально изменённому, но с вызовом функции `MyMPIScatter`, а также саму реализацию этой функции. 

В ней процесс с рангом, равным рангу отправителя, получает требуемые для передачи данные. Затем, если ранг этого процесса не равен 0, то все данные пересылаются на нулевой процесс (для удобства дальнейшей рассылки).

Последующая рассылка работает следующим образом: процесс получает блок данных, где сначала идут его собственные данные, а затем — данные для следующих по номерам процессов, за которые он отвечает в схеме рассылки, которая имеет вид биномиального дерева. То, кому отправлять, вычисляется так: представим ранг процесса в двоичном виде, найдём наименее значимую единицу (младший ненулевой бит) и будем последовательно устанавливать единицу в позициях правее неё, формируя ранги процессов-получателей.

Например, для процесса с рангом 4 (`100`) наименее значимая единица находится в позиции 2 (третий бит справа). Устанавливая единицу правее (в позиции 1 и 0), получаем ранги `110`, то есть 6, и `101`, то есть 5. Таким образом, процесс 4 должен отправить часть данных процессу 6 и 5. 

Этот метод гарантирует, что вся рассылка завершится за логарифмическое число шагов.
## 5. Детали реализации
Стоит отметить следующие особенности реализации:
1. Данные в `MyMPIScatter` отправляются побайтово без каких-либо преобразований типов, после получения они интерпретируются "как есть"
2. Перед отправкой данных есть проверка на то, что процессы суммарно не получат данных больше, чем отправлено
```cpp
  if (sendcount * sendtype_size < recvcount * recvtype_size) {
    return MPI_ERR_ARG;
  }
```
3. Каждый процесс перед получением данных сначала узнаёт, данные скольких процессов он получит
```cpp
MPI_Send(&send_blocks, 1, MPI_INT, child_rank, 0, comm);
...
MPI_Recv(&blocks_in_subtree, 1, MPI_INT, parent_rank, 0, comm, MPI_STATUS_IGNORE);
```
4. Ранги родителей и детей процессы получают посредством битовых операций, например ранг родителя получается отбрасыванием последней единицы числа:
```cpp
int parent_rank = rank & (rank - 1);
```
## 6. Условия проведения экспериментов
- **Аппаратное обеспечение/ОС:** Intel Core i5-10400f, 6 ядер/12 логических процессоров, 32GB RAM DDR4 2667 Mhz, Ubuntu 24.04.2/Docker (под Windows 10 Home 22H2);
- **Инструменты сборки:** GCC 13.3.0, Release, Cmake 3.28.3;
- **Переменные окружения:** Не использовались (запуск тестов происходил с флагом `-np <x>`);
- **Данные:** Тестовые данные генерировались вручную.

## 7. Результаты
### 7.1 Корректность
Код последовательной и параллельной версии проверялся на некоторой группе тестов, обладающей некоторыми особенностями из-за структуры фреймворка:
1. Код тестировался только на данных типа `int` из-за необходимости конкретного определения типа `InType`
2. Отправляемые данные дополняются нулями (либо отбрасываются) таким образом, чтобы каждый процесс получил одно и то же количество данных
3. Данные всегда рассылались с нулевого процесса из-за аналогичной с 1-ым пунктом проблемы (при заполнении массива `kTestParam` нельзя гарантированно указать номер существующего процесса помимо нулевого, так как запуск MPI версии может быть произведён на ровно одном процессе) 

Сами тесты состоят из некоторых отправляемых массивов целых чисел, с различным числом отправляемых каждому процессу элементов.

### 7.2 Производительность
Эффективность собственной реализации определялась на массиве размера `150'000'000`, где число отправляемых каждому процессу данных зависело от числа процессов.

В результате были собраны следующие данные:

| Число процессов | Время MPI версии, с | Время собественной версии, с | Ускорение |
| --------------- | ------------------- | ---------------------------- | --------- |
| 1               | 0.68                | 0.97                         | 0.70      |
| 2               | 0.72                | 1.21                         | 0.59      |
| 3               | 0.68                | 1.16                         | 0.58      |
| 4               | 0.68                | 1.31                         | 0.52      |
| 5               | 0.68                | 1.25                         | 0.54      |
| 6               | 0.68                | 1.31                         | 0.52      |
| 7               | 0.73                | 1.29                         | 0.57      |
| 8               | 0.71                | 1.32                         | 0.54      |
| 9               | 0.75                | 1.48                         | 0.51      |
| 10              | 0.80                | 1.42                         | 0.56      |
| 11              | 0.78                | 1.53                         | 0.51      |
| 12              | 0.82                | 1.56                         | 0.53      |
| 16              | 1.03                | 1.94                         | 0.53      |
| 24              | 1.40                | 2.51                         | 0.56      |
Реализация `MyMPIScatter` показала отрицательное ускорение относительно стандартного `MPI_Scatter`: значение ускорения находится в диапазоне 0.51-0.70, что означает, что собственная версия работает в 1.4-2 раза медленнее библиотечной, при этом с ростом числа процессов ускорение начинает стабилизироваться относительно значения ~0.53.

Такой результат может быть объяснён, например, излишними накладными расходами на передачу данных либо иной схемой рассылки, отличной от стандартной. 

## 8. Заключение
В ходе выполнения работы была реализована собственная версия функции рассылки данных `MyMPIScatter` с использованием базовых функций `MPI_Send` и `MPI_Recv`. Проведённые эксперименты показали, что разработанный алгоритм на основе биномиального дерева корректно выполняет рассылку, однако его производительность примерно в 1.5-2 раза ниже, чем у оптимизированной библиотечной версии `MPI_Scatter`. Полученное отрицательное ускорение свидетельствует о наличии дополнительных затрат, связанных с организацией рассылки данных. Это показывает важность использования стандартных, оптимизированных коллективных операций MPI в реальных параллельных приложениях.

## 9. Список литературы

1. Microsoft. MPI_Scatter function [Электронный ресурс]. – URL: https://learn.microsoft.com/ru-ru/message-passing-interface/mpi-scatter-function (дата обращения: 12.12.2025);
2. Оболенский А., Нестеров А. Parallel Programming course. MPI (detailed API overview) [Электронный ресурс]. – Нижегородский государственный университет. – URL:  https://learning-process.github.io/parallel_programming_slides/slides/03-mpi-api.pdf (дата обращения: 18.11.2025);
3. Оболенский А., Нестеров А. Parallel Programming course. Introduction [Электронный ресурс]. – Нижегородский государственный университет. – URL:  https://learning-process.github.io/parallel_programming_slides/slides/01-intro.pdf (дата обращения: 18.11.2025).