# Сумма значений по столбцам матрицы

- Студент: Гутянский Алексей Сергеевич, группа 3823Б1ФИ3
- Технология: SEQ | MPI
- Вариант: 12

## 1. Введение

Суммирование элементов матрицы по столбцам относится к числу базовых операций, которые часто становятся частью более сложных вычислительных цепочек: статистической обработки данных, численных методов, разложения больших наборов измерений. Несмотря на простоту формулировки, практическая реализация такой операции для крупных матриц требует продуманной организации данных и эффективного распределения нагрузки между вычислительными узлами.

Работа посвящена разработке решения для суммирования матрицы по столбцам, а также анализу возможностей его параллельного исполнения. Рассматриваются ключевые аспекты реализации, потенциальные источники узких мест и факторы, влияющие на производительность при различных способах разбиения данных.

## 2. Постановка задачи

Дана прямоугольная матрица \(A\) размера \(n \times m\), содержащая целые числа. Для каждого столбца требуется вычислить сумму элементов в нём.

**Входные данные:** два целых числа \(n\) и \(m\), задающих соответственно кол-во строк и столбцов матрицы, \(n \times m\) чисел задающих элементы матрицы. Гарантируется, что элементы матрицы и ответ помещаются в 32-битный знаковый тип.

**Выходные данные:** \(m\) целых чисел - сумм по каждому из столбцов матрицы.

## 3. Базовый алгоритм (Последовательный)

Будем рассматривать решение как массив длины \(m\). Изначально все его элементы инициализированы нулями.

```cpp
std::fill(GetOutput().begin(), GetOutput().end(), 0);

for (size_t i = 0; i < GetInput().rows; i++) {
    for (size_t j = 0; j < GetInput().cols; j++) {
        GetOutput()[j] += GetInput().data[(i * GetInput().cols) + j];
    }
}
```

Для каждой строки матрицы от \(1\) до \(n\) столбцы обходятся в порядке от \(1\) до \(m\). Ответ накапливается в соответствующих ячейках массива.

При описанной схеме обхода достигается почти последовательный доступ к памяти. При этом внутренний цикл по \(j\) может быть легко векторизован.

## 4. Параллелизация

Отдадим каждому процессу на обработку одну или более последовательных строк матрицы. Количество строк вычисляется по формуле:

```cpp
int rows = floor(n / proc_count) + (proc_rank == 0 ? (n % proc_count) : 0);
```

В формуле _proc_count_ обозначает число запущенных процессов, а _proc_rank_ - ранг процесса.

Если \(n\) не делится на _proc_count_ нацело, то остаток обрабатывается на нулевом процессе. В худшем случае, нулевой процесс обработает почти в два раза больше данных.

Полную копию данных имеет только процесс с нулевым рангом. Из него в другие процессы через _MPI_Bcast_ рассылаются сначала размеры матрицы, а потом через _MPI_Scatter_ блоки для обработки.

Для вычисления частичного ответа используется код аналогичный последовательной версии за тем исключением, что суммирование выполняется не по всей матрице, а по ее части.

Полное решение задачи вычисляется с использованием функции _MPI_Reduce_. Ответ становится доступен в памяти процесса с нулевым рангом.

## 5. Детали реализации

Для функциональных тестов реализована загрузка из файлов, хранящихся в директории data/. Матрица большого размера, сохраненная в текстовом файле, может занимать большой объем места на диске, но для функциональных тестов возможность использования больших матриц является избыточной.

Для теста производительности используется квадратная матрица размера 16000 на 16000 элементов. Матрица состоит из единиц, что никак не влияет на скорость исполнения, но упрощает генерацию теста.

Как уже отмечалось ранее, для параллельной версии загрузка тестовых данных и проверка ответа происходит только на процессе с нулевым рангом.

## 6. Тестовое окружение

- Аппаратное обеспечение/Операционная система: Intel Core i5 14600KF, 6P+8E ядер, 64Gb Ddr5 5600Mhz, Windows 10, MS-MPI.
- Инструменты сборки: Cmake 4.2.0-rc4, Visual Studio 2022, MSVC, x64 Release.
- Переменные окружения: PPC_NUM_THREADS=PPC_NUM_PROC=1/2/3/4/8/12/16/20/24/32/42/48, PPC_PERF_MAX_TIME=10000.
- Данные: вручную созданные тесты небольшого размера, случайным образом сгенерированный тест 1000x1000 элементов, матрица из единиц размера 16000x16000.
- Дополнительно: в scripts/run_tests.py были отключены perf тесты для всех технологий, кроме seq и mpi.

## 7. Результаты

### 7.1 Корректность

Решение, как в последовательной, так и в параллельной версии было протестировано на входных данных разного размера и содержания. В числе тестов: матрица с одним столбцом и одной строкой, матрица, состоящая из нулей, неквадратная матрица и другие.

Скорость исполнения проверена на матрице из единиц размера 16000х16000 элементов. Время исполнения стабильно и равно 110мс.

### 7.2 Производительность

Время, ускорение, эффективность:

| Режим        | Кол-во процессов | Время, сек | Ускорение | Эффективность параллелизма |
|-------------|-------|---------|---------|------------|
| seq         | 1     | 0.106  | 1.00    | N/A        |
| mpi         | 2     | 0.235  | 0.303   | 30.3%      |
| mpi         | 3     | 0.175  | 0.605   | 20.1%      |
| mpi         | 4     | 0.211  | 0.512   | 12.8%      |
| mpi         | 8     | 0.208  | 0.524   | 6.5%       |
| mpi         | 12    | 0.163  | 0.68    | 5.6%       |
| mpi         | 16    | 0.222  | 0.48    | 3.0%       |
| mpi         | 20    | 0.191  | 0.56    | 2.8%       |
| mpi         | 24    | 0.177  | 0.62    | 2.5%       |
| mpi         | 32    | 0.240  | 0.44    | 1.3%       |
| mpi         | 42    | 0.206  | 0.52    | 1.2%       |
| mpi         | 48    | 0.205  | 0.52    | 1.0%       |

![Ускорение от количества процессов](./report_assets/pic1.png)
![Эффективность параллелизма](./report_assets/pic2.png)

Первое, что стоит отметить, это замедление MPI версии относительно последовательной. Данных для обработки много, в то время как операций над ними мало. Большая часть времени тратится на пересылку данных между процессами.

При отсутствии рассылки входных данных удавалось достичь ускорения относительно последовательной версии в 5.2 раза, что не являлось пределом, так как то решение не было оптимизировано и хранило полную копию входных данных на каждом из процессов.

Видно, что производительность растет до достижения 12 процессов, что равно количеству производительных ядер процессора с учетом Hyper-Threading. Ускорение в максимуме достигло 2.3 раз относительно запуска с одним процессом.

Далее накладные расходы от рассылки данных начинают съедать производительность и график, хоть и нелинейно, начинает опускаться.

## 8. Заключение

Была разработана программа, решающая задачу суммирования матрицы по столбцам и проанализированы возможности её параллелизации с использованием MPI. Были найдены и описаны узкие места и возможные причины их возникновения.

## 9. Источники

1. Microsoft MPI : документация [Электронный ресурс] // Microsoft Learn. – URL: https://learn.microsoft.com/ru-ru/message-passing-interface/microsoft-mpi (дата обращения: 18.11.2025).
2. Курс лекций "Параллельная обработка данных" : Лаборатория Параллельных Информационных Технологий, НИВЦ МГУ [Электронный ресурс] // PARALLEL.RU. - URL: https://parallel.ru/vvv/mpi.html (дата обращения: 03.11.2025)
3. Сысоев А. В. Курс лекций по параллельному программированию.