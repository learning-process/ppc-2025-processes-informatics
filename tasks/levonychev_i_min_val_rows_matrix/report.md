# Нахождение минимальных значений по строкам матрицы

- Студент: Левонычев Иван Александрович, группа 3823Б1ФИ3
- Технология: SEQ, MPI
- Вариант: 17

## 1. Введение
- Мотивация: изучить базовые возможности и инструменты MPI на простой задаче, посмотреть, какой прирост производительности даст MPI версия относительно последовательной.
- Проблема: задача нахождения минимальных значений по строкам матрицы - распространненая задача при работе с матрицами, поэтому ее оптмизация вызывает интерес.
- Ожидаемый результат: ожидается, что MPI версия будет работать быстрее последовательной за счёт распределения вычислительной нагрузки между несколькими процессами.

## 2. Постановка задачи
На вход программе подаются 3 параметра:
1. Матрица в виде одномерного массива чисел. Тип элементов - int.
2. Количество строк (*rows*).
3. Количество столбцов (*cols*).
Требуется найти минимумы по строкам матрицы, то есть на выходе должен быть одномерный массив чисел длины *rows*, в котором на i-ой позиции находится минимум из i-ой строки исходной матрицы.

## 3. Базовый алгоритм (последовательный)
Последовательный алгоритм довольно прост. Мы проходим в цикле по строкам матрицы. Инициализируем изначальный минимум первым элементов строки, а затем каждый раз находим минимум из текущего минимума и следующего элемента строки и присваиваем в переменную, содержащую текущий минимум.
Код функции RunImpl:
```cpp
bool LevonychevIMinValRowsMatrixSEQ::RunImpl() {
  const std::vector<int> &matrix = std::get<0>(GetInput());
  const int rows = std::get<1>(GetInput());
  const int cols = std::get<2>(GetInput());
  OutType &result = GetOutput();

  for (int i = 0; i < rows; ++i) {
    int min_val = matrix[static_cast<size_t>(cols) * static_cast<size_t>(i)];
    for (int j = 1; j < cols; ++j) {
      min_val = std::min(matrix[(cols * i) + j], min_val);
    }
    result[i] = min_val;
  }

  return true;
}
```

## 4. Описание параллельного алгоритма
Параллельный алгоритм основан на последовательной версии, единственное различие в том, что каждый процесс считает минимумы в своих строках.
Распределение строк происходит следующим образом:
1. Если количество строк меньше количества процессов, то все строки отдаются процессу с рангом 0, остальные процессы ничего не делают.
2. Если количество строк нацело делится на количество процессов, то всем процессам достается равное количество строк. Например, матрица содержит 90 строк, и у нас 3 процесса. То процессу с рангом 0 достанутся строки 0-29, процессу с рангом 1 строки 30-59, и процессу с рангом 2 строки 60-89.
3. Если количество строк не делится нацело на количество процессов, то последнему процессу добавляются оставшиеся строки. Например, матрица содержит 100 строк, и у нас 3 процесса. Тогда процессу с рангом 0 достанутся строки 0-29, процессу с рангом 1 строки 30-59, а процессу с рангом 2 строки 60-99.

Каждый процесс считает свои минимумы, затем с помощью функции GatherV все минимумы объдиняются в общий массив на процессе с рангом 0. Потом процесс с рангом 0 рассылает этот массив всем оставшимся процессам с помощью функции MPI_Bcast.

## 5. Experimental Setup
- Hardware/OS: Intel i5-12450H, 8 ядер, RAM 16GB, Windows 11
- Toolchain: Microsoft Visual C++ (MSVC), Release
- Data: Матрица размера 10000 на 40000, заполнение начинается от 0 с шагом 1.

## 6. Результаты

### 6.1 Корректность
Коррекность последовательного и MPI алгоритмов проверена функциональными тестами в количестве 10 штук. Тестировалось на матрицах разных размерностей, в том числе содержащих 1 строку и/или 1 столбец.

### 6.2 Производительность
Входные данные: Матрица размера 10000 на 40000.

| Mode        | Count | Time, s | Speedup | Efficiency |
|-------------|-------|---------|---------|------------|
| seq         | 1     | 0.211   | 1.00    | N/A        |
| mpi         | 2     | 0.114   | 1.85    | 92.5%      |
| mpi         | 4     | 0.068   | 3.10    | 77.5%      |

Можем видеть, что на двух процессах практическо двухкратное ускорение, на 4 процессах чуть похуже. Это происходит из-за возрастающих накладных расходов на коммуникацию и синхронизацию.

## 7. Выводы
На основе результатов производительности можно сделать вывод, что параллельный алгоритм, решающий данную задачу, имеет смысл, так как мы уменьшаем время работы практически в столько раз, сколько у нас процессов. Самого лучшего ускорения, видимо, не удалось достигнуть по следующим причинам:
1. Накладные расходы повышаются с увеличением количества процессов.
2. Имеющихся начальных знаний в области параллельного программирования оказалось недостаточно, чтобы получить наилучшее ускорение.

## 8. Литература
1. Стандарт MPI.
2. Лекции и практики по параллельному программированию.

## 9. Приложение

```cpp
bool LevonychevIMinValRowsMatrixMPI::RunImpl() {
  const std::vector<int> &matrix = std::get<0>(GetInput());
  const int rows = std::get<1>(GetInput());
  const int cols = std::get<2>(GetInput());
  OutType &global_min_values = GetOutput();

  if (global_min_values.size() != static_cast<size_t>(rows)) {
    global_min_values.resize(rows);
  }
  int proc_num = 0;
  int proc_rank = 0;
  MPI_Comm_size(MPI_COMM_WORLD, &proc_num);
  MPI_Comm_rank(MPI_COMM_WORLD, &proc_rank);

  int local_count_of_rows = rows / proc_num;
  if (proc_rank == (proc_num - 1)) {
    local_count_of_rows += (rows % proc_num);
  }
  int start_id = proc_rank * (rows / proc_num) * cols;

  std::vector<int> local_min_values(local_count_of_rows);
  for (int i = 0; i < local_count_of_rows; ++i) {
    const int start_row_id = start_id + (cols * i);
    int min_value = matrix[start_row_id];
    for (int j = 1; j < cols; ++j) {
      min_value = std::min(matrix[start_row_id + j], min_value);
    }
    local_min_values[i] = min_value;
  }

  std::vector<int> recvcounts(proc_num);
  std::vector<int> displs(proc_num);
  int current_displacement = 0;

  for (int i = 0; i < proc_num; ++i) {
    int count_i = rows / proc_num;
    if (i == (proc_num - 1)) {
      count_i += rows % proc_num;
    }
    recvcounts[i] = count_i;
    displs[i] = current_displacement;
    current_displacement += count_i;
  }
  MPI_Gatherv(local_min_values.data(), local_count_of_rows, MPI_INT, global_min_values.data(), recvcounts.data(),
              displs.data(), MPI_INT, 0, MPI_COMM_WORLD);
  MPI_Bcast(global_min_values.data(), rows, MPI_INT, 0, MPI_COMM_WORLD);
  return true;
}
```
