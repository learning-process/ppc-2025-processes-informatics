# Отчет по лабораторной работе №2
## "Реализация коллективной операции Reduce"

**Студент:** Кутергин Валентин
**Группа:** 3823Б1ФИ3
**Вариант:** 2

### 1. Введение
**Мотивация:** Коллективные операции, такие как `MPI_Reduce`, являются фундаментальными основами для построения сложных параллельных алгоритмов. Понимание их внутреннего устройства необходимо для написания эффективного и масштабируемого MPI-кода.

**Цель работы:** Реализовать собственную версию коллективной операции `MPI_Reduce`, используя только базовые функции `MPI_Send` и `MPI_Recv`. Исследовать корректность и производительность реализованного алгоритма.

### 2. Постановка задачи
**Задано:** Реализовать функцию, полностью повторяющую прототип и функциональность `MPI_Reduce`.
`int Reduce(void* sendbuf, void* recvbuf, int count, MPI_Datatype, MPI_Op, int root, MPI_Comm);`

**Требуется:**
*   Реализация должна использовать **древовидную схему** сбора данных.
*   Функция должна корректно работать для произвольного `root`-процесса.
*   Тесты должны проверять работу с различными типами данных (`MPI_INT`).

### 3. Схема распараллеливания
Параллельный алгоритм реализует сбор и обработку данных по схеме **рекурсивного удвоения** (бинарного дерева), что позволяет избежать "бутылочного горлышка" на `root`-процессе.

1.  **Локальные данные:** Каждый процесс начинает с локального значения в `sendbuf`.
2.  **Древовидный сбор:** На каждом шаге `k` цикла `log2(P)` (где P - число процессов) половина "активных" процессов отправляет свои промежуточные результаты своим "партнерам" и завершает работу. Вторая половина принимает данные, выполняет операцию редукции (например, сложение) со своими данными и переходит на следующий шаг. Разделение на "отправителей" и "получателей" на каждом шаге элегантно реализуется с помощью битовой маски.
3.  **Финализация:** После завершения древовидного сбора итоговый результат всегда оказывается на процессе с `rank = 0`. Если `root`-процесс, указанный в аргументах, не равен `0`, выполняется дополнительная `Send-Recv` операция для передачи финального результата от `0` к `root`.

### 4. Экспериментальная установка
*   **Hardware/OS:** AMD Ryzen 5 7500F, 6 ядер, 32GB RAM, Windows 10
*   **Toolchain & Environment:**
    *   Работа производилась внутри **Dev Container** (Ubuntu)
    *   Компилятор: **GCC 14.2.0**
    *   MPI-библиотека: **Open MPI 3.1**
    *   Тип сборки: **Release**
*   **Data:** Для замеров производительности каждый процесс сначала локально суммировал вектор из **2,000,000** случайных целых чисел. Затем операция `My_Reduce` собирала эти локальные суммы.

### 5. Результаты и обсуждение
#### 5.1 Корректность
Корректность реализации `Reduce` подтверждена набором функциональных тестов. Тесты проверяли совпадение результата с эталоном, который вычислялся на `root`-процессе путем **пересоздания и суммирования тех же псевдослучайных последовательностей**, что и на рабочих процессах. Проверялась работа с различными `root`-процессами и размерами векторов. Все тесты пройдены успешно.

#### 5.2 Производительность
Замеры производительности проводились для `seq` и `mpi` реализаций на разном количестве процессов (от 2 до 8, на машине с 6 физическими ядрами). В таблице представлено время выполнения `RunImpl`, включающее локальное суммирование и операцию редукции.

| Mode | Count | Time, s | Speedup | Efficiency |
| :--- | :--- | :--- | :--- | :--- |
| seq | 1 | **0.000210** | 1.00 | N/A |
| mpi | 2 | **0.000386** | 0.54 | 27.0% |
| mpi | 3 | **0.000303** | 0.69 | 23.0% |
| mpi | 4 | **0.002501** | 0.08 | 2.0% |
| mpi | 5 | **0.002423** | 0.09 | 1.8% |
| mpi | 6 | **0.002749** | 0.08 | 1.3% |
| mpi | 7 | **0.001026** | 0.20 | 2.9% |
| mpi | 8 | **0.000305** | 0.69 | 8.6% |

*   **Speedup** = `Time(seq) / Time(mpi)`
*   **Efficiency** = `Speedup / Count * 100%`

**Обсуждение:**
Полученные результаты полностью подтверждают гипотезу о неэффективности MPI для задач с низкой вычислительной интенсивностью. На всех конфигурациях параллельная реализация работает **существенно медленнее** последовательной, демонстрируя ускорение значительно меньше 1.0.

Особого внимания заслуживает нелинейная зависимость времени выполнения от числа процессов. Наихудшие результаты показаны при 4-6 процессах. Улучшение времени при 7 и 8 процессах (в режиме перегрузки, `oversubscription`) может быть связано с особенностями работы планировщика операционной системы и самого MPI-рантайма при большом количестве очень коротких задач.

Тем не менее, общий вывод остается неизменным: накладные расходы на MPI-коммуникации (создание процессов, вызовы `MPI_Send`/`MPI_Recv`, синхронизация) **доминируют** над временем, затраченным на полезные вычисления (локальное суммирование). Это делает древовидный `Reduce`, несмотря на его алгоритмическую эффективность, невыгодным для таких легковесных операций.

### 6. Заключение
В ходе работы была успешно реализована собственная версия коллективной операции `MPI_Reduce` с использованием древовидного алгоритма. Функциональные тесты подтвердили корректность реализации.

Эксперименты по производительности показали, что для задач с низкой вычислительной интенсивностью (как суммирование `int`) накладные расходы на MPI-коммуникации могут доминировать над временем полезных вычислений, приводя к отрицательному ускорению. Это доказывает, что применение параллельных технологий требует тщательного анализа соотношения вычислений и коммуникаций (Amdahl's Law) для достижения реального выигрыша в производительности.

### 7. Приложение: Исходный код `My_Reduce`
```cpp
int Reduce(void *sendbuf, void *recvbuf, int count, MPI_Datatype datatype, MPI_Op op, int root, MPI_Comm comm) {
  int process_rank = 0;
  int process_count = 0;
  MPI_Comm_rank(comm, &process_rank);
  MPI_Comm_size(comm, &process_count);

  if (root < 0 || root >= process_count)  // root-процесс не существует
  {
    return MPI_ERR_ROOT;  // возвращение стандартного кода ошибки MPI
  }

  int type_size = 0;
  MPI_Type_size(datatype, &type_size);

  // Древовидный сбор
  for (int mask = 1; mask < process_count;
       mask <<= 1)  // удвоение битовой маски на каждой итерации посредством битового сдвига
  {
    if ((process_rank & mask) != 0)  // процессы-отправители
    {
      MPI_Send(sendbuf, count, datatype, process_rank - mask, 0, comm);
      break;
    }

    if (process_rank + mask < process_count)  // процессы-получатели
    {
      auto *recv_temp = new uint8_t[static_cast<size_t>(count) * type_size];
      MPI_Recv(recv_temp, count, datatype, process_rank + mask, 0, comm, MPI_STATUS_IGNORE);

      ApplyOp(sendbuf, recv_temp, count, datatype, op);  // выполнение op (MPI_SUM) для datatype (MPI_INT)

      delete[] recv_temp;
    }
  }

  // Результат с процесса 0 отправляется на процесс 'root'
  if (process_rank == 0 && root != 0) {
    MPI_Send(sendbuf, count, datatype, root, 0, comm);
  }

  // Корневой процесс 'root' получает финальный результат
  if (process_rank == root) {
    if (root == 0) {
      std::memcpy(recvbuf, sendbuf, static_cast<size_t>(count) * type_size);
    } else {
      MPI_Recv(recvbuf, count, datatype, 0, 0, comm, MPI_STATUS_IGNORE);
    }
  }

  return MPI_SUCCESS;
}