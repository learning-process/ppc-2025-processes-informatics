# Отчет по лабораторной работе №2
## "Реализация коллективной операции Reduce"

**Студент:** Кутергин Валентин
**Группа:** 3823Б1ФИ3
**Вариант:** 2

### 1. Введение
**Мотивация:** Коллективные операции, такие как `MPI_Reduce`, являются фундаментальными основами для построения сложных параллельных алгоритмов. Понимание их внутреннего устройства необходимо для написания эффективного и масштабируемого MPI-кода.

**Цель работы:** Реализовать собственную версию коллективной операции `MPI_Reduce`, используя только базовые функции `MPI_Send` и `MPI_Recv`. Исследовать корректность и производительность реализованного алгоритма.

### 2. Постановка задачи
**Задано:** Реализовать функцию, полностью повторяющую прототип и функциональность `MPI_Reduce`.
`int Reduce(void* sendbuf, void* recvbuf, int count, MPI_Datatype, MPI_Op, int root, MPI_Comm);`

**Требуется:**
*   Реализация должна использовать **древовидную схему** сбора данных.
*   Функция должна корректно работать для произвольного `root`-процесса.
*   Тесты должны проверять работу с различными типами данных (`MPI_INT`).

### 3. Схема распараллеливания
Параллельный алгоритм реализует сбор и обработку данных по схеме **рекурсивного удвоения** (бинарного дерева), что позволяет избежать "бутылочного горлышка" на `root`-процессе.

1.  **Локальные данные:** Каждый процесс начинает с локального значения в `sendbuf`.
2.  **Древовидный сбор:** На каждом шаге `k` цикла `log2(P)` (где P - число процессов) половина "активных" процессов отправляет свои промежуточные результаты своим "партнерам" и завершает работу. Вторая половина принимает данные, выполняет операцию редукции (например, сложение) со своими данными и переходит на следующий шаг. Разделение на "отправителей" и "получателей" на каждом шаге элегантно реализуется с помощью битовой маски.
3.  **Финализация:** После завершения древовидного сбора итоговый результат всегда оказывается на процессе с `rank = 0`. Если `root`-процесс, указанный в аргументах, не равен `0`, выполняется дополнительная `Send-Recv` операция для передачи финального результата от `0` к `root`.

### 4. Экспериментальная установка
*   **Hardware/OS:** AMD Ryzen 5 7500F, 6 ядер, 32GB RAM, Windows 10
*   **Toolchain & Environment:**
    *   Работа производилась внутри **Dev Container** (Ubuntu)
    *   Компилятор: **GCC 14.2.0**
    *   MPI-библиотека: **Open MPI 3.1**
    *   Тип сборки: **Release**
*   **Data:** Для замеров производительности каждый процесс сначала локально суммировал вектор из **2,000,000** случайных целых чисел. Затем операция `My_Reduce` собирала эти локальные суммы.

### 5. Результаты и обсуждение
#### 5.1 Корректность
Корректность реализации `Reduce` подтверждена набором функциональных тестов. Тесты проверяли совпадение результата с эталоном, который вычислялся на `root`-процессе путем **пересоздания и суммирования тех же псевдослучайных последовательностей**, что и на рабочих процессах. Проверялась работа с различными `root`-процессами и размерами векторов. Все тесты пройдены успешно.

#### 5.2 Производительность
Замеры производительности проводились на системе, указанной в п.5. В таблице представлено время выполнения "обертки" `RunImpl`, включающей локальное суммирование и операцию редукции.

| Mode | Count | Time, s | Speedup | Efficiency |
| :--- | :--- | :--- | :--- | :--- |
| seq | 1 | **0.000210** | 1.00 | N/A |
| mpi | 4 | **0.000714** | **0.29** | **7.25%** |

*   **Speedup** = `Time(seq) / Time(mpi)`
*   **Efficiency** = `Speedup / Count * 100%`

**Обсуждение:**
Полученные результаты демонстрируют **отрицательное ускорение**: параллельная реализация на 4-х процессах оказалась примерно в **3.4 раза медленнее** последовательной.

Это объясняется тем, что задача локального суммирования 2 млн. целых чисел является **вычислительно очень быстрой** (`~0.2` миллисекунды). На этом фоне накладные расходы на **даже самый эффективный** древовидный алгоритм коммуникации (`My_Reduce`) оказываются **значительно выше**. Время, затраченное на вызовы `MPI_Send` и `MPI_Recv`, синхронизацию и передачу данных, "съедает" весь выигрыш от распараллеливания вычислений и приводит к общему замедлению.

### 6. Заключение
В ходе работы была успешно реализована собственная версия коллективной операции `MPI_Reduce` с использованием древовидного алгоритма. Функциональные тесты подтвердили корректность реализации.

Эксперименты по производительности показали, что для задач с низкой вычислительной интенсивностью (как суммирование `int`) накладные расходы на MPI-коммуникации могут доминировать над временем полезных вычислений, приводя к отрицательному ускорению. Это доказывает, что применение параллельных технологий требует тщательного анализа соотношения вычислений и коммуникаций (Amdahl's Law) для достижения реального выигрыша в производительности.

### 7. Приложение: Исходный код `My_Reduce`
```cpp
int Reduce(void *sendbuf, void *recvbuf, int count, MPI_Datatype datatype, MPI_Op op, int root, MPI_Comm comm) {
  int process_rank = 0;
  int process_count = 0;
  MPI_Comm_rank(comm, &process_rank);
  MPI_Comm_size(comm, &process_count);

  if (root < 0 || root >= process_count)  // root-процесс не существует
  {
    return MPI_ERR_ROOT;  // возвращение стандартного кода ошибки MPI
  }

  int type_size = 0;
  MPI_Type_size(datatype, &type_size);

  // Древовидный сбор
  for (int mask = 1; mask < process_count;
       mask <<= 1)  // удвоение битовой маски на каждой итерации посредством битового сдвига
  {
    if ((process_rank & mask) != 0)  // процессы-отправители
    {
      MPI_Send(sendbuf, count, datatype, process_rank - mask, 0, comm);
      break;
    }

    if (process_rank + mask < process_count)  // процессы-получатели
    {
      auto *recv_temp = new uint8_t[static_cast<size_t>(count) * type_size];
      MPI_Recv(recv_temp, count, datatype, process_rank + mask, 0, comm, MPI_STATUS_IGNORE);

      ApplyOp(sendbuf, recv_temp, count, datatype, op);  // выполнение op (MPI_SUM) для datatype (MPI_INT)

      delete[] recv_temp;
    }
  }

  // Результат с процесса 0 отправляется на процесс 'root'
  if (process_rank == 0 && root != 0) {
    MPI_Send(sendbuf, count, datatype, root, 0, comm);
  }

  // Корневой процесс 'root' получает финальный результат
  if (process_rank == root) {
    if (root == 0) {
      std::memcpy(recvbuf, sendbuf, static_cast<size_t>(count) * type_size);
    } else {
      MPI_Recv(recvbuf, count, datatype, 0, 0, comm, MPI_STATUS_IGNORE);
    }
  }

  return MPI_SUCCESS;
}