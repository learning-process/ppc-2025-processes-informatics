# Ленточная горизонтальная схема - умножение матрицы на вектор. 

- Student: Лузан Егор Андреевич, group 3823Б1ФИ3
- Technology: SEQ | MPI
- Variant: 11


## 1. Introduction

Цель работы - реализовать и сравнить две версии программы, выполняющей умножение матрицы на вектор:
1. последовательную,
2. параллельную, использующую библиотеку **MPI** и ленточную горизонтальную схему распределения данных.

Подобная задачи широко используются при моделировании разнообразных процессов, явлений и систем, проведении научных и инженерных расчётов, а также при работе в области теория вычислительной сложности.

Ожидается ускорение MPI-версии относительно последовательной реализации.


## 2. Problem Statement

- Даны:
	- Матрица **A** размерности `height × width`
	- Вектор **v** размерности `width`

- Требуется вычислить вектор **b** размерности `height`, где:

$$ b[i] = \sum_{j=0}^{width-1} A[i][j] \cdot v[j] $$

**Входные данные:**
- Матрица в виде одномерного вектора (`std::vector<int>`) размерности `height × width`
- Вектор (`std::vector<int>`) размерности `width`

**Выходные данные:**
- Вектор (`std::vector<int>`) размерности `height`, содержащий результат умножения

**Ограничения:**
- Элементы матрицы - целые числа в диапазоне `[0; 17 640 000]`
- Элементы вектора - целые числа в диапазоне `[0; 1 764 000]`


## 3. Baseline Algorithm (Sequential)

1. Инициализация результирующего вектора нулями.
2. Для каждой строки матрицы:
	- Вычислить скалярное произведение строки на вектор по формуле выше.
	- Записать результат в соответствующий элемент выходного вектора.


## 4. Parallelization Scheme

**Распределение данных:**
- Матрица: 
	1. Делим высоту матрицы нацело на количество процессов, получая `rows_per_proc` - минимальное количество строк для обработки каждый процессом.
	2. Получаем остаток `rest` от предыдущего деления.
	3. Раскладываем оставшиеся `rest` строк по процессам по одной, начиная с `rank=0`. Также вычисляем `shift` - массив смещений, который будет использован при рассылке данных с корневого процесса.
	4. Таким образом первые `rest` процессов обработают `rows_per_proc` + 1 строку, остальные `rows_per_proc`.
- Вектор:
	1. Вектор **v** передаётся полностью каждому процессу.

**Роли процессов:**
- `rank = 0`:
	- Загружает матрицу и вектор.
	- Рассылает части матрицы и полный вектор всем процессам.
	- Собирает результаты с помощью `MPI_Gatherv`.
- Остальные процессы:
	- Получают свои строки матрицы и вектор.
	- Выполняют умножение своей части.


## 5. Implementation Details

- Для тестов матрица и вектор генерируется внутри программы. Результат заранее предсказуем.
- Загрузка матрицы производиться только на root-процесс для экономии памяти.
- Проверены граничные случаи:
	- Матрица 1×1,
	- Вектор длины 1,
	- Матрица с одной строкой/столбцом


## 6. Experimental Setup

- **Аппаратное обеспечение / ОС:**
  - CPU: Intel Core i7-13620H (6 P-cores, 4 E-cores)
  - RAM: 16 GB
  - OS: Windows 11 x64
- **Инструменты:**
  - CMake 3.28.3
  - Компилятор: gcc 13.3.0 (Ubuntu 24.04.2 LTS в Docker)
  - Режим сборки: Release
- **Данные:** 
	- Матрица 10 000 × 10 000,  элементы - целые числа из отрезка `[0; 17 640 000]`.
	- Вектор длины 10 000, элементы - целые числа из отрезка `[0; 1 764 000]`.


## 7. Results and Discussion

### 7.1 Correctness
Корректность проверена с использованием GoogleTest на матрицах размерностей:
3×3, 2×5, 10×70, 2000×5, 5×2000, 1000 × 1000,  1×1, 100 × 1, 1 × 100.

### 7.2 Performance
Present time, speedup and efficiency. Example table:

| Mode        | Count | Time, s | Speedup | Efficiency |
|-------------|-------|---------|---------|------------|
| seq         | 1     | 0.040   | 1.000   | N/A        |
| mpi         | 2     | 0.205   | 0.195   | 9.75%      |
| mpi         | 4     | 0.153   | 0.261   | 6.54%      |
| mpi         | 6     | 0.132   | 0.303   | 5.05%      |
| mpi         | 7     | 0.147   | 0.272   | 3.88%      |
| mpi         | 8     | 0.149   | 0.269   | 3.36%      |

Как видно из таблицы, рост производительности прекращается на 6 процессах, что соответствует количеству ядер на используемом процессе.


## 8. Conclusions
Разработана программа для умножения матрицы на вектор с использованием MPI и ленточной горизонтальной схемы распределения данных. Параллельная версия показала низкую эффективность из-за высоких накладных расходов на пересылку данных.


## 9. References
1. Курс лекций ННГУ "Параллельное программирование для кластерных систем".
2. Стандарт MPI.