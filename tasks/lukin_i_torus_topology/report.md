# <Сумма элементов вектора>

- Student: <Лукин Иван Антонович>, group <3823Б1ФИ3>
- Technology: <SEQ | MPI >
- Variant: <1>

## 1. Introduction
Мотивация: Исследовать э  ффективность распараллеливания простой операции через MPI

Проблема: Сложение векторов имеет низкую арифметическую интенсивность - затраты на коммуникации могут превысить выгоду от параллельных вычислений

Ожидаемый результат: MPI-версия может уступать последовательной из-за преобладания накладных расходов распределения данных над скоростью вычислений

## 2. Problem Statement
На вход приходит вектор произвольного размера (даже размера 0).
Задача - вычислить сумму его элементов (целые числа)

## 3. Baseline Algorithm (Sequential)
Базовый (последовательный) алгоритм проходит по всему вектору и складывает его элементы с помощью std::accumulate

## 4. Parallelization Scheme
Параллельный алгоритм реализует распределенное суммирование элементов вектора с использованием MPI. Сначала процессы определяют общий размер вектора. Корневой процесс распределяет данные между всеми процессами с помощью операции Scatterv, обеспечивая равномерное распределение элементов с учетом возможного остатка (Всем по целой части, остаток распределяется между первыми reminder процессами). Каждый процесс независимо вычисляет локальную сумму своих элементов. Затем с помощью операции Allreduce все частичные суммы складываются, и итоговый результат становится доступен всем процессам одновременно. Для случая, когда количество процессов превышает размер вектора, алгоритм предусматривает специальную обработку, где только root-процесс выполняет последовательное суммирование, а результа  fdfdgfdgfdgт рассылается остальным процессам через Broadcast

## 5. Experimental Setup
- Hardware/OS: Intel I5-13420H, 8 cores, 16GB RAM, Win10 OS
- Toolchain: Microsoft Visual C++ (MSVC), MSBuild 17.14.23, Release
- Environment: PPC_NUM_PROC
- Data: Вектор на 20'000'000 элементов, элементы составляют арифметическую прогрессию от 1 до vector.size с шагом 1

## 6. Results and Discussion

### 6.1 Correctness
Корректность реализаций проверена функциональными тестами. Они проверяют, может ли параллельный алгоритм обрабатывать все сценарии распределения ресурсов между процессами: на вход поступают векторы размером от 0 до 11.

### 6.2 Performance
Это тесты производительности на векторе из 10 млн элементов. В этой тестовой ситуации время счета еще сопоставимо с затратами на коммуникацию.

| Mode        | Count | Time, s | Speedup | Efficiency |
|-------------|-------|---------|---------|------------|
| seq         | 1     | 0.0047  | 1.00    | N/A        |
| mpi         | 4     | 0.008   | 0.6     | 15.0%      |
| mpi         | 8     | 0.0095  | 0.53    | 7.0%       |

Это тесты производительности на векторе из 100 млн элементов. Видно, что расходы на коммуникацию возросли и эффективность параллельной реализации упала еще сильнее.

| Mode        | Count | Time, s | Speedup | Efficiency |
|-------------|-------|---------|---------|------------|
| seq         | 1     | 0.02    | 1.00    | N/A        |
| mpi         | 4     | 0.09    | 0.22    | 5.0%       |
| mpi         | 8     | 0.1     | 0.2     | 2.5%       |

Это тесты производительности на векторе из 500 млн элементов. Видно, что расходы на коммуникацию многократно превысили время счета. Также стоит отметить, что тест на 8 процессах не удалось осуществить, что подтверждает выводы о серьезных затратах на Scatterv.

| Mode        | Count | Time, s | Speedup | Efficiency |
|-------------|-------|---------|---------|------------|
| seq         | 1     | 0.17    | 1.00    | N/A        |
| mpi         | 4     | 1.61    | 0.1     | 2.5%       |
| mpi         | 8     |    -    |    -    |    -       |

## 7. Conclusions
Реализация распределенного суммирования векторов с использованием MPI показала свою принципиальную неэффективность для данной задачи. Основная проблема заключается в катастрофическом преобладании коммуникационных затрат над вычислительными, что особенно проявляется при работе с большими объемами данных. С ростом размера вектора и количества процессов накладные расходы на распределение данных через операцию Scatterv становятся настолько значительными, что полностью нивелируют преимущества параллельных вычислений. Низкая арифметическая интенсивность операции сложения векторов делает этот тип задач непригодным для эффективного распараллеливания с использованием MPI-подхода, что подтверждается резким падением эффективности до 2-3% при работе с крупными массивами данных.

## 8. References
1. Лекции по параллельному программированию ННГУ
2. Стандарт MPI