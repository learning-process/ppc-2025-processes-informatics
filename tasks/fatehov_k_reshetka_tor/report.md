# Максимальное значение элементов матрицы

- Студент: Фатехов Камиль Гаярович, группа 3823Б1ФИ3
- Технология: SEQ | MPI
- Вариант: 9

## 1. Введение
**Цель лабораторной**: 
Разработка последовательной и параллельной версии поиска максимального значения элементов матрицы с использованием решётки-тора в MPI и последующим сравнением этих двух версий.

**Задачи лабораторной**:  
- Реализовать последовательную версию поиска максимального значения элементов матрицы.  
- Изучить основы MPI и реализовать параллельную версию программы с топологией решётки-тора.  
- Сравнить производительность и эффективность этих реализаций.
## 2. Постановка задачи
**Задача**: Определить максимальное значение элементов матрицы после «утяжеления» вычислений.

**Входные данные**  
    Данные представлены в виде `tuple(rows, columns, matrix)`, где:  
    - `rows` - количество строк матрицы (*size_t*)  
    - `columns` - количество столбцов матрицы (*size_t*)  
    - `matrix` - непосредственно сама матрица (*vector*)

**Выходные данные:**  
На выходе получается единичное значение типа *double* – максимальный элемент в матрице после обработки. 

**Ограничения**:  
    - `rows <= 10000`  
    - `cols <= 10000` 
 

## 3. Базовый алгоритм (Последовательный)

 **Реализован следующий код**:
```cpp 
bool FatehovKReshetkaTorSEQ::RunImpl() {
  auto &data = GetInput();
  std::vector<double> &matrix = std::get<2>(data);
  double global_max = -1e18;
  for (double val : matrix) {
    double heavy_val = val;
    for (int k = 0; k < 100; ++k) {
      heavy_val = (std::sin(heavy_val) * std::cos(heavy_val)) + std::exp(std::complex<double>(0, heavy_val).real()) +
                  std::sqrt(std::abs(heavy_val) + 1.0);
      if (std::isinf(heavy_val)) {
        heavy_val = val;
      }
    }
    global_max = std::max(heavy_val, global_max);
  }
  GetOutput() = global_max;
  return true;
}
```

**Алгоритм работы**:  
1. Получаем входные данные: матрицу в виде вектора.

2. Инициализируем максимум очень малым числом.

3. Для каждого элемента матрицы выполняем «утяжеление» (100 итераций вычислений с тригонометрическими и другими функциями).

4. Сравниваем полученное значение с текущим максимумом и обновляем его при необходимости.

5. Возвращаем итоговый максимум.

## 4. Схема распараллеливания

**Описание**:  
- Код реализует параллельный поиск максимального элемента в матрице с использованием MPI и топологии решётки-тора. Матрица разбивается на блоки по строкам и столбцам, каждый процесс обрабатывает свой блок, после чего выполняется редукция максимума по тору.

**Алгоритм работы**:  

1. **Инициализация MPI и получение размеров матрицы**

```cpp
MPI_Comm_rank(MPI_COMM_WORLD, &world_rank);
MPI_Comm_size(MPI_COMM_WORLD, &world_size);

size_t total_rows = 0;
size_t total_cols = 0;
std::vector<double> global_matrix;

if (world_rank == 0) {
   auto &data = GetInput();
   total_rows = std::get<0>(data);
   total_cols = std::get<1>(data);
   global_matrix = std::get<2>(data);
}

MPI_Bcast(&total_rows, 1, MPI_UNSIGNED_LONG, 0, MPI_COMM_WORLD);
MPI_Bcast(&total_cols, 1, MPI_UNSIGNED_LONG, 0, MPI_COMM_WORLD);
```

2. **Создание решётки-тора**
```cpp
CalculateGridDimensions(world_size, grid_rows, grid_cols);
// каждый процесс получает свои координаты в сетке
GetGridCoordinates(world_rank, grid_cols, row, col);
```

3. **Распределение данных по процессам**  

- Матрица разбивается на блоки по строкам и столбцам.

- Каждый процесс получает свой локальный блок матрицы.

- Распределение выполняется через MPI_Send/MPI_Recv. 

4. **Локальные вычисления**

```cpp
double local_max = FindLocalMax(local_matrix);
// где FindLocalMax выполняет утяжеление и поиск максимума в локальном блоке

```
5. **Глобальная редукция через тор**
```cpp
TorusAllReduce(local_max, world_rank, grid_rows, grid_cols);
// выполняются попарные обмены по строкам и столбцам сетки
```

6. **Возврат результата**
```cpp
GetOutput() = global_max;
```

## 5. Детали реализации

### **Утяжеление вычислений**

Для увеличения вычислительной нагрузки каждый элемент матрицы обрабатывается 100 раз с использованием:

- ``std::sin``

-  ``std::cos``

- ``std::exp``

- ``std::sqrt``

- ``std::complex``

Это сделано для увеличения времени вычислений и лучшего сравнения SEQ и MPI версий.

**Генерация тестовых данных**:  
Для создания тестовой матрицы используется линейный конгруэнтный генератор (ЛКГ) - алгоритм для генерации псевдослучайных чисел, который обеспечивает детерминированность и воспроизводимость данных.

```cpp
for (size_t i = 0; i < total; ++i) {
    state = (a * state + c) % m;
    double value = ((static_cast<double>(state) / m) * 2000.0) - 1000.0;
    matrix.push_back(value);
}
```

где:

- `state` - текущее состояние генератора  
- `a` - множитель (1664525)
- `c` - приращение (1013904223)
- `m` - модуль (2²² = 4194304)

**Процесс генерации**: 

Процесс генерации:

1. Инициализация: Начинаем с начального состояния state = 42

2. Итеративное вычисление: Для каждого элемента матрицы:

- Вычисляем новое состояние по формуле: ``state = (a × state + c) mod m``

- Нормализуем значение: ``value = (state / m) × 2000.0 - 1000.0``

- Получаем число в диапазоне ``[-1000.0, 1000.0)``

- Сохраняем значение в матрице

3. Размер матрицы: В тестах производительности используется матрица 1000×1000 элементов (1 миллион чисел)

**Преимущества для тестирования**:

- Генерирует одинаковые данные на всех процессах

- Быстрый и простой алгоритм

- Предсказуемый результат для проверки правильности



## 6. Экспериментальная среда
- Hardware/OS: AMD RYZEN 5 5600 6-Core Processor, 12-Threads, 16GB, Ubuntu (DevContainer/WSL 2.0)
- Toolchain: GCC 14.1.0, cmake version 3.31.1, Release
- Data: происходит генерация данных через функцию, описанную в пункте 5

## 7. Результаты и обсуждение

### 7.1 Корректность
**Модульные тесты**

В коде реализовано 3 тестовых случая:

- Матрица 3×4 с положительными числами

- Матрица 3×3 с отрицательными числами

- Матрица 5×5 со смешанными значениями

**Условие корректности:**  
SEQ и MPI версии дают идентичные результаты с точностью до ``1e-10``.


Таким образом, корректность подтверждается совпадением результатов с эталонными значениями, успешным прохождением модульных тестов и идентичностью результатов разных реализаций алгоритма.

### 7.2 Производительность
- Тестирование проводилось на матрице 1000×1000 с утяжелением вычислений.

| **Режим** | **Количество процессов** | **Время, с** | **Ускорение** | **Эффективность** |
|-------------|-------|---------|---------|------------|
| seq         | 1     | 2.0917   | 1.00    | N/A       |
| mpi         | 2     | 1.0553   | 1.98    | 99.0%      |
| mpi         | 4     | 0.5460   | 3.83    | 95.75%     |
| mpi         | 6     | 0.3735   | 5.60    | 93.33%       |

**Анализ результатов**:

**Высокая эффективность параллелизации**:

- MPI версия демонстрирует почти линейное ускорение

- Эффективность выше 93% даже для 6 процессов

- Ускорение 5.60× на 6 процессах приближается к теоретическому максимуму

**Причины высокой производительности MPI**:

- Значительные вычисления на элемент: 100 итераций тяжёлых операций (тригонометрия, экспоненты, комплексные числа)

- Оптимальное соотношение вычислений и коммуникаций: время вычислений существенно превосходит время обменов данными

- Эффективная топология тора: минимизирует накладные расходы на коммуникации

- Локальность данных: каждый процесс работает со своим блоком матрицы

**Сравнение с SEQ версией**:

- SEQ: 2.0917 с (один поток)

- MPI (6 процессов): 0.3735 с (в 5.6 раз быстрее)

- MPI (4 процесса): 0.5460 с (в 3.83 раз быстрее)

**Масштабируемость**:

- Рост числа процессов приводит к пропорциональному уменьшению времени выполнения

- Незначительное падение эффективности (с 99% до 93%) объясняется увеличением коммуникационных затрат

- Алгоритм хорошо масштабируется до 6 процессов



## 8. Заключение

В ходе лабораторной работы были успешно реализованы и сравнены последовательная и параллельная версии алгоритма поиска максимального значения элементов матрицы с учётом «утяжеления» вычислений.

**Основные выводы**:

1. **Корректность реализации**:

- Обе версии (SEQ и MPI) дают идентичные результаты с высокой точностью

- MPI версия с топологией решётки-тора корректно выполняет распределённые вычисления

- Все модульные тесты успешно пройдены

2. **Высокая эффективность MPI-реализации**:

- Существенное ускорение: MPI версия с 6 процессами работает в 5.6 раза быстрее последовательной версии

- Отличное масштабирование: почти линейное ускорение (ускорение 3.83× на 4 процессах, 5.60× на 6 процессах)

- Высокая эффективность: 93-99% эффективности использования вычислительных ресурсов

3. **Факторы успеха параллельной реализации**:

- Значительная вычислительная нагрузка: 100 итераций сложных операций на элемент матрицы

- Оптимальное соотношение вычислений/коммуникаций: время вычислений доминирует над временем обменов

- Эффективная топология: решётка-тор минимизирует коммуникационные задержки

- Сбалансированное распределение данных: равномерное распределение блоков матрицы по процессам

## 9. Источники
[Линейный конгруэнтный генератор](https://www.tutorialspoint.com/cplusplus-program-to-implement-the-linear-congruential-generator-for-pseudo-random-number-generation)



