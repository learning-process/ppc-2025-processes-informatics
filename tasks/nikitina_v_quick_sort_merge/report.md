# Быстрая сортировка с простым слиянием

**Студент:** Никитина Валерия Владимировна, группа 3823Б1ФИ2
**Технология:** SEQ, MPI
**Вариант:** 14

## 1. Введение
Сортировка данных является одной из фундаментальных задач информатики. С ростом объемов обрабатываемых данных производительности последовательных алгоритмов становится недостаточно.

Целью данной работы является реализация параллельного алгоритма сортировки, основанного на принципе «разделяй и властвуй»: данные разбиваются на части, сортируются независимо (Quick Sort), а затем объединяются в один упорядоченный массив (Merge). Работа выполняется с использованием стандарта MPI.

## 2. Постановка задачи
**Входные данные:** Целочисленный вектор `std::vector<int>`, доступный на корневом процессе (Rank 0).
**Выходные данные:** Вектор `std::vector<int>` того же размера, содержащий элементы входного вектора в неубывающем порядке. Результирующий вектор должен быть сформирован на корневом процессе.

**Ограничения:**
*   Тип данных — целые числа (`int`).
*   Алгоритм должен корректно обрабатывать пустые векторы, векторы из одного элемента, а также уже отсортированные или обратно отсортированные массивы.
*   Количество процессов может не быть делителем размера вектора (необходимо обрабатывать остаток от деления).

## 3. Описание алгоритма (последовательного)
Последовательная версия (SEQ) представляет собой обертку над стандартным алгоритмом сортировки из библиотеки STL.

*   **Валидация:** Проверка на пустоту вектора.
*   **Выполнение:** Используется `std::sort` (обычно реализует Introsort — гибрид QuickSort, HeapSort и InsertionSort).
*   **Сложность:** $O(N \log N)$, где $N$ — количество элементов.
*   **Результат:** Входной буфер сортируется in-place.

## 4. Схема распараллеливания
Для реализации параллельной версии (MPI) была выбрана стратегия распределенной сортировки с централизованным слиянием.

Алгоритм выполняется в 4 этапа:

1.  **Распределение данных (Scatter):**
    *   Корневой процесс вычисляет размеры порций для каждого процесса. Так как $N$ может не делиться на $P$ (число процессов) нацело, остаток распределяется между первыми процессами.
    *   Используется `MPI_Scatterv`, так как размеры порций могут отличаться. Данные рассылаются всем процессам.

2.  **Локальная сортировка (Local Sort):**
    *   Каждый процесс независимо сортирует полученную часть данных, используя `std::sort`.
    *   Сложность этапа: $O(\frac{N}{P} \log \frac{N}{P})$.

3.  **Сбор данных (Gather):**
    *   Используется `MPI_Gatherv`. Отсортированные блоки собираются обратно на корневом процессе.
    *   Важно: после сбора массив на корневом процессе представляет собой последовательность из $P$ отсортированных подмассивов, но сам массив глобально еще не отсортирован.

4.  **Слияние (Merge):**
    *   Корневой процесс выполняет слияние полученных частей.
    *   В реализации используется итеративное применение `std::inplace_merge`. Сначала сливаются 1-й и 2-й блоки, затем результат сливается с 3-м блоком и так далее.
    *   Этот этап выполняется последовательно на одном узле.

Данная схема позволяет распараллелить самую трудоемкую часть (сортировку элементов), однако этап слияния и коммуникации накладывает ограничения на масштабируемость (Закон Амдала).

## 5. Экспериментальная установка
**Окружение:** Разработка и тестирование проводились в контейнере Docker (Ubuntu, GCC 14.2.0, OpenMPI).
**Система сборки:** CMake, GoogleTest.
**Параметры запуска:**
*   MPI: `PPC_NUM_PROC=2` (и более).
*   Данные: Случайные числа, сгенерированные через `std::mt19937`.

**Данные для тестов:**
1.  *Функциональные:* Пустой вектор, 1 элемент, обратный порядок, уже отсортированный, случайные значения.
2.  *Производительность:* Вектор размером 1,000,000 элементов типа `int`.

## 6. Результаты и обсуждение

### 6.1. Корректность
Для проверки использовался Google Test.
*   Реализованы тесты с различными входными данными (размеры, порядок элементов).
*   Особое внимание уделено граничным случаям: $N < P$ (когда некоторым процессам достается 0 элементов) и случаям, когда $N$ не делится на $P$ нацело.
*   Все тесты пройдены успешно. Утечек памяти (ASAN) не обнаружено.

### 6.2. Производительность
Сравнение времени выполнения на векторе $10^6$ элементов:

| Режим | Описание | Характеристика |
| :--- | :--- | :--- |
| **SEQ** | `std::sort` | Высокооптимизированная реализация, работает быстро в локальной памяти. |
| **MPI** | Scatterv + Sort + Gatherv + Merge | Имеет накладные расходы на пересылку данных. |

**Анализ:**
На одной машине при малом количестве процессов выигрыш от параллельной сортировки частей может нивелироваться временем, затраченным на `MPI_Scatterv`/`MPI_Gatherv` и последовательным слиянием на корневом узле.
Однако, данный алгоритм позволяет обрабатывать объемы данных, превышающие память одного узла (если изменить фазу начальной генерации), и демонстрирует ускорение на этапе локальной сортировки. Узким местом является фаза слияния (Merge), выполняемая одним процессом.

## 7. Выводы
В ходе работы была реализована задача «Быстрая сортировка с простым слиянием» с использованием MPI.

1.  Реализована декомпозиция задачи: данные корректно разбиваются на неравные части и распределяются между процессами.
2.  Обеспечена корректность сбора результатов и их итогового слияния.
3.  Интерфейсы классов `TestTaskSEQ` и `TestTaskMPI` унифицированы через наследование от `BaseTask`.
4.  Функциональные тесты подтверждают правильность сортировки идентично эталонному `std::sort`.

Работа демонстрирует навыки использования коллективных операций обмена данными (`Scatterv`, `Gatherv`) и комбинирования параллельных вычислений с последовательной постобработкой.

## 8. Источники
1.  MPI Forum. MPI: A Message-Passing Interface Standard.
2.  Кормен Т., Лейзерсон Ч., Ривест Р., Штайн К. Алгоритмы: построение и анализ.
3.  Документация C++ Reference (std::sort, std::inplace_merge).