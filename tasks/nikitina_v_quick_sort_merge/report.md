# Отчет: Быстрая сортировка с простым слиянием

- **Студент:** Никитина Валерия Владимировна
- **Группа:** 3823Б1ФИ2
- **Технология:** MPI, SEQ
- **Вариант:** 14

## 1. Введение

Сортировка больших массивов данных — классическая задача, требовательная к вычислительным ресурсам. С увеличением объема данных время работы последовательных алгоритмов ($O(N \log N)$) становится критичным. Параллельные вычисления позволяют разделить задачу на подзадачи, выполняемые одновременно на разных вычислительных узлах.

В данной работе реализован параллельный алгоритм сортировки, использующий стратегию «разделяй и властвуй». Данные распределяются между процессами, сортируются локально, а затем собираются и сливаются в итоговый массив.

## 2. Постановка задачи

**Цель:** Разработать MPI-приложение для сортировки целочисленного вектора.
**Входные данные:** Вектор `std::vector<int>`, доступный на корневом процессе.
**Выходные данные:** Отсортированный вектор на корневом процессе.

**Требования:**
1.  Реализовать алгоритм быстрой сортировки (QuickSort) вручную, не используя стандартные библиотечные функции сортировки.
2.  Реализовать параллельную версию (MPI) с использованием операций `Scatterv`, локальной сортировки и `Gatherv` с последующим слиянием.
3.  Обеспечить корректность на любых размерах данных.

## 3. Описание алгоритмов

Для обеспечения честного сравнения и в SEQ, и в MPI версиях используется **одинаковая реализация** алгоритма Хоара (QuickSort), написанная вручную.

### 3.1. Реализация QuickSort (Ядро)
Используется классическая схема с рекурсией:
*   Выбирается опорный элемент (pivot) из середины массива.
*   Массив разделяется на две части: слева элементы меньше pivot, справа — больше.
*   Алгоритм рекурсивно вызывается для левой и правой частей.
*   Сложность в среднем: $O(N \log N)$.

### 3.2. Последовательный алгоритм (SEQ)
Последовательная версия выполняет функцию `QuickSortImpl` на всем входном массиве в рамках одного процесса. Это служит базой для измерения "чистого" времени вычисления без накладных расходов на сеть.

### 3.3. Параллельный алгоритм (MPI)
Алгоритм состоит из четырех этапов:

1.  **Распределение (Scatter):**
    *   Корневой процесс (Rank 0) делит входной массив размером $N$ на $P$ частей.
    *   Размер части для процесса $i$: $count_i = N/P + (i < N\%P ? 1 : 0)$.
    *   Используется `MPI_Scatterv` для рассылки блоков разного размера.

2.  **Локальная сортировка (Compute):**
    *   Каждый процесс (включая Root) запускает `QuickSortImpl` для полученного локального буфера.
    *   Этот этап выполняется полностью параллельно.

3.  **Сбор данных (Gather):**
    *   Отсортированные локальные части собираются обратно на корневой процесс через `MPI_Gatherv`.
    *   На этом этапе массив на Root состоит из $P$ отсортированных кусков, идущих подряд.

4.  **Слияние (Merge):**
    *   Корневой процесс выполняет слияние полученных частей в один отсортированный массив.
    *   Используется функция `std::inplace_merge`, применяемая последовательно к границам собранных блоков.

## 4. Экспериментальная часть

### 4.1. Условия эксперимента
*   **Платформа:** Docker-контейнер на локальной машине (4 физических ядра).
*   **Компилятор:** GCC 14.2.0.
*   **Данные:** Вектор размером **1,000,000** элементов (`int`), случайное заполнение.

### 4.2. Результаты измерений
Замеры времени выполнения (среднее по 5 запускам):

| Число процессов (P) | Время выполнения (сек) | Ускорение ($S$) | Эффективность ($E$) |
| :---: | :---: | :---: | :---: |
| **SEQ (1)** | **0.082** | 1.00 | 100% |
| **MPI (1)** | 0.085 | 0.96 | 96% |
| **MPI (2)** | 0.051 | 1.60 | 80% |
| **MPI (3)** | 0.046 | 1.78 | 59% |
| **MPI (4)** | 0.048 | 1.70 | 42% |
| **MPI (8)** | 0.065 | 1.26 | 15% |

### 4.3. Анализ производительности
1.  **SEQ vs MPI(1):** Время практически идентично, небольшое замедление MPI(1) обусловлено инициализацией буферов и лишним копированием памяти при `Scatter/Gather` внутри одного процесса.
2.  **Масштабируемость:**
    *   На 2 и 3 процессах наблюдается хорошее ускорение (до 1.78x). Параллельная сортировка частей перекрывает затраты на коммуникацию.
    *   На 4 и более процессах рост производительности останавливается. Это связано с тем, что этап **Слияния (Merge)** выполняется последовательно на одном узле. Чем больше процессов, тем больше частей нужно слить, и сложность этого этапа начинает доминировать над выигрышем от сортировки.
    *   Также влияет ограничение физических ядер (тесты запускались на 4 ядрах), что при $P > 4$ вызывает конкуренцию за процессорное время.

## 5. Выводы

В ходе работы была реализована параллельная версия алгоритма QuickSort.

1.  **Корректность:** Реализована собственная функция сортировки, которая успешно интегрирована как в SEQ, так и в MPI версии. Тесты подтверждают правильность работы на любых входных данных.
2.  **Эффективность:** Алгоритм показал прирост производительности на малом числе процессов.
3.  **Архитектура:** Использование схемы `Scatter -> Local Sort -> Gather -> Merge` является эффективным решением для распределенной памяти, однако фаза последовательного слияния является узким местом (bottleneck), ограничивающим бесконечное масштабирование.

## 6. Список литературы
1.  MPI Forum. MPI: A Message-Passing Interface Standard.
2.  Кормен Т., Лейзерсон Ч., Ривест Р., Штайн К. Алгоритмы: построение и анализ.
3.  Документация C++ Reference (std::sort, std::inplace_merge).
