# Интегрирование – метод Монте-Карло

**Student:** Дорофеев Иван Денисович, group 3823Б1ФИ1  
**Technology:** SEQ | MPI  
**Variant:** 21

---

## 1. Introduction

Метод Монте-Карло — один из универсальных способов численного интегрирования, позволяющий эффективно оценивать интегралы любой размерности. Однако точность метода и время работы напрямую зависят от количества случайных выборок (samples). Увеличение количества выборок повышает точность, но также увеличивает время выполнения, что делает задачу подходящей для распараллеливания. 

Ожидается, что MPI-версия позволит уменьшить время выполнения за счёт распределения выборок между процессами.

---

## 2. Problem statement

Требуется вычислить значение определённого интеграла методом Монте-Карло.

### Входные данные:

- Границы интегрирования:  
  $a = 0,\quad b = 1$

- Количество выборок: 190000

- Функция:  
  $f(x) = x^2$

### Выходные данные:

- Одно действительное число — приближённое значение интеграла.

---

## 3. Baseline Algorithm (Sequential)

1. Сгенерировать N равномерных случайных точек из интервала $[a, b]$.

2. Вычислить значение функции в каждой точке.

3. Найти среднее всех вычисленных значений.

4. Умножить среднее на длину интервала $(b-a)$.

Последовательная реализация проста, но требует выполнения всех N вычислений в одном процессе.

---

## 4. Parallelization Scheme (MPI)

Для распараллеливания используется идея независимости выборок:  
каждый процесс может самостоятельно выполнить часть выборок и затем отправить свой частичный результат.

### Схема:

1. Корневой процесс рассылает параметры задачи всем участникам.

2. Каждый процесс:
   
   - выполняет часть выборок:   
     $N_p = \frac{N}{P} $
   
   - рассчитывает среднее значение функции на своём подмножестве.

3. Результаты всех процессов объединяются с помощью **MPI_Reduce**.

4. Корневой процесс вычисляет итоговый интеграл.

Поскольку метод Монте-Карло "естественно" распараллеливается, эффективность MPI в идеальных условиях может быть высокой.

---

## 5. Experimental Setup

- Hardware/OS:
  - CPU: 13th Gen Intel i5-13420H (12) @ 4.6GHz, 8 ядер
  - RAM: 16GB RAM
  - OS: Ubuntu 25.10 x86_64
  - Среда выполнения: Docker (Ubuntu trixie/sid (noble) x86_64)
- Toolchain: compiler, version, build type (Release/RelWithDebInfo)
  - CMake 3.28.3
  - g++ 13.3.0
  - OpenMPI
  - Сборка: Release
- Data:
  - Количество выборок: 190000
  - Функция: $f(x)=x^2$
  - Тесты производительности запускались для **SEQ**, **MPI: 2**, **MPI: 4** процессов.

---

## 6. Results and Discussion

### 6.1 Correctness

Корректность проверена тестами GoogleTest.  
Точное значение интеграла:

$\int_0^1 x^2 dx = \frac{1}{3} \approx 0.333333 $


Все варианты укладываются в допуск ±0.05.

---

## 6.2 Performance

| Mode    | Processes | Time (s)     | Speedup  | Efficiency |
| ------- | --------- | ------------ | -------- | ---------- |
| **seq** | 1         | **0.002277** | **1.00** | —          |
| **mpi** | 2         | **0.001751** | **1.30** | **65%**    |
| **mpi** | 4         | **0.001137** | **2.00** | **50%**    |

---

## 7. Discussion

Результаты экспериментов показывают, что MPI-версия действительно работает быстрее последовательной SEQ-реализации при одинаковом количестве выборок.

Для 2 процессов ускорение составляет **1.30×** при эффективности **65%**.  
Для 4 процессов ускорение достигает **2.00×**, что соответствует эффективности **50%**.

Эффективность снижается с ростом числа процессов, это ожидаемое поведение для MPI, так как накладные расходы растут, а объём работы, приходящийся на каждый процесс, уменьшается.

Основные источники потерь производительности:

- необходимость синхронизации между процессами,

- выполнение коллективных операций (например, **MPI_Allreduce**),

- накладные расходы MPI, усиливающиеся при запуске внутри Docker-контейнера,

- относительно малый объём вычислений на один процесс при 4-процессном запуске.

Тем не менее ускорение остаётся значительным — даже с учётом накладных расходов распараллеливание уменьшает общее время расчёта примерно вдвое.

---

## 8. Conclusions

Метод Монте-Карло хорошо поддаётся параллелизации, и MPI-версия показывает ощутимое ускорение.

Хотя эффективность не достигает 100% из-за накладных расходов, ускорение в 2 раза при использовании 4 процессов подтверждает преимущества распараллеливания.

MPI-подход особенно полезен при значительно больших объёмах выборок, где накладные расходы становятся менее заметными.

---

## 9. References

1. "Параллельное программирование для кластерных систем" ИИТММ, ННГУ им. Лобачевского
2. [Open MPI Documentation](https://www.open-mpi.org/doc/)
3. [MPI Reference - Message Passing Interface | Microsoft Learn](https://learn.microsoft.com/en-us/message-passing-interface/mpi-reference)
4. [MPI: A Message-Passing Interface Standard](https://www.mpi-forum.org/docs/mpi-5.0/mpi50-report.pdf)


