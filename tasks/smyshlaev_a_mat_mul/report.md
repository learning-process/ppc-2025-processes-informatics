# <Ленточная горизонтальная схема А, вертикальное В - умножение матрицы на матрицу>

- Student: Смышляев Александр Павлович, group 3823Б1ФИ2
- Technology: SEQ | MPI
- Variant: 14

## 1. Introduction
**Мотивация:** Матричное умножение является одной из базовых и наиболее ресурсоемких операций в линейной алгебре. В отличие от задач с переменной сложностью, здесь объем вычислений детерминирован ($O(N^3)$), что делает задачу идеальным кандидатом для распараллеливания.

**Проблема:** При увеличении размерности матриц время последовательного выполнения растет кубически. Кроме того, стандартный алгоритм умножения часто страдает от неэффективного использования кэш-памяти при доступе к столбцам второй матрицы.

**Ожидаемый результат:** Распараллеливание с использованием MPI и схемы сдвига данных (Ring Shift) должно обеспечить ускорение вычислений. Дополнительная оптимизация (транспонирование матрицы B) должна улучшить локальность данных. Однако на малых размерностях накладные расходы на пересылку данных могут снизить эффективность масштабирования.

## 2. Problem Statement
Даны матрица $A$ размером $M \times K$ и матрица $B$ размером $K \times N$. Требуется вычислить результирующую матрицу $C = A \times B$.
Используемая схема разбиения данных: **горизонтальная для A** (распределение по строкам), **вертикальная для B** (распределение по столбцам).

## 3. Baseline Algorithm (Sequential)
Базовый последовательный алгоритм реализует классическое тройное вложение циклов. Элементы результирующей матрицы вычисляются как скалярное произведение строки $A$ на столбец $B$.

## 4. Parallelization Scheme
Матрица A разбивается на горизонтальные полосы (группы строк), которые хранятся в процессах. Матрица B разбивается на вертикальные полосы (группы столбцов). Чтобы каждый процесс мог вычислить свои строки результирующей матрицы, части матрицы B последовательно передаются от процесса к процессу по кольцевой топологии.

1.  **Препроцессинг:** На корневом процессе матрица $B$ транспонируется. Это позволяет хранить столбцы исходной матрицы $B$ как непрерывные участки памяти (строки).
2.  **Распределение данных:**
    *   Матрица $A$ делится на горизонтальные полосы и рассылается процессам через `MPI_Scatterv`.
    *   Транспонированная матрица $B$ также делится (логически по столбцам, физически по строкам) и рассылается через `MPI_Scatterv`.
3.  **Кольцевой алгоритм (Ring Shift):**
    *   Каждый процесс вычисляет произведение своей полосы $A$ на текущую имеющуюся у него полосу $B$, добавляя результат в соответствующие позиции локальной части $C$.
    *   После вычисления происходит обмен частями матрицы $B$ между соседними процессами по кольцу (используется `MPI_Sendrecv`).
    *   Шаги повторяются `size` раз, пока через каждый процесс не пройдут все блоки матрицы $B$.
4.  **Сбор результатов:** Локальные части матрицы $C$ собираются на корневом процессе с помощью `MPI_Gatherv` и результат рассылается всем участникам c помощью `MPI_BCast`.

## 5. Experimental Setup
- **Hardware/OS:** `Intel Core i7-1255U` (10 ядер, 12 потоков), `16GB RAM`, `Windows 11`
- **Toolchain:** `MSVC v19.38.33130 (Visual Studio 2022)`, `MS-MPI`, `Release`
- **Environment:** `PPC_NUM_PROC`
- **Data:** Квадратные матрицы размером $500 \times 500$ элементов.

## 6. Results and Discussion

### 6.1 Correctness
Корректность алгоритма установлена с помощью Google Test. Тесты покрывают умножение квадратных и прямоугольных матриц, векторов, единичных и нулевых матриц, а также случаи, когда размер матрицы не кратен числу процессов.

### 6.2 Performance
Результаты замеров времени выполнения (TaskRun) для матриц $500 \times 500$:

| Mode        | Count | Time, s | Speedup | Efficiency |
|-------------|-------|---------|---------|------------|
| seq (mpi-1) | 1     |  0.0587 |   1.00  | N/A        |
| mpi         | 2     |  0.0335 |   1.75  | 87.5%      |
| mpi         | 4     |  0.0290 |   2.02  | 50.5%      |
| mpi         | 8     |  0.0242 |   2.43  | 30.4%      |

**Анализ результатов:**
1.  **На 2 процессах** получено хорошее ускорение (1.75x) и высокая эффективность. Распараллеливание оправдывает затраты на коммуникацию.
2.  **На 4 процессах** наблюдается рост производительности, но эффективность падает до 50%. Время выполнения сократилось на ~4.5 мс по сравнению с 2 процессами.
3.  **На 8 процессах** ускорение достигает 2.43x, но эффективность низкая (~30%).

**Причины снижения эффективности:**
Размер задачи ($500 \times 500$) является относительно малым для 8 процессов (время счета всего ~24 мс). В таких условиях накладные расходы на:
*   `MPI_Scatterv` и `MPI_Gatherv`;
*   `MPI_Sendrecv` внутри цикла;
*   `MPI_Bcast` результата;
начинают доминировать над полезными вычислениями.

## 7. Conclusions
Реализованный алгоритм ленточного умножения матриц с использованием MPI корректно решает поставленную задачу. Применение транспонирования матрицы $B$ на этапе препроцессинга позволило оптимизировать доступ к памяти. Эксперименты показали, что для матриц среднего размера ($500 \times 500$) оптимальным является использование 2-4 процессов. Дальнейшее масштабирование (до 8 процессов) дает прирост производительности, но снижает эффективность использования ресурсов из-за доминирования коммуникационных расходов над вычислительными.

## 8. References
1.  Лекции по параллельному программированию ННГУ
2.  Стандарт MPI
3.  Гергель В.П. — Теория и практика параллельных вычислений
4.  Michael J. Quinn — Parallel Programming in C with MPI and OpenMP 