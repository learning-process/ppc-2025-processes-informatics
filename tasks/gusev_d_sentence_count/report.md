# Подсчёт числа предложений в строке
- Студент: Гусев Дмитрий Алексеевич, 3823Б1ФИ1
- Технология: SEQ, MPI 
- Вариант: 25

---

## 1. Введение
Подсчёт предложений в текстовых данных — одна из базовых задач анализа текста. Такая функциональность востребована в системах обработки естественного языка, инструментах оценки сложности текста, а также в приложениях для автоматического создания аннотаций.

В рамках данной работы разработана параллельная реализация алгоритма подсчёта предложений на базе MPI. Проведена верификация корректности работы и исследование масштабируемости решения относительно последовательной версии.

---

## 2. Постановка задачи
**Исходные данные**: произвольная текстовая строка, содержащая буквенные символы, цифры, знаки препинания и пробельные символы.

**Цель**: вычислить число предложений в данной строке.

**Определение предложения**: фрагмент текста, оканчивающийся терминатором — точкой (`.`), восклицательным (`!`) или вопросительным (`?`) знаком. Группы идущих подряд терминаторов (типа `!!!` или `...`) трактуются как окончание одного предложения.

**Вход**: строка произвольной длины.
**Выход**: целое неотрицательное число — количество предложений.

**Требования**:
- алгоритм должен обрабатывать строки значительного размера (мегабайты);
- решение должно функционировать с произвольным числом MPI-процессов;
- разбиение текста на части между процессами не должно приводить к ошибкам в подсчёте.

---

## 3. Последовательный алгоритм
Алгоритм последовательной обработки выполняет однократный проход по строке. При обнаружении терминатора проверяется, не является ли следующий символ также терминатором. Если нет (или достигнут конец строки), счётчик предложений увеличивается.

Алгоритм:
```cpp
sentence_count = 0;
for i from 0 to length(input) - 1:
	if input[i] in {'.', '!', '?'}:
		if i + 1 >= length(input) or input[i + 1] not in {'.', '!', '?'}:
			sentence_count += 1
```

Временная сложность: O(n), где n — длина входной строки.

---

## 4. Параллельная схема
- **Разделение данных**:
	- Текст делится на равномерные сегменты, количество которых равно числу процессов.
	- Распределение сегментов выполняется посредством `MPI_Scatterv`.
	- Для корректной обработки границ каждому процессу передаётся один дополнительный символ из следующего сегмента.
- **Локальные вычисления**:
	Процессы параллельно анализируют свои сегменты функцией `CountSentencesInChunk`, определяя терминаторы и проверяя, что за ними не следует другой терминатор.
- **Сбор результатов**:
	Локальные счётчики суммируются через `MPI_Reduce` на процессе с рангом 0.
- **Коммуникационная модель**:
	Применяется стандартный коммуникатор `MPI_COMM_WORLD` с равноправными процессами. Используются коллективные операции распределения и редукции.

---

## 5. Реализация
- **Структура проекта:**
	- `gusev_d_sentence_count/seq/include/ops_seq.hpp` и `gusev_d_sentence_count/seq/src/ops_seq.cpp` — однопоточная версия.
	- `gusev_d_sentence_count/mpi/include/ops_mpi.hpp` и `gusev_d_sentence_count/mpi/src/ops_mpi.cpp` — MPI-версия.
	- `gusev_d_sentence_count/common/include/common.hpp` — общие типы и интерфейсы.
- **Функциональность**:
	- `IsTerminator(char)` — определяет, является ли символ терминатором (`.`, `!`, `?`).
	- `CountSentencesInChunk(std::vector<char>, int)` — считает предложения в локальном сегменте процесса.
- **Реализационные детали**:
	- **Обработка пустой строки.**  
	    На старте MPI-алгоритма проверяется, не пуста ли входная строка. При пустой строке сразу возвращается `0`, что исключает лишние коммуникационные операции.
	- **Выравнивание длины строки.**  
	    Длина строки приводится к кратности числу процессов путём добавления пробелов в конец.  
	    Это упрощает равномерное распределение данных и устраняет необходимость специальной обработки последнего процесса.  
	    Расширение строки максимум на `comm_size - 1` символов практически не влияет на производительность и обычно не требует переаллокации памяти.
	- **Распределение сегментов.**  
	    Процесс получает свой блок из `chunk_size` символов плюс один символ из начала следующего блока (итого `chunk_size + 1` в буфере).  
	    Это даёт возможность корректно обработать ситуацию, когда предложение разорвано между процессами. В основном цикле обрабатываются только `chunk_size` символов, при этом для каждого найденного терминатора проверяется следующий символ:
	    ```cpp
	    if (IsTerminator(local_chunk[i])) {
			if (!IsTerminator(local_chunk[i + 1])) {
				sentence_count++;
			}
		}
	    ```
	    Такой подход обеспечивает правильный учёт групп терминаторов (например, `!!!`, `...`) как одного предложения, даже если они попадают на разные процессы.
	- **Единообразные буферы.**  
		Все процессы получают буферы одинакового размера `chunk_size + 1`, где лишний символ нужен только для проверки границы. Это делает распределение данных симметричным и упрощает код.

---

## 6. Тестовая конфигурация
- **Оборудование**: 
	Процессор: Intel(R) Core(TM) Ultra 9 185H (16 ядер, 22 логических процессора)  
	ОЗУ: 32 ГБ
- **ОС:** Windows 10 IoT Enterprise Subscription 2009
- **Компилятор:** Microsoft Visual C++ версии 19.44.35220
- **MPI:** Microsoft MPI версии 10.1.12498.18
- **Режим компиляции:** Release с оптимизацией (`/O2`)

---

## 7. Результаты экспериментов
### 7.1 Верификация
Проверка корректности работы алгоритма выполнялась путём сравнения результатов параллельной MPI-реализации с эталонной последовательной версией.

Тестирование проводилось на широком наборе входных данных:
- пустые строки;
- тексты без терминаторов;
- строки с одним предложением;
- тексты с множественными терминаторами;
- комбинации различных символов и терминаторов;
- граничные случаи: группы терминаторов (`!!!`, `...`, `?!`), терминаторы в начале и конце строки.

Во всех тестовых сценариях результаты параллельной и последовательной версий полностью совпадают, что подтверждает корректность реализации алгоритма.

### 7.2 Измерения производительности
Эксперименты по оценке производительности выполнялись на конфигурации, описанной в разделе 6. В качестве тестовых данных использовался текстовый файл значительного размера (порядка нескольких мегабайт).

Измерения времени выполнения проводились для последовательной (SEQ) и параллельной (MPI) версий при различном количестве процессов. Ниже приведены две таблицы: первая — по метрике `pipeline`, вторая — по метрике `task_run`.

Таблица по времени `pipeline`:

| Mode | Count | Time, s | Speedup | Efficiency |
|------|-------|---------|---------|------------|
| seq  |   1   | 0.01728 |   1.00  |     N/A    |
| mpi  |   2   | 0.00723 |   2.39  |   119.5%   |
| mpi  |   4   | 0.00561 |   3.08  |    77.0%   |
| mpi  |   8   | 0.00554 |   3.12  |    39.0%   |

Таблица по времени `task_run`:

| Mode | Count | Time, s | Speedup | Efficiency |
|------|-------|---------|---------|------------|
| seq  |   1   | 0.01717 |   1.00  |     N/A    |
| mpi  |   2   | 0.00688 |   2.49  |   124.5%   |
| mpi  |   4   | 0.00543 |   3.16  |    79.0%   |
| mpi  |   8   | 0.00495 |   3.47  |    43.4%   |

**Анализ результатов:**
- Последовательная версия (seq) при 1 процессе выполнялась за примерно 0.01728 секунды (время `pipeline`) и использовалась как базовое значение для расчёта ускорения.
- При использовании 2 процессов MPI-версия демонстрирует ускорение около 2.39 относительно последовательной версии с эффективностью ~119.5%. Эффективность выше 100% объясняется тем, что при параллельном запуске лучше используются кэш и вычислительные ресурсы процессора (в том числе за счёт особенностей планировщика и агрессивных оптимизаций компилятора).
- При увеличении числа процессов до 4 ускорение возрастает до ≈3.08, однако эффективность снижается до ~77.0% из‑за роста накладных расходов на коммуникацию и синхронизацию между процессами.
- При использовании 8 процессов достигается максимальное ускорение порядка 3.12, но эффективность падает до ~39.0%: добавление процессов даёт небольшой дополнительный выигрыш по времени, однако относительная доля коммуникаций и организационных накладных расходов становится существенной.
- В целом результаты подтверждают целесообразность применения параллельного подхода для обработки больших текстовых данных: MPI‑реализация стабильно опережает последовательную по времени выполнения, а оптимальный выбор числа процессов определяется компромиссом между максимальным ускорением (8 процессов) и эффективным использованием ресурсов (2–4 процесса).

---

## 8. Выводы
- Разработанный параллельный алгоритм корректно выполняет подсчёт предложений в текстовой строке с использованием технологии MPI.
- Параллельная реализация демонстрирует заметное ускорение по сравнению с последовательной версией, достигая максимального ускорения около 3.1–3.2 при использовании 8 процессов (по метрике `pipeline`).
- Наилучшая эффективность (порядка 120%) достигается при 2 процессах, что говорит о хорошем балансе между вычислениями и накладными расходами; при дальнейшем увеличении числа процессов ускорение растёт менее существенно, а эффективность закономерно снижается.
- Основными факторами, ограничивающими масштабируемость, являются накладные расходы на межпроцессное взаимодействие и синхронизацию: при 4 и 8 процессах заметную долю времени начинают занимать коллективные операции MPI, хотя общее время выполнения по‑прежнему меньше, чем у последовательной реализации.
- Алгоритм успешно обрабатывает все граничные случаи, включая пустые строки, группы терминаторов и различные комбинации символов.

---

## 9. Литература
- Материалы курса "Параллельное программирование"
- Справочная документация Open MPI
- Документация Microsoft MPI
- Документация Microsoft Visual C++

---

## 10. Приложение

```cpp
namespace {

bool IsTerminator(char c) {
  return c == '.' || c == '!' || c == '?';
}

size_t CountSentencesInChunk(const std::vector<char> &local_chunk, int chunk_size) {
  size_t sentence_count = 0;

  for (int i = 0; i < chunk_size; ++i) {
    if (IsTerminator(local_chunk[i])) {
      if (!IsTerminator(local_chunk[i + 1])) {
        sentence_count++;
      }
    }
  }
  return sentence_count;
}

}  // namespace

bool GusevDSentenceCountMPI::RunImpl() {
  int rank = 0;
  int comm_size = 0;
  MPI_Comm_rank(MPI_COMM_WORLD, &rank);
  MPI_Comm_size(MPI_COMM_WORLD, &comm_size);

  std::string &text_data = GetInput();

  if (text_data.empty()) {
    GetOutput() = 0;
    return true;
  }

  int chunk_size = 0;
  if (rank == 0) {
    size_t length = text_data.size();

    if (length % comm_size != 0) {
      size_t pad = comm_size - (length % comm_size);
      text_data.resize(length + pad, ' ');
      length = text_data.size();
    }

    text_data.push_back(' ');

    chunk_size = static_cast<int>(length / comm_size);
  }

  MPI_Bcast(&chunk_size, 1, MPI_INT, 0, MPI_COMM_WORLD);

  std::vector<char> local_buffer(chunk_size + 1);
  std::vector<int> counts(comm_size, chunk_size + 1);
  std::vector<int> offsets(comm_size);

  if (rank == 0) {
    for (int i = 0; i < comm_size; ++i) {
      offsets[i] = i * chunk_size;
    }
  }

  MPI_Scatterv(text_data.data(), counts.data(), offsets.data(), MPI_CHAR, local_buffer.data(), chunk_size + 1, MPI_CHAR,
               0, MPI_COMM_WORLD);

  size_t local_res = CountSentencesInChunk(local_buffer, chunk_size);

  size_t total_res = 0;
  MPI_Reduce(&local_res, &total_res, 1, MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);

  if (rank == 0) {
    GetOutput() = total_res;
  }

  return true;
}
```