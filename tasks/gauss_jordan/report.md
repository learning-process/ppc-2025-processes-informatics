**Student:** Лобанов Дмитрий Александрович,  
**Group** 3823Б1ФИ2  
- **Technology:** SEQ-MPI  
- **Variant:** 17  

## 1. Introduction

**Purpose of work**  Реализовать алгоритм  СЛАУ методом Гаусса - Жордана в последовательном и параллельном формате.

    Метод Гаусса-Жордана является классическим алгоритмом линейной алгебры для решения систем линейных уравнений и нахождения обратной матрицы. При работе с большими системами уравнений (размером 1000×1000 и более) последовательная реализация становится вычислительно затратной. Цель данной работы — реализовать параллельную версию метода Гаусса-Жордана с использованием MPI для решения систем линейных уравнений вида AX = B, где A — матрица коэффициентов, X — вектор неизвестных, B — вектор свободных членов. Параллельная реализация позволяет значительно ускорить решение больших систем уравнений за счет распределения вычислений между несколькими процессами.

**Tasks:**
1. Реализовать последовательную(SEQ) версию 
2. Реализовать параллельную(MPI) версию 
3. Сравнить 2 реализации


## 2.Problem Statement

Задача: Решить систему n линейных уравнений с n неизвестными методом Гаусса-Жордана.

Для системы уравнений:

a₁₁x₁ + a₁₂x₂ + ... + a₁ₙxₙ = b₁  
a₂₁x₁ + a₂₂x₂ + ... + a₂ₙxₙ = b₂  
...  
aₙ₁x₁ + aₙ₂x₂ + ... + aₙₙxₙ = bₙ  

Найти вектор X = [x₁, x₂, ..., xₙ]ᵀ, удовлетворяющий всем уравнениям.  

Форматы данных:
Входные данные: расширенная матрица системы std::vector<std::vector<double>> размером n × (n+1)
Выходные данные: вектор решений std::vector<double> размером n или пустой вектор, если система не имеет решения

Ограничения и требования:  
1. Матрица должна быть квадратной (n уравнений, n неизвестных)
2. Система должна быть совместной (иметь хотя бы одно решение)
3. Матрица не должна быть вырожденной (определитель ≠ 0 для единственного решения)
4. Поддерживаются вещественные числа типа double
5. Размер системы может достигать 1000×1000 элементов
6. Алгоритм должен корректно обрабатывать:  
Нулевые элементы на главной диагонали (требуется перестановка строк)  
Близкие к нулю значения (с использованием эпсилон-окрестности)  
Несовместные системы (возврат пустого вектора)  

**Types of solution:**
1. **Единственное решение** — система совместна и определена 
2. **Нет решения** — система несовместна.
3. **Бесконечно много решений** — система совместна, но неопределена  

## 3. Baseline Algorithm (Sequential)

**Алгоритм последовательного вычисления:**

Этап 1: Прямой ход (приведение к ступенчатому виду)
1. Для каждого столбца k от 0 до n-1:
    - Найти строку с максимальным по модулю элементом в столбце k (начиная с строки k)
    - Если максимальный элемент близок к нулю (< ε), пропустить столбец
    - Переставить текущую строку со строкой, содержащей максимальный элемент
    - Нормировать строку k, разделив все её элементы на ведущий элемент aₖₖ
    - Для всех строк i ≠ k вычесть из строки i строку k, умноженную на коэффициент aᵢₖ/aₖₖ
2. Этап 2: Проверка совместности системы
    1. Проверить наличие строк вида [0 0 ... 0 | b], где b ≠ 0 (несовместная система)
    2. Вычислить ранг матрицы системы
    3. Проверить условие совместности: ранг(A) = ранг([A|B])
3. Этап 3: Извлечение решения  
Если система совместна и определена, решение находится в последнем столбце приведённой матрицы


**Реализация на C++:**

```cpp
void TransformToReducedRowEchelonForm(std::vector<std::vector<double>> &augmented_matrix, 
                                     int equations_count, int augmented_columns) {
  int current_row = 0;
  
  for (int current_col = 0; current_col < augmented_columns - 1 && current_row < equations_count; current_col++) {
    // Поиск ведущего элемента
    int pivot_row = FindPivotRow(augmented_matrix, current_row, current_col, equations_count);
    
    // Пропуск нулевого столбца
    if (ShouldSkipColumn(std::abs(augmented_matrix[pivot_row][current_col]))) {
      continue;
    }
    
    // Перестановка и нормировка
    ProcessPivotRow(augmented_matrix, current_row, current_col, pivot_row, augmented_columns);
    
    // Исключение переменной из других строк
    EliminateFromOtherRows(augmented_matrix, current_row, current_col, equations_count, augmented_columns);
    
    current_row++;
  }
}
```
## 4. Parallelization Scheme
**Стратегия параллелизации:**

1. Полное дублирование матрицы: Каждый процесс получает полную копию расширенной матрицы
2. Коллективные операции: Все процессы участвуют в каждой операции
3. Синхронизация: Использование MPI_Barrier для координации вычислений

**Коммуникационные операции**:
1. MPI_Bcast — рассылка данных всем процессам:
    - Размеров матрицы (уравнения, столбцы)
    - Ведущей строки после нормировки
    - Финального решения
2. MPI_Reduce — сбор информации о системе:
    - Наличие несовместных уравнений (MPI_LOR)
    - Ранг матрицы (MPI_MAX)
3. MPI_Barrier — синхронизация процессов между этапами вычислений
**Схема работы каждого процесса:**
```
Фаза 1: Инициализация
    Получить rank и обменяться размерами матрицы
    Получить полную копию расширенной матрицы

Фаза 2: Прямой ход метода Гаусса-Жордана
    Для каждого столбца k от 0 до n-1:
        Найти ведущую строку в столбце k
        Если rank == 0:
            Переставить строки при необходимости
            Нормировать ведущую строку
        Разослать ведущую строку всем процессам
        Исключить переменную из всех строк (кроме ведущей)
        Синхронизировать все процессы

Фаза 3: Анализ результата
    Проверить наличие несовместных уравнений
    Вычислить ранг матрицы
    Собрать информацию на процессе 0
    Если система совместна и определена:
        Извлечь решение из последнего столбца
    Разослать решение всем процессам
```
**Ключевые оптимизации:**
1. Минимизация коммуникаций: Передача только ведущей строки вместо всей матрицы
2. Балансировка нагрузки: Все процессы выполняют одинаковый объём вычислений
3. Коллективные операции: Использование эффективных MPI-операций для синхронизации

## 6. Тестированиe
    
### Экспериментальная установка
* Процессор: AMD Ryzen 5 5600G  (6 физических ядер, 12 логических потока).
* Оперативная память: 16 GB.
* Операционная система: Microsoft Windows 10.
* Конфигурация Release x64.          

### Тестовые данные
**Функциональные тесты:**
- Единичные матрицы 3×3, 4×4, 5×5, 6×6, 7×7
- Диагональные матрицы
- Системы с перестановкой строк
- Системы с отрицательными коэффициентами
- Системы с дробными коэффициентами
- Верхнетреугольные системы
- Системы с большими и малыми числами
- Системы с почти нулевыми диагональными элементами

**Тесты производительности:**
- Малые матрицы: 200×200
- Средние матрицы: 500×500
- Большие матрицы: 1000×1000
## 7. Результаты

Рализация MPI и реализация SEQ проходят все функциональные тесты.

### 7.2 Производительность

В тесте на производительность была матрица 200 на 200, было несколько запусков с разным количеством процессов и разными задачами task_run, pipiline  
- task_run - только вычисления 
- task_pipeline - Полное время выполнения 
```
* результаты для task_run: 
| Режим| Процессы   | Время, мс  | Ускорение  | Efficiency |  
|------|------------|------------|------------|------------|  
| SEQ  |   1        | 19.60      | 1.00       | —          |  
| MPI  |   2        | 12.63      | 1.55       | 78%        |  
| MPI  |   4        | 8.35       | 2.35       | 59%        |  
| MPI  |   8        | 6.53       | 3.00       | 38%        |  
  
* результаты для task_pipeline:   
| Режим            | Процессы | Время, мс | Speedup | Efficiency |  
|------------------|----------|-----------|---------|------------|  
| pipeline SEQ     | 1        | 19.84     | 1.00    | —          |  
| pipeline MPI     | 2        | 13.44     | 1.48    | 74%        |  
| pipeline MPI     | 4        | 9.35      | 2.12    | 53%        |  
| pipeline MPI     | 8        | 7.65      | 2.59    | 32%        |  
```

**Вывод**  
Ключевые результаты производительности:  
Ограниченное ускорение при малых матрицах — Для матрицы 200×200 достигнуто:
- 2 процесса: ускорение 1.55 (эффективность 78%)  
- 4 процесса: ускорение 2.35 (эффективность 59%)  
- 8 процессов: ускорение 3.00 (эффективность 38%)  

Работа успешно выполнена — реализован корректный, тестируемый параллельный алгоритм с предсказуемыми характеристиками производительности. Основные цели достигнуты: понимание принципов MPI-программирования, анализ ограничений параллельных алгоритмов, получение практического опыта в высокопроизводительных вычислениях.