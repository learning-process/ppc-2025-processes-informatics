# Отчёт по реализации алгоритма коммуникаций в звёздной топологии

**Дисциплина:** Параллельное программирование  
**Преподаватели:** Нестеров Александр Юрьевич, Оболенский Арсений Андреевич  
**Студент:** Фролова Софья Юрьевна, 3823Б1ФИ3  
**Вариант:** 8, Звезда

---

## Введение

В рамках данной лабораторной работы была реализована задача организации коммуникаций между процессами в топологии типа "звезда" (star topology). Была разработана параллельная версия алгоритма (MPI), использующая модель передачи сообщений.

Реализация интегрирована в инфраструктуру параллельных задач фреймворка PPC и обладает набором функциональных и производительных тестов.

---

## Постановка задачи

Дана система из $P$ процессов, где один процесс выступает в роли координатора (процесс 0), а остальные $P-1$ процессов — в роли исполнителей.

Каждый процесс $i$ (где $i \neq 0$) имеет:
- целевой процесс $dst_i$ для отправки данных;
- набор данных $data_i$ размером $L$ элементов.

Требуется организовать следующий алгоритм:

1. Каждый рабочий процесс отправляет координатору свой целевой процесс и данные;
2. Координатор принимает информацию, перенаправляет данные нужному процессу и отправляет информацию о источнике;
3. Каждый рабочий процесс получает целевой процесс и данные от координатора;
4. Процесс повторяет шаги до получения сигнала завершения.

Входной тип: `InType = int` (целевой процесс для отправки данных)  
Выходной тип: `OutType = int` (размер полученного массива данных)

---
## Описание схемы последовательного алгоритма (SEQ)

Была добавлена заглушка 

## Описание схемы параллельного алгоритма (MPI)

В звёздной топологии реализована следующая схема:

### Роли процессов

- **Процесс 0 (координатор):**
  - Получает от каждого рабочего процесса целевой процесс и данные
  - Перенаправляет данные нужному адресату
  - Отправляет информацию об источнике данных
  - Отправляет сигнал завершения после обработки всех $P-1$ рабочих процессов

- **Рабочие процессы (1 до P-1):**
  - Отправляют координатору целевой процесс и данные
  - Получают целевой процесс и размер данных от координатора
  - Повторно получают перенаправленные данные
  - Ожидают сигнала завершения (значение -1)

### Последовательность коммуникаций

1. **Инициализация:** каждый рабочий процесс подготавливает буфер для вывода размером $L$
2. **Отправка запроса:** рабочие процессы отправляют координатору целевой процесс
3. **Отправка данных:** рабочие процессы отправляют данные координатору
4. **Маршрутизация:** координатор для каждого полученного запроса:
   - Получает целевой процесс
   - Проверяет размер данных (MPI_Probe)
   - Получает полные данные
   - Отправляет целевому процессу информацию об источнике
   - Отправляет целевому процессу полные данные
5. **Получение перенаправленных данных:** каждый рабочий процесс в цикле:
   - Получает информацию об источнике
   - Проверяет размер данных от источника
   - Получает полные данные от источника
6. **Завершение:** координатор отправляет всем рабочим процессам сигнал -1

### Сложность коммуникаций

Количество сообщений:
- От каждого рабочего процесса к координатору: 2 (целевой процесс + данные)
- От координатора к целевым процессам: $2 \times (P-1)$ (источник + данные)

Всего: $4(P-1)$ сообщений

Объём передаваемых данных: $O(P \times L)$

---

## Реализация в рамках фреймворка PPC

Для задачи создан класс:

- `FrolovaSStarTopologyMPI` — параллельная реализация (MPI)

в пространстве имён: `namespace frolova_s_star_topology`

Класс наследует интерфейс `BaseTask` и реализует виртуальные методы:
- `ValidationImpl()` — проверка корректности входных данных
- `PreProcessingImpl()` — инициализация и подготовка
- `RunImpl()` — основной алгоритм
- `PostProcessingImpl()` — завершающие операции

Файл `common.hpp` определяет:
- Входной тип: `InType = int`
- Выходной тип: `OutType = int`
- Базовый класс задачи через `BaseTask`

---

## Результаты экспериментов и выводы

### Функциональные тесты

Функциональные тесты проверяют корректность работы алгоритма на различных размерах данных:

| Тест | Размер данных | Описание |
|------|---------------|---------|
| `data_length_0` | 0 элементов | Проверка обработки пустого массива |
| `data_length_64` | 64 элемента | Малый размер данных |
| `data_length_1024` | 1024 элемента | Средний размер данных |
| `data_length_32768` | 32768 элементов | Большой размер данных |

Все тесты требуют минимум 3 процесса (1 координатор + 2 рабочих) и выполняют следующие проверки:

- Генерация случайных целевых процессов и данных на процессе 0
- Распространение данных ко всем процессам через MPI_Bcast
- Запуск задачи на каждом рабочем процессе
- Проверка завершения без ошибок через MPI_Reduce

Все тесты успешно проходят для версии MPI.

### Структура данных в тестах

```cpp
std::vector<int> destinations(size - 1); 
std::vector<int> data((size - 1) * DataLength);  
```
<<<<<<< Updated upstream

Каждый рабочий процесс $i$ получает:
- Целевой процесс: `destinations[i-1]`
- Данные: `data[(i-1)*DataLength : i*DataLength]`

### Тесты производительности

Тесты производительности выполняют полный цикл коммуникаций в звёздной топологии:

=======
Каждый рабочий процесс $i$ получает:
- Целевой процесс: `destinations[i-1]`
- Данные: `data[(i-1)*DataLength : i*DataLength]`
### Тесты производительности
Тесты производительности выполняют полный цикл коммуникаций в звёздной топологии:
>>>>>>> Stashed changes
- Размер данных для каждого процесса: 4096 элементов
- Количество процессов: переменное (минимум 3)
- Операции:
  - Генерация случайных целевых процессов
  - Отправка данных и целевых процессов на процесс 0
  - Маршрутизация данных координатором
  - Получение перенаправленных данных
  - Нормализация результата
<<<<<<< Updated upstream

---

## Заключение

В ходе выполнения лабораторной работы были:

=======
---
## Заключение
В ходе выполнения лабораторной работы были:
>>>>>>> Stashed changes
- интегрирована параллельная версия алгоритма коммуникаций в звёздной топологии (MPI);
- интегрирована MPI версия для различных размеров данных (0, 64, 1024, 32768 элементов);
- реализованы тесты производительности и функциональные тесты;
- выполнено форматирование проекта в соответствии с требованиями фреймворка PPC.
<<<<<<< Updated upstream

Реализованная MPI версия демонстрирует эффективное использование топологии "звезда" для перенаправления данных между процессами через центральный координатор, что позволяет избежать прямых коммуникаций между рабочими процессами и упрощает управление коммуникационными паттернами в распределённых системах.

---

## Литература

=======
Реализованная MPI версия демонстрирует эффективное использование топологии "звезда" для перенаправления данных между процессами через центральный координатор, что позволяет избежать прямых коммуникаций между рабочими процессами и упрощает управление коммуникационными паттернами в распределённых системах.
---
## Литература
>>>>>>> Stashed changes
1. Лекции Сысоева А.В. по параллельному программированию
2. Практические занятия Нестерова А.Ю. и Оболенского А.А.
3. Документация MPI (Message Passing Interface)