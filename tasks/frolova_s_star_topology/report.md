# Отчёт по реализации алгоритма коммуникаций в звёздной топологии

**Дисциплина:** Параллельное программирование  
**Преподаватели:** Нестеров Александр Юрьевич, Оболенский Арсений Андреевич  
**Студент:** Фролова Софья Юрьевна, 3823Б1ФИ3  
**Вариант:** 8, Звезда

---

## Введение

В рамках данной лабораторной работы была реализована задача организации коммуникаций между процессами в топологии типа "звезда" (star topology). Была разработана параллельная версия алгоритма (MPI), использующая модель передачи сообщений.

Реализация интегрирована в инфраструктуру параллельных задач фреймворка PPC и обладает набором функциональных и производительных тестов.

---

## Постановка задачи

Дана система из $P$ процессов, где один процесс выступает в роли координатора (процесс 0), а остальные $P-1$ процессов — в роли исполнителей.

Каждый процесс $i$ (где $i \neq 0$) имеет:
- целевой процесс $dst_i$ для отправки данных;
- набор данных $data_i$ размером $L$ элементов.

Требуется организовать следующий алгоритм:

1. Каждый рабочий процесс отправляет координатору свой целевой процесс и данные;
2. Координатор принимает информацию, перенаправляет данные нужному процессу и отправляет информацию о источнике;
3. Каждый рабочий процесс получает целевой процесс и данные от координатора;
4. Процесс повторяет шаги до получения сигнала завершения.

Входной тип: `InType = int` (целевой процесс для отправки данных)  
Выходной тип: `OutType = int` (размер полученного массива данных)

---
## Описание схемы последовательного алгоритма (SEQ)

Была добавлена заглушка 

## Описание схемы параллельного алгоритма (MPI)

В работе реализована топология звезда с использованием библиотеки MPI. Процесс с рангом `0` выступает в роли центрального координатора, а все остальные процессы — периферийными узлами, обменивающимися данными исключительно через координатор.

---

## Роли процессов

### Процесс 0 (координатор)

Координатор выполняет следующие функции:

- принимает от каждого периферийного процесса:
  - номер целевого процесса `dst`;
  - массив данных произвольного размера;
- определяет фактический источник сообщения (`src`) на основе `MPI_Status`;
- проверяет корректность целевого процесса:
  - если `dst` некорректен или совпадает с `src`, данные возвращаются отправителю;
  - иначе данные перенаправляются целевому процессу `dst`;
- отправляет целевому процессу:
  - ранг источника данных;
  - сам массив данных;
- после обработки всех `P-1` периферийных процессов рассылает сигнал завершения `kTerm = -1`.

---

### Периферийные процессы (ранги 1 … P−1)

Каждый периферийный процесс:

- определяет целевой процесс `dest_` на основе входных данных;
- гарантирует, что `dest_`:
  - лежит в диапазоне `[1, P-1]`;
  - не совпадает с собственным рангом;
- отправляет координатору:
  - целевой процесс `dest_`;
  - массив входных данных (или пустое сообщение);
- в цикле принимает сообщения от координатора:
  - сначала ранг источника данных;
  - затем массив данных соответствующего размера;
- завершает работу при получении значения `kTerm = -1`;
- сохраняет результат в виде размера полученного массива данных.

---

## Последовательность коммуникаций

1. **Предобработка (`PreProcessingImpl`)**
   - Определяется ранг и общее число процессов
   - Для периферийных процессов корректируется `dest_`
   - Инициализируется выходной буфер

2. **Отправка данных координатору**
   - Периферийный процесс отправляет `dest_`
   - Затем отправляет массив данных (или сообщение нулевой длины)

3. **Маршрутизация у координатора**
   - Координатор принимает `dest_` от любого источника (`MPI_ANY_SOURCE`)
   - Определяет фактический источник `src`
   - С помощью `MPI_Probe` и `MPI_Get_count` определяет размер входного сообщения
   - Получает массив данных
   - В зависимости от корректности `dest_`:
     - пересылает данные целевому процессу;
     - либо возвращает данные отправителю

4. **Получение перенаправленных данных**
   - Периферийные процессы в цикле:
     - получают ранг источника;
     - определяют размер сообщения;
     - принимают массив данных;
   - выход из цикла при получении `kTerm`

5. **Завершение**
   - Координатор рассылает всем периферийным процессам сигнал завершения `-1`

---

## Корректность и устойчивость алгоритма

В реализации предусмотрены дополнительные меры защиты:

- исключена отправка сообщений самому себе;
- некорректные значения целевого процесса обрабатываются безопасно;
- поддерживается передача сообщений произвольного размера;
- завершение всех периферийных процессов гарантируется явным сигналом.

### Объём передаваемых данных

- Основной объём данных определяется суммарным размером массивов:
  \[
  O(P \cdot L)
  \]
  где `L` — средний размер массива данных одного процесса.

## Реализация в рамках фреймворка PPC

Для задачи реализован класс:

- `FrolovaSStarTopologyMPI` — MPI-реализация звёздной топологии

Пространство имён:
namespace frolova_s_star_topology

## Результаты экспериментов и выводы

### Функциональные тесты

Функциональные тесты предназначены для проверки корректности работы реализации звёздной топологии как в MPI, так и в последовательной (SEQ) версиях алгоритма.

Тестирование реализовано с использованием Google Test и параметризованных тестов (`TEST_P`).

---

#### Параметры тестов

Каждый тест определяется параметром типа `TestType`, содержащим:

- входное значение `InType` — целевой процесс;
- строковый идентификатор теста (используется для имени теста).

Набор параметров:

| Тестовый параметр | Входное значение (`InType`) | Описание |
|------------------|-----------------------------|----------|
| `(1, "1")` | 1 | Минимально допустимый целевой процесс |
| `(2, "3")` | 2 | Корректный целевой процесс |
| `(3, "4")` | 3 | Проверка работы при большем значении входа |

---

#### Проверяемые реализации

Для каждого набора параметров тесты автоматически запускаются для двух реализаций задачи:

- `FrolovaSStarTopologyMPI` — параллельная MPI-реализация;
- `FrolovaSStarTopologySEQ` — последовательная реализация.

Формирование списка тестируемых задач выполняется с помощью утилит фреймворка PPC:
- `AddFuncTask`
- `ExpandToValues`

---

#### Логика выполнения теста

Каждый тест выполняет следующие шаги:

1. **Инициализация входных данных**
   - В методе `SetUp()` из параметров теста извлекается значение `InType`.

2. **Запуск задачи**
   - Выполняется вызов `ExecuteTest(GetParam())`, который:
     - инициализирует соответствующую реализацию задачи;
     - запускает `PreProcessingImpl`, `RunImpl` и `PostProcessingImpl`.

3. **Проверка результата**
   - Метод `CheckTestOutputData` проверяет, что:
     - выходное значение `OutType` неотрицательно (`output_data >= 0`).

---

#### Критерии успешного прохождения

Тест считается успешно пройденным, если:

- выполнение завершается без ошибок;
- возвращаемое выходное значение корректно (неотрицательно);
- поведение идентично для MPI- и SEQ-реализаций при одинаковых входных данных.

---

#### Итог

Функциональные тесты подтверждают корректность работы алгоритма звёздной топологии для различных входных параметров и обеспечивают согласованность поведения последовательной и параллельной реализаций в рамках фреймворка PPC.

### Тесты производительности

Тесты производительности предназначены для оценки времени выполнения алгоритма звёздной топологии и корректности его работы при многократном запуске полного цикла вычислений и коммуникаций.

Тестирование реализовано с использованием **Google Test** и утилит производительного тестирования фреймворка **PPC**.

---

#### Конфигурация тестов

Основные параметры тестирования:

- **Размер входных данных (`InType`)**: `10000`
- **Количество повторений**: определяется инфраструктурой `BaseRunPerfTests`
- **Количество процессов**:
  - для MPI-версии — не менее 3 (1 координатор + рабочие процессы);
  - для SEQ-версии — 1 процесс
- **Тестируемые реализации**:
  - `FrolovaSStarTopologyMPI`
  - `FrolovaSStarTopologySEQ`

---

#### Подготовка данных

В методе `SetUp()` каждому тесту задаётся фиксированное входное значение:

- `input_data_ = 10000`

Данное значение используется как параметр задачи и передаётся в метод `GetTestInputData()`.

---

#### Логика выполнения теста

Каждый тест производительности выполняет следующий сценарий:

1. **Инициализация задачи**
   - Создание экземпляра соответствующей реализации (MPI или SEQ)
   - Подготовка входных данных

2. **Многократный запуск алгоритма**
   - Полный цикл выполнения:
     - `PreProcessingImpl`
     - `RunImpl`
     - `PostProcessingImpl`

3. **Коммуникации (для MPI-версии)**
   - Передача целевых процессов и данных координатору
   - Маршрутизация сообщений процессом 0
   - Получение перенаправленных данных рабочими процессами
   - Корректное завершение всех процессов

4. **Проверка результата**
   - Контроль корректности выходных данных:
     - `OutType >= 0`

---

#### Формирование набора тестов

Набор производительных тестов формируется автоматически:

- для всех режимов выполнения;
- для MPI- и SEQ-реализаций;

с использованием утилит:

- `MakeAllPerfTasks`
- `TupleToGTestValues`

Тесты параметризуются и инстанцируются через `INSTANTIATE_TEST_SUITE_P`.

---

#### Критерии успешного прохождения

Тест производительности считается успешным, если:

- выполнение завершается без ошибок;
- соблюдается корректность выходных данных;
- алгоритм стабильно работает при многократных запусках;
- поведение MPI- и SEQ-версий согласовано с точки зрения интерфейса задачи.

---

#### Итог

Проведённые тесты производительности подтверждают устойчивость и корректность реализации звёздной топологии при интенсивных запусках и позволяют сравнивать эффективность параллельной (MPI) и последовательной (SEQ) версий алгоритма.

---
## Заключение

В ходе выполнения лабораторной работы были получены следующие результаты:

- реализована и интегрирована параллельная версия алгоритма коммуникаций в звёздной топологии с использованием MPI;
- реализована последовательная (SEQ) версия алгоритма для сопоставления корректности и производительности;
- обеспечена корректная работа алгоритма при различных значениях входного параметра `InType`, определяющего целевой процесс;
- разработаны и успешно выполнены функциональные тесты, проверяющие корректность работы MPI- и SEQ-реализаций;
- реализованы тесты производительности, выполняющие многократные запуски полного цикла алгоритма при фиксированном входном параметре;
- проект приведён в соответствие с требованиями фреймворка PPC и стандартами оформления.

Разработанная MPI-реализация демонстрирует корректное и устойчивое использование звёздной топологии для маршрутизации сообщений через центральный координатор. Такой подход исключает прямые коммуникации между рабочими процессами, упрощает управление обменом данными и обеспечивает предсказуемое поведение алгоритма в распределённой среде.

---
## Литература
1. Лекции Сысоева А.В. по параллельному программированию
2. Практические занятия Нестерова А.Ю. и Оболенского А.А.
3. Документация MPI (Message Passing Interface)