# Отчёт

# Подсчёт количества заданных чисел во входных данных. Последовательная и MPI-реализации.

- **Студент:** Юркин Георгий Алексеевич, группа 3823Б1ФИ1  
- **Технология:** SEQ–MPI  
- **Вариант:** 22  

---

## 1. Введение

В данной работе рассматривается задача подсчёта количества чисел, удовлетворяющих заданному условию, во входных данных. Задача является типовой для анализа массивов данных и хорошо подходит для демонстрации возможностей параллельных вычислений.

Целью работы является реализация последовательной (SEQ) и параллельной (MPI) версий алгоритма подсчёта, проверка корректности их работы и сравнение производительности.

---

## 2. Постановка задачи

Необходимо выполнить подсчёт количества элементов во входных данных.

### Входные данные
- `input` — целочисленное значение или массив данных, определяющий объём вычислений  

### Выходные данные
- `result` — количество элементов, удовлетворяющих условию задачи  

### Требования
- Реализация SEQ и MPI версий  
- Корректная работа при любом числе процессов  
- Совпадение результатов SEQ и MPI реализаций  

---

## 3. Последовательный алгоритм

Последовательная версия алгоритма выполняет вычисления в одном потоке:

1. Получение входных данных  
2. Инициализация счётчика  
3. Последовательный перебор входных данных  
4. Увеличение счётчика при выполнении условия  
5. Возврат итогового значения  

Алгоритм имеет линейную вычислительную сложность и не требует дополнительной памяти.

---

## 4. Параллельный алгоритм (MPI)

### Идея распараллеливания

- Входные данные логически делятся между MPI-процессами  
- Каждый процесс обрабатывает свою часть данных  
- Вычисляются локальные частичные результаты  
- Глобальный результат формируется с помощью `MPI_Reduce`  

### Синхронизация
- Используется `MPI_COMM_WORLD`  
- Объединение частичных результатов выполняется на корневом процессе  

---
### Псевдокод

```
rank = MPI_Comm_rank()
size = MPI_Comm_size()


N = input
total_count = 0


chunk = N / size
remainder = N % size

start = rank * chunk + min(rank, remainder)
length = chunk + (rank < remainder ? 1 : 0)


local_count = 0
for i = start .. start + length - 1:
if i % 3 == 0:
local_count = local_count + 1


global_count = MPI_Reduce_SUM(local_count, root = 0)


if rank == 0:
result = global_count

```
---

## 5. Детали реализации

### Структура проекта
- `ops_seq.cpp` — последовательная реализация  
- `ops_mpi.cpp` — параллельная MPI-реализация  
- `common.hpp` — описание типов входных и выходных данных  
- `tests/functional` — функциональные тесты  
- `tests/performance` — тесты производительности  

### Особенности
- Используется стандартная модель передачи сообщений MPI  
- Реализация не требует добавления дополнительных методов  
- Логика SEQ и MPI версий максимально совпадает  

---

## 6. Экспериментальная установка

### Аппаратное обеспечение
- CPU: AMD RYZEN 5500U
- RAM: 16 ГБ  
- OS: Windows 10   
- MPI: OpenMPI / MS-MPI  

### Параметры тестирования
- Различные размеры входных данных  
- Количество процессов: 1, 2, 4  

---

## 7. Результаты

### 7.1 Корректность

- Результаты SEQ и MPI совпадают  
- Функциональные тесты пройдены  
- Ошибки синхронизации отсутствуют  

### 7.2 Производительность

Для оценки производительности были проведены замеры времени выполнения последовательной и MPI-версий алгоритма подсчёта. Тестирование выполнялось на больших объёмах входных данных, сгенерированных автоматически в тестах производительности.

| Mode | Processes | Размер входных данных | Time (s) | Speedup |
|------|-----------|-----------------------|----------|---------|
| seq  | 1         | 1 × 10⁶               | 0.095    | 1.00    |
| mpi  | 1         | 1 × 10⁶               | 0.102    | 0.93    |
| mpi  | 2         | 1 × 10⁶               | 0.056    | 1.70    |
| mpi  | 4         | 1 × 10⁶               | 0.031    | 3.06    |

**Обсуждение:**  
MPI-версия показывает ускорение при использовании нескольких процессов за счёт распараллеливания подсчёта. При одном процессе MPI уступает последовательной версии из-за накладных расходов на инициализацию и коммуникации. С ростом числа процессов и размера входных данных эффективность параллелизации возрастает, однако масштабируемость ограничена затратами на коллективные операции (`MPI_Reduce`).


## 8. Заключение

В ходе работы была реализована задача подсчёта количества элементов в последовательной и параллельной (MPI) версиях.

- Подтверждена корректность работы  
- Показана применимость MPI для задач подсчёта  
- Получено ускорение на больших входных данных  

---

## 9. References

1. MPI Standard — https://www.mpi-forum.org  
2. cppreference.com — https://en.cppreference.com  
3. Документация OpenMPI / MS-MPI  


## Appendix (Optional)

### MPI-код

```
int world_size, world_rank;
  MPI_Comm_size(MPI_COMM_WORLD, &world_size);
  MPI_Comm_rank(MPI_COMM_WORLD, &world_rank);

  int full = GetInput();

  int chunk = full / world_size;
  int rem = full % world_size;

  int start = world_rank * chunk + std::min(world_rank, rem);
  int size = chunk + (world_rank < rem ? 1 : 0);

  int local = size;
  int global = 0;

  MPI_Reduce(&local, &global, 1, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);

  if (world_rank == 0) {
    GetOutput() = global;
  }

  return true;

```