#  Реализация коллективной операции AllReduce

**Студент** Кутергин Антон
**Группа** 3823Б1ФИ1
**Вариант** 3

## 1 Введение
Задача AllReduce заключается в объединении данных со всех процессов с использованием некоторой операции (в данном случае — суммирования) и последующей рассылке результата обратно всем участникам.Данная работа мотивирована необходимостью эффективного вычисления глобальных агрегатов на распределенных системах, где прямой последовательный расчет становится неэффективным из-за больших объемов данных.

## 2 Постановка задачи
Требуется реализовать функцию, повторяюшая MPI_Reduce.
Необходимо вычислить общую сумму всех элементов вектора со всех процессов и сделать этот результат доступным каждому процессу.

## 3 Последовательная версия
Последовательный алгоритм реализован в классе AllreduceSequential. Он принимает на вход весь вектор данных и использует стандартную функцию std::accumulate из заголовочного файла <numeric>. Алгоритм проходит по вектору один раз, выполняя N-1 операций сложения, где $N$ — общее количество элементов.

## 4 Параллельная версия
Схема параллельной реализации базируется на распределении данных и коллективном взаимодействии:
* Распределение данных: Общий объем данных делится поровну между P процессами.
* Локальное вычисление: Каждый процесс параллельно вычисляет сумму своей части вектора с помощью std::accumulate.
* Коммуникационный паттерн: Используется функция MPI_Allreduce. Внутри она может быть реализована через дерево (Recursive Doubling) или алгоритм кольца (Bruck algorithm), что обеспечивает логарифмическую сложность обмена данными O(log P).
* Роли рангов: Все процессы симметричны, по завершении каждый получает итоговое значение.

## 5 Детали реализации
ops_seq.cpp/hpp: Последовательная реализация.
ops_mpi.cpp/hpp: Параллельная реализация с вызовом MPI.
Память: Использование std::vector обеспечивает эффективное управление памятью. Данные в тестах производительности генерируются «на лету» для экономии места.

## 6 Экспериментальная установка
* Hardware/OS: Intel Core i5-8300H, 4 ядра, 12 Gb RAM, Windows 10
* Toolchain: MSVC (Visual Studio 2022)
* Build type: Release
* Environment: Тесты запускались через mpiexec с флагами -n 1, -n 2, -n 4, -n 8.
* Data: Вектор из 100 000 000 элементов.

## 7 Результаты 

# 7.1 Корректность
Корректность была подтверждена с помощью функциональных тестов, покрывающих:
* Случаи с пустыми векторами.
* Векторы разного размера.
* Различные идентификаторы корневого процесса.
* Сравнение результата MPI с эталонным последовательным результатом.

# 7.2 Перфоманс
| Mode        | Count | Time, s | Speedup | Efficiency |
|-------------|-------|---------|---------|------------|
| seq         | 1     | 0.022678| 1.00    | N/A        |
| mpi         | 2     | 0.017855| 1.27    | 63.5%      |
| mpi         | 4     | 0.017777| 1.28    | 32.0%      |
| mpi         | 8     | 0.016690| 1.36    | 17.0%      |

# 7.3 Обсуждение
На объеме в 100 млн элементов удалось достичь стабильного ускорения. Максимальное ускорение (1.36x) наблюдается при 8 процессах. Относительно низкая эффективность при росте числа ядер объясняется тем, что операция суммирования является memory-bound (ограничена скоростью памяти). Процессы конкурируют за пропускную способность шины памяти при чтении вектора, что не дает времени выполнения сокращаться линейно.

# 8 Вывод
В ходе работы реализован параллельный алгоритм AllReduce, полностью повторяющий MPI_Reduce. Эксперименты показали, что параллелизация эффективна для больших массивов данных, однако для простых арифметических операций основным сдерживающим фактором является пропускная способность памяти. MPI-реализация успешно проходит все тесты на корректность и демонстрирует преимущество над последовательной версией.