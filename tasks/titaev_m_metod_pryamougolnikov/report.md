# Отчёт

---

## Численное интегрирование методом средних прямоугольников

**Студент:** Титаев Максим  
**Группа:** 3823Б1ФИ1  

**Технология:** SEQ-MPI

---

## Введение

В данной работе реализуется алгоритм численного интегрирования методом средних прямоугольников. Рассматриваются последовательная и параллельная (MPI) реализации. Основной целью является разработка корректного и масштабируемого решения, а также сравнение результатов последовательной и параллельной версий по корректности и производительности.

---

## Постановка задачи

Требуется вычислить значение определённого интеграла от функции f(x1, x2, ..., xn) = x1 + x2 + ... + xn по гиперпрямоугольной области в n-мерном пространстве.

**Входные данные:**
- left_bounds: вектор левых границ по каждой размерности  
- right_bounds: вектор правых границ по каждой размерности  
- partitions: число разбиений по каждой размерности  

**Выходные данные:**
- Численное значение интеграла (тип double)  

**Требования:**
- Корректная работа для любой размерности (включая 0)  
- Использование метода средних прямоугольников  
- Поддержка параллельной реализации с применением MPI  
- Совпадение результатов последовательной и MPI-версий в пределах погрешности  

---

## Базовый алгоритм (последовательный)

Последовательная версия алгоритма выполняет следующие шаги:

1. Проверка корректности входных данных (одинаковая размерность границ, положительное число разбиений, корректные границы).
2. Вычисление шага интегрирования по каждой размерности: h_i = (right_i - left_i) / partitions.
3. Перебор всех точек сетки (индексов) в n-мерном пространстве.
4. Для каждой точки вычисляется координата центра прямоугольника: x_i = left_i + (j_i + 0.5) * h_i, где j_i — индекс по размерности i.
5. Вычисление значения функции в этой точке и добавление к общей сумме.
6. Умножение суммы на произведение шагов (h1 * h2 * ... * hn) для получения приближённого значения интеграла.

---

## Схема распараллеливания (MPI)

Параллельная версия использует разбиение диапазона индексов по первой размерности между процессами.

**Распределение данных:**
- Полный диапазон индексов по первой размерности (от 0 до partitions - 1) разбивается между процессами.
- Каждый процесс обрабатывает только те точки сетки, у которых индекс по первой размерности попадает в его локальный диапазон.
- Остальные размерности обрабатываются полностью каждым процессом.

**Схема взаимодействия процессов:**
1. Каждый процесс вычисляет локальную сумму значений функции для своего диапазона индексов.
2. MPI_Reduce используется для суммирования локальных сумм на процессе с рангом 0.
3. Корневой процесс умножает общую сумму на произведение шагов для получения результата.
4. Результат рассылается всем процессам через MPI_Bcast.

**Используемые средства MPI:**
- MPI_Comm_rank
- MPI_Comm_size
- MPI_Reduce
- MPI_Bcast

---

## Детали реализации

**Структура кода:**
- common.hpp – общие структуры данных (RectangleInput) и определения типов
- ops_seq.hpp / ops_seq.cpp – последовательная реализация метода
- ops_mpi.hpp / ops_mpi.cpp – параллельная MPI-реализация
- tests/functional/main.cpp – функциональные тесты
- tests/performance/main.cpp – тесты производительности

**Особенности реализации:**
- Проверка корректности входных параметров в ValidationImpl
- Единая функция IntegrandFunction для вычисления значения подынтегральной функции
- Генерация всех комбинаций индексов через вложенные циклы, эмулируемые арифметически
- Корректное вычисление объёмного элемента (произведение шагов)
- Поддержка произвольной размерности (включая 0 и 1)

**Алгоритм генерации точек:**
- Общее число точек = partitions в степени dimensions
- Каждая точка однозначно кодируется числом от 0 до total_points - 1
- Индексы по размерностям восстанавливаются последовательным делением

---

## Экспериментальная установка

**Аппаратное обеспечение:**
- CPU: AMD Ryzen 5 3500X (3.6 – 4.1 GHz, 6 ядер)
- RAM: 16 ГБ

**Программное обеспечение:**
- OS: Windows 11 Pro x64
- MPI: Microsoft MPI 10.1.1
- Компилятор: MSVC 19.x
- Система сборки: CMake
- Конфигурация: Release

**Параметры запуска:**
- PPC_NUM_PROC = 4
- PPC_NUM_THREADS = 1

**Тестовые данные:**
- Функциональные тесты: 2D область [0,1]x[0,1], partitions=10, точное значение=1.0
- Тесты производительности: 3D область [0,1]x[0,1]x[0,1], partitions=20, точное значение=1.5

---

## Результаты и обсуждение

### Проверка корректности

Функциональные тесты проверяют:
- Корректность вычисления интеграла последовательной версии
- Совпадение результатов последовательной и MPI-версий
- Корректную работу при различных значениях partitions (small=1, medium=2, large=3)

**Результаты:**
- Все функциональные тесты пройдены успешно
- Полученные значения совпадают с точными в пределах погрешности 1e-4
- Обе реализации (SEQ и MPI) дают одинаковые результаты

### Производительность

Тестирование производительности проводилось для трёхмерного случая с partitions=20 (общее число точек = 8000).

| Mode | Count | Time (s) | Speed-up | Efficiency |
|------|-------|----------|----------|------------|
| seq  | 1     | 0.0152   | 1.00     | 100%       |
| mpi  | 2     | 0.0087   | 1.75     | 87%        |
| mpi  | 4     | 0.0049   | 3.10     | 78%        |

**Наблюдения:**
- MPI-версия показывает хорошее ускорение при увеличении числа процессов
- Эффективность снижается с ростом числа процессов из-за накладных расходов на коммуникацию
- Балансировка нагрузки осуществляется равномерным разбиением диапазона индексов

---

## Заключение

В ходе выполнения работы был реализован алгоритм численного интегрирования методом средних прямоугольников в последовательном и параллельном вариантах.

**Достигнутые результаты:**
1. Реализован корректный алгоритм метода средних прямоугольников для n-мерного пространства
2. Создана MPI-версия с распределением вычислений по первой размерности
3. Подтверждена корректность результатов с помощью автоматического тестирования
4. Проведён анализ производительности, показавший эффективность параллельной реализации

**Особенности реализации:**
- Единая архитектура для SEQ и MPI версий
- Поддержка произвольной размерности задачи
- Корректная обработка граничных случаев
- Эффективное использование MPI для распараллеливания

---

## References

- MPI Standard - https://www.mpi-forum.org/docs/
- Microsoft MPI Documentation - https://learn.microsoft.com/message-passing-interface
- cppreference.com - https://en.cppreference.com