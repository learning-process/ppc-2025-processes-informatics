# Передача от всех одному (reduce)

- Student: Завьялов Алексей Алексеевич, group 3823Б1ФИ3
- Technology: SEQ | MPI
- Variant: 2

## 1. Introduction

Операция редукции является одной из ключевых коллективных операций в параллельных вычислениях и широко используется при агрегации данных, полученных в разных процессах. В частности, операция Reduce применяется для вычисления суммы, минимума, максимума и других ассоциативных функций.

Целью данной работы является реализация пользовательской версии операции MPI_Reduce и сравнение её производительности со встроенной реализацией, предоставляемой MPI-библиотекой.

Ожидается, что пользовательская реализация будет уступать оптимизированной библиотечной версии, особенно при малом числе процессов, однако при увеличении числа процессов разница во времени выполнения может уменьшаться.

## 2. Problem Statement

Задача редукции заключается в объединении данных, распределённых между процессами, в одно результирующее значение на заданном корневом процессе.

В данной работе требуется реализовать операцию Reduce для следующих функций:
- суммирование (MPI_SUM);
- поиск минимума (MPI_MIN);

для массивов целых и вещественных чисел.

Формально операция редукции для суммирования может быть записана следующим образом:

```math
res_i = \sum_{p=0}^{P-1} a_{p,i},
```

где $P$ — число процессов, $a_{p,i}$ — $i$-й элемент массива процесса $p$, $res_i$ — $i$-й элемент результирующего массива.

Операция редуцирования для минимума аналогично записывается в следующем виде:

$$
R_i = \min_{p = 0,\ldots,P-1} a_{p,i}
$$


где $P$ — число процессов, $a_{p,i}$ — $i$-й элемент массива процесса $p$, $res_i$ — $i$-й элемент результирующего массива.

### Входные данные

Тип операции (MPI_SUM или MPI_MIN), тип данных (int, float, double), размер массива, указатель на входной массив и номер корневого процесса.

### Выходные данные

Массив, содержащий результат редукции.

## 3. Baseline Algorithm (Sequential)

В последовательной версии используется встроенная функция MPI_Reduce, реализованная в MPI-библиотеке.

Для корректной обработки seq версии на CI, данную реализацию пришлось закомментировать. При это тесты для замера производительности были проведены локально.

## 4. Parallelization Scheme

В пользовательской MPI-версии операция Reduce реализована вручную с использованием бинарного дерева.

Каждый процесс копирует свои входные данные во внутренний буфер. Далее процессы объединяются в группы, размер которых удваивается на каждой итерации. На каждом уровне бинарного дерева процессы-лидеры принимают данные от соседних процессов и агрегируют полученные значения с помощью выбранной операции.

После завершения редукции результат находится у корневого процесса. Для обеспечения доступности результата всем процессам выполняется рассылка результата с помощью MPI_Bcast.

Для обмена данными используются явные вызовы MPI_Send и MPI_Recv.

## 5. Experimental Setup

Hardware/OS: AMD Ryzen 5 7520U, 4 ядра, 16 GB RAM, Windows 10 x64.

Toolchain:
- CMake 3.28.3;
- компилятор g++ (Ubuntu 13.3.0-6ubuntu2~24.04) 13.3.0;
- использовался Docker-контейнер;
- режим сборки Release.

Data: для замера производительности использовались массивы размером 20 000 000 элементов. Выполнялась редукция с операцией суммирования целых чисел. Все элементы исходных массивов равны 1. 

## 6. Results and Discussion

### 6.1 Correctness

Проверка корректности выполнена через Google Test на 36 тестовых конфигурациях:
- Типы данных: `MPI_INT`, `MPI_FLOAT`, `MPI_DOUBLE`
- Размеры: 9, 10, 50 элементов
- Получатели: ранги 0 и 1
- Операция: `MPI_SUM`, `MPI_MIN`

### 6.2 Performance
| Mode | Count | Time, s | Speedup | Efficiency |
|------|-------|---------|---------|------------|
| seq  | 2     | 0.388   | 1.00    | N/A        |
| mpi  | 2     | 0.847   | 0.46    | 45.80%     |
| seq  | 3     | 0.538   | 1.00    | N/A        |
| mpi  | 3     | 0.760   | 0.71    | 70.77%     |
| seq  | 4     | 1.001   | 1.00    | N/A        |
| mpi  | 4     | 1.096   | 0.91    | 91.32%     |

## 7. Conclusions

В MPI-версии используется пользовательская реализация операции Reduce на основе бинарного дерева с явными вызовами MPI_Send и MPI_Recv и последующим использованием MPI_Bcast.

В последовательной версии применяется оптимизированная реализация MPI_Reduce, предоставляемая MPI-библиотекой.

Из-за накладных расходов на коммуникации и синхронизацию, а также отсутствия низкоуровневых оптимизаций, пользовательская MPI-реализация уступает встроенной MPI_Reduce по времени выполнения, особенно при малом числе процессов.

Полученные результаты соответствуют теоретическим ожиданиям. Накладные расходы на управление процессами и передачу данных занимают значительную часть времени выполнения, что снижает эффективность параллельной версии.

## 8. References

1. Курс лекций ННГУ «Параллельное программирование для кластерных систем»
