# Обобщенная передача от одного всем (scatter)
- Студент: Пихоцкий Роман Владимирович, 3823Б1ФИ1 
- Технологии: SEQ | MPI
- Вариант: 4

## 1. Введение
Одной из самых популярных в библиотеке MPI функций является `MPI_Scatter` из-за ее оптимального сочитания простоты использования и эффективности, ведь при написании параллельного кода часто возникает необходимость распределять исходные данные между процессами для их последующей независимой обработки.

Целью данной работы является реализация собственной функции для рассылки данных с использованием базовых функций передачи сообщений `MPI_Send` и `MPI_Recv`.

## 2. Постановка задачи
Пусть задан коммуникатор `MPI_Comm`, объединяющий *p* параллельных процессов, и один из процессов с рангом `root` содержит массив данных, состоящий из элементов некоторого типа. Требуется реализовать функцию `CustomScatter`, осуществляющую распределение данных от процесса `root` ко всем процессам коммуникатора таким образом, что каждому процессу передаётся непрерывный блок элементов одинакового размера.

Реализуемая функция должна иметь интерфейс, аналогичный стандартной функции `MPI_Scatter`, и использовать для обмена данными только базовые операции передачи сообщений `MPI_Send` и `MPI_Recv`.

### Входные данные:
- указатель `void* sendbuf` на массив исходных данных, расположенный на процессе `root`;
- целое число `int sendcount` — количество элементов, передаваемых каждому процессу;
- тип данных `MPI_Datatype sendtype` элементов массива;
- указатель `void* recvbuf` на буфер для приёма данных;
- целое число `int recvcount` — количество элементов, принимаемых каждым процессом;
- тип данных `MPI_Datatype recvtype` принимаемых элементов;
- целое число `int root` — ранг процесса-источника данных;
- коммуникатор `MPI_Comm`.

### Выходные данные:
- массив `void* recvbuf`, содержащий часть исходных данных, соответствующую рангу текущего процесса (возврат массива осуществляется посредством заполнения данных по переданному указателю).

## 3. Базовый алгоритм
Последовательная реализация `CostumScatter` является упрощённой версией, предназначенной для работы в однопроцессорном режиме. Основная цель этой реализации — предоставить базовый функционал для тестирования и сравнения производительности с параллельной версией, а также обеспечить работу в средах, где MPI недоступен или не требуется.

**Код реализации**

```cpp
bool PikhotskiyRScatterSEQ::RunImpl() {
  const auto& input_data = GetInput();
  const void* send_data_ptr = std::get<0>(input_data);
  int send_element_count = std::get<1>(input_data);
  MPI_Datatype send_datatype = std::get<2>(input_data);
  void* receive_data_ptr = std::get<3>(input_data);
  int receive_element_count = std::get<4>(input_data);
  
  size_t element_size = GetTypeSize(send_datatype);
  
  size_t total_copy_size = static_cast<size_t>(receive_element_count) * element_size;
  
  if (total_copy_size > 0) {
    if (send_data_ptr != nullptr) {
      std::memcpy(receive_data_ptr, send_data_ptr, total_copy_size);
    } else {
      std::memset(receive_data_ptr, 0, total_copy_size);
    }
  }
  
  GetOutput() = receive_data_ptr;
  return true;
}
```


## 4. Параллельный алгоритм

### 4.1 Алгоритм
В ходе работы была разработана простая и эффективная реализация функции `CustomScatter`, использующая линейный алгоритм рассылки данных. Основная идея алгоритма заключается в следующем:

1. **Корневой процесс**:
   - Копирует свою часть данных из общего буфера в локальный буфер приёма
   - Последовательно отправляет данные каждому из остальных процессов

2. **Не корневые процессы**:
   - Получают свои данные от корневого процесса

### 4.2 Код реализации
```cpp
int CustomScatterInt(const void* sendbuf, int sendcount, void* recvbuf, 
                     int recvcount, int root, MPI_Comm comm) {
    int rank, size;
    MPI_Comm_rank(comm, &rank);
    MPI_Comm_size(comm, &size);
    
    int* recv_data = static_cast<int*>(recvbuf);
    
    if (rank == root) {
        const int* send_data = static_cast<const int*>(sendbuf);
        
        // Копируем данные для корневого процесса
        std::memcpy(recv_data, send_data + rank * sendcount, 
                   sendcount * sizeof(int));
        
        // Отправляем данные остальным процессам
        for (int i = 0; i < size; ++i) {
            if (i != root) {
                MPI_Send(send_data + i * sendcount, sendcount, 
                         MPI_INT, i, 0, comm);
            }
        }
    } else {
        MPI_Recv(recv_data, recvcount, MPI_INT, root, 0, comm, 
                MPI_STATUS_IGNORE);
    }
    
    return MPI_SUCCESS;
}
```
*Поддержка трёх типов: Реализация поддерживает `MPI_INT`, `MPI_FLOAT` и `MPI_DOUBLE`.*

## 5. Условия проведения экспериментов
- **Аппаратное обеспечение/ОС:** Intel Core i5-10400f, 4 ядра, 16GB RAM, Ubuntu 24.04.2;
- **Инструменты сборки:** GCC 13.3.0, Release;
- **Данные:** Тестовые данные генерировались вручную.

## 6. Результаты
### 6.1 Функциональные тесты

Было разработано 4 функциональных теста для проверки корректности работы реализации:

    Тест с целыми числами (MPI_INT): Проверяется рассылка массива из 6 целых чисел [1, 2, 3, 4, 5, 6] на 3 процесса по 2 элемента на каждый процесс.

    Тест с числами с плавающей точкой одинарной точности (MPI_FLOAT): Проверяется рассылка массива из 6 чисел [1.1, 2.2, 3.3, 4.4, 5.5, 6.6] на 3 процесса по 2 элемента на процесс.

    Тест с числами с плавающей точкой двойной точности (MPI_DOUBLE): Проверяется рассылка массива из 6 чисел [1.11, 2.22, 3.33, 4.44, 5.55, 6.66] на 3 процесса по 2 элемента на процесс.

    Пустой тест: Проверяется корректность обработки случая с нулевым количеством данных.

Все функциональные тесты успешно пройдены как для последовательной (SEQ), так и для параллельной (MPI) версий реализации. Результаты работы собственной реализации совпадают с результатами стандартной функции MPI_Scatter.
### 6.2 Тесты производительности

Для оценки эффективности реализации проводились замеры времени выполнения на массиве из 1 миллиона целых чисел при различном количестве процессов. Были получены следующие результаты:
|Число процессов | Время собственной реализации, с |
|----------------|---------------------------------|
| 1              | 0.0010                          |
| 2	             | 0.0018                          |
| 4              | 0.0035                          |
| 8              | 0.0073                          | 

### 6.3 Анализ производительности

Результаты тестирования показывают, что время выполнения собственной реализации PikhotskiyMPIScatter линейно растёт с увеличением числа процессов. Это объясняется алгоритмической сложностью реализации:

 - Линейная сложность алгоритма: Реализованный алгоритм имеет временную сложность O(N), где N — количество процессов. Корневой процесс выполняет N-1 операций отправки данных, причём все эти операции выполняются последовательно.

 - Последовательные операции отправки: В текущей реализации корневой процесс отправляет данные каждому процессу в цикле, что создаёт следующие проблемы:
        1. Каждая операция MPI_Send имеет фиксированные накладные расходы на установление соединения.
        2. Отправки выполняются синхронно, процесс ожидает завершения каждой отправки перед началом следующей.
        3. Нет перекрытия операций, которые могли бы выполняться параллельно.

 - С ростом числа процессов увеличивается:
        1. Количество сетевых соединений
        2. Конкуренция за пропускную способность сети
        3. Время на синхронизацию процессов

## 7. Заключение

В ходе выполнения работы была реализована собственная версия функции рассылки данных PikhotskiyMPIScatter с использованием базовых функций MPI_Send и MPI_Recv. Реализация корректно выполняет рассылку данных различных типов (int, float, double) и проходит все функциональные тесты.

Анализ производительности показал, что реализованный алгоритм имеет линейную сложность O(N) и плохо масштабируется с ростом числа процессов. Время выполнения увеличивается пропорционально количеству процессов, что обусловлено последовательной отправкой данных от корневого процесса каждому получателю.

Полученные результаты демонстрируют фундаментальное различие между простой реализацией scatter и оптимизированной библиотечной версией. Стандартная реализация MPI_Scatter использует более сложные алгоритмы и оптимизации, специфичные для оборудования, что позволяет достичь лучшей масштабируемости.

## 8. Список литературы

1. Microsoft. MPI_Scatter function [Электронный ресурс]. – URL: https://learn.microsoft.com/ru-ru/message-passing-interface/mpi-scatter-function (дата обращения: 12.12.2025);