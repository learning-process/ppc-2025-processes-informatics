# Нахождение числа нарушений упорядоченности соседних элементов вектора
(последовательная и MPI-реализации)

Студентка: Горячева Ксения Александровна, группа 3823Б1ФИ2
Технологии: SEQ, MPI
Вариант: №6

---

## 1. Введение

В работе рассматривается задача анализа упорядоченности одномерного массива целых чисел.

Целью данной работы является разработка и исследование алгоритмов подсчёта количества нарушений порядка между соседними элементами вектора целых чисел, а также реализация параллельной версии алгоритма с использованием технологии MPI.

Нарушением порядка считается ситуация, при которой для двух соседних элементов выполняется условие:

a[i] > a[i+1]
---

## 2. Постановка задачи

Входные данные:
вектор целых чисел v длины n.

Выходные данные:
целое число — количество индексов i, для которых:

0 ≤ i < n − 1 и v[i] > v[i+1]

### Ограничения и допущения

* элементы вектора могут принимать любые значения типа int;
* для векторов длины 0 или 1 результат равен 0;
* результаты SEQ и MPI реализаций должны совпадать.

---

## 3. Последовательная реализация (SEQ)

Последовательный алгоритм выполняет линейный проход по вектору и сравнивает каждую пару соседних элементов.

Алгоритм:

result_ = 0;
for(size_t i = 0; i + 1 < input_vec_.size(); ++i){
    if (input_vec_[i] > input_vec_[i + 1]){
      ++result_;
    }
}

Сложность алгоритма:

* по времени: O(n)
* по памяти: O(1) (без учёта входных данных)

---

## 4. Параллельная реализация (MPI)

В MPI-версии входной вектор разбивается на непрерывные блоки элементов, которые распределяются между процессами.

Каждый процесс подсчитывает количество нарушений порядка внутри своего блока, после чего дополнительно учитывается возможное нарушение на границе соседних блоков.

### 4.1 Распределение данных

Пусть:

* n — размер вектора,
* p — количество MPI-процессов.

Размер локального блока для процесса rank вычисляется как:

base = n / p
rem  = n % p
local_size = base + (rank < rem ? 1 : 0)

Процесс с рангом 0 распределяет соответствующие части массива остальным процессам с помощью MPI_Send, а каждый процесс получает свой локальный подмассив.

---

### 4.2 Локальный подсчёт

Каждый процесс независимо подсчитывает количество нарушений порядка внутри своего блока:

for (int i = 0; i + 1 < local_size; ++i){ 
      if (local[i] > local[i+1]){
        ++local_count;
      }
    }
---

### 4.3 Учёт границ блоков

Так как пары элементов могут пересекать границу между блоками, необходимо дополнительно проверить соседние элементы между процессами.

Для этого используется функция MPI_Sendrecv, позволяющая:

* отправить последний элемент текущего блока следующему процессу;
* получить последний элемент предыдущего процесса.

Если для процесса с rank > 0 выполняется:

left_last > local.front()

то счётчик локальных нарушений увеличивается на 1.

---

### 4.4 Сбор результата

После завершения локальных вычислений используется:

* MPI_Reduce — для суммирования локальных результатов на процессе 0;
* MPI_Bcast — для рассылки итогового значения всем процессам.

---

## 5. Тестирование корректности

Для проверки корректности реализации был разработан набор функциональных тестов, включающий:

* отсортированные и обратно отсортированные векторы;
* массивы с повторяющимися значениями;
* чередующиеся значения;
* отрицательные числа;
* массивы длины 0, 1 и 2;
* различные комбинации локальных спадов и подъёмов.

Всего выполнено 76 функциональных тестов, все из которых успешно пройдены как для SEQ, так и для MPI версии.

---

## 6. Экспериментальное исследование производительности

Измерения проводились на векторе из 100 миллионов элементов с использованием 4 MPI-процессов.

### Результаты измерений

| Реализация | Процессы | Режим    | Время (с) |
| ---------- | -------- | -------- | --------- |
| SEQ        | 1        | task_run | 0.8988    |
| MPI        | 2        | task_run | 0.3964    |
| MPI        | 4        | task_run | 0.2755    |

### Расчёт ускорения и эффективности

Ускорение на двух процессах:

S = 0.8988 / 0.3964 ≈ 2.26

Эффективность на двух процессах:

E = 2.26 / 4 ≈ 0.57

Ускорение на четырёх процессах:

S = 0.8988 / 0.2755 ≈ 3.26

Эффективность на четырёх процессах:

E = 3.26 / 4 ≈ 0.82

---

## 7. Анализ результатов

Полученные результаты показывают, что при большом объёме входных данных параллельная реализация значительно превосходит последовательную по времени выполнения.

Высокая эффективность объясняется следующими факторами:

* линейная вычислительная сложность алгоритма;
* равномерное распределение данных между процессами;
* минимальный объём межпроцессного взаимодействия;
* использование MPI_Sendrecv, позволяющее корректно учитывать граничные элементы без сложных схем рассылки.

В отличие от вариантов с перекрывающимся распределением данных, применённая схема демонстрирует хорошую масштабируемость.

---

## 8. Заключение

В ходе выполнения работы были реализованы и исследованы последовательная и параллельная версии алгоритма подсчёта нарушений порядка соседних элементов вектора.

Основные результаты:

* подтверждена корректность алгоритма на широком наборе тестов;
* реализована эффективная MPI-версия с учётом граничных условий;
* достигнуто ускорение порядка 2.26 раз на 2 процессах;
* достигнуто ускорение порядка 3.26 раз на 4 процессах;
* показано, что для больших массивов MPI-реализация оправдана и эффективна.

---

## 9. Используемые источники

1.  Parallel Programming Course - [https://learning-process.github.io/parallel_programming_course/ru/](https://learning-process.github.io/parallel_programming_course/ru/)
2.  Parallel Programming 2025-2026 Video-Records - [https://disk.yandex.ru/d/NvHFyhOJCQU65w](https://disk.yandex.ru/d/NvHFyhOJCQU65w)
3.  Open MPI: Documentation — [https://www.open-mpi.org/doc/](https://www.open-mpi.org/doc/)

---