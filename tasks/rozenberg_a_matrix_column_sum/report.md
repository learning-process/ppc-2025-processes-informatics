# Сумма значений по столбцам матрицы

- Студент: Розенберг Алексей Светославович, группа 3823Б1ФИ2
- Технология: SEQ | MPI 
- Вариант: 12

## 1. Введение
В данной работе рассматривается задача вычисления суммы значений по столбцам матрицы.
Цель — реализовать последовательное и параллельное (MPI) решения, сравнить их производительность и исследовать влияние параллелизма на ускорение вычислений.

Подобные операции встречаются в обработке изображений, анализе данных, системах машинного обучения и численных методах, где матрицы могут быть большими, а время вычислений критичным. Параллелизация позволяет снизить время выполнения за счёт распределения нагрузки между несколькими процессами.

## 2. Постановка задачи
Дана прямоугольная матрица $A$ размера $N \times M$ с произвольными входными данными (целые числа).

Требуется вычислить одномерный массив $out$ длины $M$, такой, что
```math
out[j] = \sum_{i=0}^{N-1}A[i][j], j = 0...M-1
```
То есть необходимо вычислить сумму значений по каждому столбцу матрицы.

Результат должен иметь такой же порядок столбцов, как и входная матрица.

## 3. Описание алгоритма (Последовательный)
Последовательный алгоритм реализуется максимально просто и напрямую:

1. Проверить, что матрица не пуста.
2. Инициализировать выходной вектор `out` длины `M = number_of_columns`.
3. Для каждой строки `i` от `0` до `N-1`:
   - Для каждого столбца `j` от `0` до `M-1`:
```math
out[j] += A[i][j]
```
Пример реализации:
```cpp
if (A.empty()) {
    return false;
  }

  std::fill(out.begin(), out.end(), 0);

  for (size_t i = 0; i < N; i++) {
    for (size_t j = 0; j < M; j++) {
      out[j] += A[i][j];
    }
  }
```
Асимптотическая сложность: $O(N \times M)$

## 4. Схема распараллеливания
### MPI-вариант
Основная идея — распределить строки матрицы между MPI-процессами.

1. Разбиение данных

    - Матрица распределяется по строкам.

    - Каждый процесс получает приблизительно одинаковое количество строк.

2. Локальное вычисление

    - Каждый процесс вычисляет сумму значений по столбцам для своей части матрицы.

    - Локальный результат имеет размер `M`.

3. Коммуникация

    - Используется операция `MPI_Allreduce` для редуцирования всех процессов.

    - Процесс `rank 0` получает окончательный ответ.

4. Топология

    - Все процессы передают данные корневому процессу.

## 5. Детали имплементации
### Структура проекта
<pre>
rozenberg_a_matrix_column_sum
├───common
│   └───include
│           common.hpp
├───data
├───mpi
│   ├───include
│   │       ops_mpi.hpp
│   └───src
│           ops_mpi.cpp
├───seq
│   ├───include
│   │       ops_seq.hpp
│   └───src
│           ops_seq.cpp
└───tests
    ├───functional
    │       main.cpp
    └───performance
            main.cpp
</pre>

- Матрица представлена как `std::vector<std::vector<int>>`.

- Последовательная реализация работает напрямую с входными данными.

- MPI-вариант корректно обрабатывает случаи:

    - неравномерного количества строк,

    - небольших матриц (1×N, N×1).

- Входные данные не модифицируются.

- Память под результат выделяется заранее.

### Пограничные случаи

- Матрица из одной строки.

- Матрица из одного столбца.

- Матрица единичного размера.

## 6. Экспериментальное окружение
- Аппаратное обеспечение/ОС: AMD Ryzen 7 7800X3D, 8 cores/16 threads, RAM 32 GB 6000 MHz, Windows 11 Home
- Toolchain: MSVC v143, Release
- Настройки окружения: PPC_NUM_THREADS 16 / PPC_NUM_PROC 8
- Данные: матрица размером 4000x4000 элементов, элементы сгенерированы случайным образом

## 7. Экспериментальный результаты

### 7.1 Корректность
Корректность проверялась следующими методами:
- сравнение с эталонным результатом, хранящимся в файле;
- модульные тесты, проверяющие различные размеры матриц;
- проверка свойств:
    - сумма столбцов матрицы 1×M равна самой строке,
    - сумма нулевой матрицы равна нулевому вектору,
    - инвариант: результат сохраняет порядок столбцов.

Все проверки успешно пройдены.

### 7.2 Производительность
Ниже представлена таблица экспериментальных измерений:

| Mode        | Count | Time, s      | Speedup | Efficiency |
|-------------|-------|--------------|---------|------------|
| seq         | 1     | 0.00784644   | 1.00    | N/A        |
| mpi         | 2     | 0.00364688   | 2.15    | 107.5%     |
| mpi         | 4     | 0.00297700   | 2.64    | 66.0%      |
| mpi         | 8     | 0.00169680   | 4.62    | 57.8%      |
| mpi         | 16    | 0.00176476   | 4.45    | 27.8%      |

- Последовательный алгоритм показывает лучшую эффективность для матриц малого размера
- MPI обеспечивает отличное ускорение для матриц большого размера при использовании 2 процессов, однако эффективность падает с повышением их числа.
- Основной bottleneck - операция редукции в MPI, которая имеет большее влияние с ростом числа запущенных процессов

## 8. Выводы
В рамках работы была реализована задача вычисления суммы значений по столбцам матрицы в последовательном и MPI вариантах.

Экспериментально показано:

- последовательный алгоритм эффективен для малых матриц,

- MPI-вариант обеспечивает ускорение при больших данных,

- масштабируемость ограничивается стоимостью коммуникаций между узлами,

- распределение по строкам является эффективной схемой разбиения.

Решение корректно работает для произвольных матриц и может использоваться в качестве базового блока для более сложных алгоритмов.

## 9. Источники
1. [Microsoft MPI Reference](https://learn.microsoft.com/en-us/message-passing-interface/mpi-reference)
2. [C++ Reference](https://en.cppreference.com/)
3. [Python Documentation](https://docs.python.org/3/index.html)
4. [Документация по курсу «Параллельное программирование»](https://learning-process.github.io/parallel_programming_course/ru/index.html)
5. Курс лекций и практик по параллельному программированию, ННГУ
