# Максимальное значение элементов матрицы

- Студент: Пылаева Светлана Алексеевна, группа 3823Б1ФИ3
- Технология: SEQ, MPI
- Вариант: 13

## 1. Введение
Параллельные вычисления являются важным направлением в современном программировании, позволяющим значительно ускорить обработку больших объемов данных. В данной работе реализованы последовательная и параллельная версии алгоритма нахождения максимального значения элементов матрицы с использованием технологии MPI (Message Passing Interface). 

## 2. Постановка задачи
Дана матрица из n*m элементов (N строк, M столбцов). Необходимо найти максимальный элемент в матрице.  
Матрица хранится **по строкам** в виде одномерного массива (вектор `std::vector<int>`), таким образом индекс элемента *(i, j)* можно вычислить по формуле `index = i * M + j`.

### Входные данные: 
`InType = std::tuple<size_t, std::vector<int>>`  
- Размер матрицы (`size_t`)
- Вектор элементов матрицы (`std::vector<int>`)

### Выходные данные:
`OutType = int`
- Максимальное значение среди элементов матрицы (`int`)

## 3. Описание линейного алгоритма

```cpp
   int max_element = matrix_data[0];
   for (size_t i = 1; i < matrix_size; ++i) {
      max_element = std::max(matrix_data[i], max_element);
   }
``` 
Алгоритм поиска максимального значения в матрице основан на последовательном просмотре всех элементов с сохранением текущего максимального значения.

1. Инициализировать переменную `max_element` минимальным значением типа `int`
2. Для каждого элемента матрицы:
   - Сравнить текущий элемент с `max_element`
   - Если текущий элемент больше, обновить `max_element`

## 4. Описание схемы параллельного алгоритма

Параллельная версия использует распределение строк матрицы между MPI-процессами

1. **Распределение данных**: Для каждого MPI-процесса рассчитываются `start` и `end` индексы начала блока в матрице для обработки. Распределение учитывает возможный остаток при неравномерном делении размера матрицы на количество процессов

2. **Локальная обработка**: Каждый процесс независимо находит максимальный элемент в своем блоке. В результате каждый процесс формирует локальное значение максимума `local_max`  
```cpp
  int local_max = std::numeric_limits<int>::min();
  for (int i = start; i < end; i++) {
    local_max = std::max(local_max, matrix_data[i]);
  }
```

3. **Глобальная редукция**: Все процессы участвуют в операции `MPI_Allreduce`, которая одновременно выполняет редукцию по всем процессам с использованием операции `MPI_MAX` и распространяет результат глобального максимума на все процессы


## 5. Детали реализации

**Файловая структура:**

pylaeva_s_max_elem_matrix/  
├── common/include/common.hpp  
├── data/  
├── mpi/include/ops_mpi.hpp  
├── mpi/src/ops_mpi.cpp  
├── seq/include/ops_seq.hpp  
├── seq/src/ops_seq.cpp  
├── tests/functional/main.cpp  
└── tests/performance/main.cpp  

**Ключевые классы и файлы:**

1. Последовательная реализация (`seq`):
   - `ops_seq.hpp` - объявление класса `PylaevaSMaxElemMatrixSEQ`
   - `ops_seq.cpp` - реализация методов

2. MPI реализация (`mpi`):
   - `ops_mpi.hpp` - объявление класса `PylaevaSMaxElemMatrixMPI`
   - `ops_mpi.cpp` - реализация методов

3. Общие компоненты (`common`):
   - `common.hpp` - общие типы данных и константы

4. Тестовые данные (`data`)
   - `matrix_3x3.txt` - квадратная матрица 3x3 содержит 1 максимальный элемент (100) в начале
   - `matrix_5x5.txt` - квадратная матрица 5x5 содержит 1 максимальный элемент (500) в середине
   - `matrix_11x11.txt` - квадратная матрица 11x11 содержит 1 максимальный элемент (21) в конце
   - `matrix_50x50.txt` - квадратная матрица 50x50 содержит несколько максимальных элементов (-10)
   - `matrix_100x100.txt` - квадратная матрица 100x100 содержит несколько максимальных элементов (2048)
   - `matrix_150x100.txt` - квадратная матрица 150x100 содержит 1 максимальный элемент (-100) в конце
   - `matrix_200x200.txt` - квадратная матрица 200x200 содержит 1 максимальный элемент (0) в конце
   - `matrix_500x1000.txt` - прямоугольная матрица 500x1000 содержит несколько максимальных элементов
   - `matrix_1000x1000.txt` - квадратная матрица 1000x1000 содержит несколько максимальных элементов 
   - `matrix_1500x1000.txt` - прямоугольная матрица 1500x1000 содержит 1 максимальный элемент () в начале


## 6. Экспериментальная установка
- CPU: Intel Core i5-10210U @ 1.60GHz (4 cores, 8 threads)
- RAM: 16 GB
- OS: Windows 10 version 22H2
- Компилятор: MinGW-w64 GCC, C++20, тип сборки Release
- MPI: Microsoft MPI 10.1.12498.52
- CMake: 3.30.3
- Данные:  
    1. **Func_tests** данные сгенерированы в текстовых файлах. 
    - Название: `matrix_NxM.txt`
    - Формат хранения: `N` (число строк) `M` (число столбцов) `expected_max` (ожидаемое максимальное значение) далее `elem` элементы матрицы в количестве N*M
    2. **Perf_tests**  `matrix_data` = {7000}, `matrix_data[i]` = i, `max` = 48999999


### Управление процессами

PPC_NUM_PROC: устанавливается через параметр -np в mpiexec

```cpp
//Запуск с различным количеством процессов MPI
mpiexec -np 1 ./build/bin/ppc_perf_tests --gtest_filter="*MaxElemMatrix*"
mpiexec -np 2 ./build/bin/ppc_perf_tests --gtest_filter="*MaxElemMatrix*"
mpiexec -np 4 ./build/bin/ppc_perf_tests --gtest_filter="*MaxElemMatrix*"
```

## 7. Результаты экспериментов и выводы

Ниже приведены таблицы с результатами Perf тестов для матрицы на 100_000_000 (10000*10000) элементов.

*результаты были получены при запуске build/bin/ppc_perf_tests на 1, 2 и 4 процессах*

| Mode        | Count | Time, s | Speedup |
|-------------|-------|---------|---------|
| seq         | 1     | 0.078   | 1.00    |
| omp         | 2     | 0.360   | 0.22    |
| omp         | 4     | 0.337   | 0.24    |

**Анализ результатов:** 
- Полученные результаты показывают замедление MPI-версии относительно последовательной реализации.    

**Возможные причины замедления:**  
- Время коммуникации между процессами превышает время вычислений
- Операция сравнения имеет очень низкую вычислительную сложность
- Накладные расходы MPI не компенсируются распараллеливанием

## 8. Заключение
В ходе работы были реализованы последовательная и параллельная MPI-версии алгоритма поиска максимального элемента в матрице. 

**Основные выводы:**

1. **Эффективность параллелизации**: Для задачи поиска максимума параллельная реализация на MPI не показала ожидаемого ускорения.

2. **Возможные причины низкой производительности MPI**:
   - Высокие накладные расходы, т.к. при использовании MPI_Allreduce все процессы должны синхронизироваться
   - Маленький объем вычислений на процесс: Для операции поиска максимума слишком мало вычислений на один элемент, чтобы компенсировать затраты на коммуникацию.

## Источники
1. [Документация OpenMPI](https://www.open-mpi.org/doc/)
2. Сысоев А. В. Курс лекций по параллельному программированию
