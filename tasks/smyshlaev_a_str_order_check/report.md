# <Проверка лексикографической упорядоченности двух строк>

- Student: Смышляев Александр Павлович, group 3823Б1ФИ2
- Technology: SEQ | MPI
- Variant: 26

## 1. Introduction
**Мотивация:** Исследовать эффективность распараллеливания задачи сравнения строк через MPI, которая, в отличие от простых арифметических операций, имеет переменную вычислительную сложность.

**Проблема:** Производительность алгоритма сильно зависит от входных данных. В "лучшем случае" (различие в начале строк) затраты на коммуникацию могут превысить выгоду, в то время как в "худшем" (различие в конце) распараллеливание должно быть эффективным.

**Ожидаемый результат:** MPI-версия покажет значительное ускорение по сравнению с последовательной в "худшем случае", но эффективность будет снижаться с ростом числа процессов из-за накладных расходов.
## 2. Problem Statement
На вход поступают две строки произвольного размера. Задача — определить их лексикографический порядок.

## 3. Baseline Algorithm (Sequential)
Базовый (последовательный) алгоритм посимвольно сравнивает строки до первого различия. Если общая часть идентична (одна строка является префиксом другой), результат определяется сравнением длин строк.

## 4. Parallelization Scheme
1.  **Распределение работы:** 0-й процесс получает входные данные и рассылает длины строк всем процессам через MPI_Bcast. Затем вычисляются смещения (offsets) и размеры порций (counts). С помощью операции MPI_Scatterv части строк рассылаются по процессам. Таким образом, память расходуется экономно, и каждый процесс хранит только свою часть данных.
2.  **Локальное сравнение:** Каждый процесс выполняет сравнение символов только в своем диапазоне. Если различие найдено, он сохраняет локальный результат (`-1` или `1`), иначе — `0`.
3.  **Сбор результатов:** С помощью операции `MPI_Allgather` каждый процесс отправляет свой локальный результат всем остальным. В итоге каждый процесс получает полный массив результатов от всех участников.
4.  **Финализация:** Каждый процесс анализирует полученный массив. Итоговый результат — это первый ненулевой элемент в этом массиве (так как массив упорядочен по рангам, это гарантирует нахождение самого первого различия в строках). Если все результаты нулевые, итоговый ответ определяется сравнением длин строк.

## 5. Experimental Setup
- **Hardware/OS:** `Intel Core i7-1255U` (10 ядер, 12 потоков), `16GB RAM`, `Windows 11`
- **Toolchain:** `MSVC v19.38.33130 (Visual Studio 2022)`, `MS-MPI`, `Release`
- **Environment:** `PPC_NUM_PROC`
- **Data:** Две строки на 20'000'000 элементов, различающиеся в последнем символе ("худший случай" для вычислений).

## 6. Results and Discussion

### 6.1 Correctness
Корректность реализаций проверена функциональными тестами (GTest). Тесты покрывают все основные сценарии: равные строки, пустые строки, строки-префиксы и строки с различиями в разных позициях.

### 6.2 Performance
Результаты исследования производительности для различного числа процессов:

| Mode        | Count | Time, s | Speedup | Efficiency |
|-------------|-------|---------|---------|------------|
| seq         | 1     |  0.0243 |   1.00  | N/A        |
| mpi         | 2     |  0.0149 |   1.64  | 82.0%      |
| mpi         | 4     |  0.014  |   1.74  | 44.0%      |
| mpi         | 8     | 0.0153  |   1.60  | 20.0%      |
Результаты показывают, что алгоритм эффективно масштабируется при переходе от 1 к 2 процессам (ускорение 1.64, эффективность 82%).

Однако при дальнейшем увеличении числа процессов (N=4) прирост производительности становится незначительным (ускорение выросло лишь до 1.74), а эффективность резко падает.

На 8 процессах наблюдается деградация производительности: время выполнения (0.01527 с) оказалось больше, чем на 4 процессах (0.01401 с). Это объясняется тем, что абсолютное время решения задачи очень мало (порядка 15-20 мс). В таких условиях накладные расходы на инициализацию коммуникаций, пересылку данных (MPI_Scatterv) и синхронизацию потоков начинают превышать выгоду от распараллеливания вычислений.
## 7. Conclusions
Реализация распределенного сравнения строк с использованием MPI показала свою высокую эффективность для сценариев с большой вычислительной нагрузкой. Использование MPI_Scatterv позволило выполнить эффективную декомпозицию данных, передав каждому процессу только необходимый фрагмент строки для независимой обработки. Однако, накладные расходы на пересылку данных и синхронизацию (Scatterv + Allgather) при большом числе процессов (8) начинают превышать выигрыш от параллельных вычислений, что видно по падению эффективности. В целом, MPI-подход является оправданным и эффективным решением для сравнения очень больших, преимущественно схожих строк.

## 8. References
1.  Лекции по параллельному программированию ННГУ
2.  Стандарт MPI
