# Умножение плотных матриц. Элементы типа double. Блочная схема, алгоритм Фокса

- Студент: Никитин Антон Александрович, группа 3823Б1ФИ1
- Технология: SEQ | MPI 
- Вариант: 2

## 1. Введение
Распараллеливание умножения матриц необходимо для обработки больших данных в научных вычислениях и машинном обучении. Алгоритм Фокса решает проблему эффективного распределения вычислений между процессами через блочную декомпозицию. Ожидается разработка MPI-реализации с оценкой ускорения и эффективности при различном количестве процессов.

## 2. Постановка задачи
**Задача:**: Умножение плотных матриц. Элементы типа double. Блочная схема, алгоритм Фокса. Алгоритм реализует блочное умножение матриц с использованием циклического сдвига блоков для минимизации коммуникационных затрат в распределенных системах.
**Формат ввода-вывода**:
   - **Вход**: Две квадратные матрицы A и B размера N×N, где элементы имеют тип double
   - **Выход**: Результирующая матрица C = A × B того же размера N×N
**Ограничения**:
   - Матрицы должны быть квадратными одинакового размера
   - Размер матрицы должен быть положительным целым числом
   - Элементы матриц - числа с плавающей точкой двойной точности
   - Реализация поддерживает как последовательное, так и параллельное выполнение

## 3. Базовый алгоритм (последовательный)

Алгоритм Фокса в последовательной реализации использует блочное разбиение матриц для улучшения использования кэша процессора. Основные шаги алгоритма:

**Разбиение матриц**: Исходные квадратные матрицы A и B разбиваются на блоки одинакового размера. Размер блока выбирается оптимальным для кэша процессора (обычно 64×64 элементов).

**Циклическое умножение блоков**: Для каждой итерации алгоритма:
   - Выбирается "активный" блок матрицы A в соответствии с номером итерации и текущей позицией
   - Блоки матрицы A циклически сдвигаются по строкам на каждой итерации
   - Каждый блок матрицы A умножается на соответствующие блоки матрицы B

**Накопительное суммирование**: Результаты умножения блоков накапливаются в соответствующих позициях результирующей матрицы C.

**Завершение**: После выполнения всех итераций, равных количеству блоков по горизонтали/вертикали, получается полная матрица произведения.

Алгоритм минимизирует промахи кэша за счет работы с блоками данных, которые помещаются в кэш-память процессора, что значительно ускоряет вычисления по сравнению с наивным тройным вложенным циклом.

## 4. Схема распараллеливания

### 4.1 Распределение данных
- Матрица A распределяется по строкам между процессами MPI
- Матрица B полностью копируется на все процессы (широковещательная рассылка)
- Каждый процесс получает свою часть матрицы A и полную матрицу B

### 4.2 Схема связи и топология
- Используется линейная топология (1D-декомпозиция)
- Процесс 0 (root) выполняет роль координатора:
  - Инициирует распределение матриц
  - Собирает результаты от всех процессов
  - Рассылает финальный результат всем процессам
- Остальные процессы выполняют локальные вычисления и отправляют результаты процессу 0

### 4.3 Алгоритм выполнения

**Процесс 0:**
1. Проверяет корректность входных данных
2. Рассылает размер матрицы всем процессам
3. Рассылает матрицу B всем процессам  
4. Распределяет строки матрицы A по процессам

**Все процессы:**
1. Получают матрицу B
2. Получают свою часть матрицы A
3. Выполняют локальное умножение своей части A на всю B
4. Отправляют результаты процессу 0

**Процесс 0:**
1. Собирает результаты от всех процессов
2. Формирует полную результирующую матрицу
3. Рассылает результат всем процессам


### 4.4 Ранжирование ролей
- **Процесс 0 (ранг 0)**: Координатор, распределяет данные, собирает результаты
- **Процессы 1..(P-1)**: Вычислительные узлы, выполняют локальные умножения
- Все процессы в конце получают полный результат для возможной последующей обработки

## 5. Детали реализации

### 5.1 Структура кода
nikitin_a_fox_algorithm/
├── common/
│   └── include/
│       └── common.hpp          # Общие типы данных и определения
├── mpi/
│   ├── include/
│   │   └── ops_mpi.hpp         # Интерфейс MPI-реализации
│   └── src/
│       └── ops_mpi.cpp         # Реализация MPI-алгоритма
├── seq/
│   ├── include/
│   │   └── ops_seq.hpp         # Интерфейс последовательной реализации
│   └── src/
│       └── ops_seq.cpp         # Последовательная реализация
└── tests/
    ├── functional/
    │   └── main.cpp            # Функциональные тесты
    └── performance/
        └── main.cpp            # Тесты производительности


**Ключевые классы:**
- `NikitinAFoxAlgorithmMPI` - MPI-реализация алгоритма
- `NikitinAFoxAlgorithmSEQ` - последовательная реализация алгоритма

**Основные функции:**
- `ValidationImpl()` - проверка входных данных
- `RunImpl()` - основная логика выполнения
- Вспомогательные методы для распределения матриц и вычислений

### 5.2 Важные допущения и основные случаи
- Матрицы предполагаются квадратными и плотными
- Размер матрицы должен быть положительным
- Поддерживаются специальные значения (NaN, Inf) в тестах
- Реализация обрабатывает случай, когда размер матрицы не делится нацело на количество процессов

### 5.3 Рекомендации по использованию памяти
- Используется поэлементное копирование для минимизации накладных расходов
- Локальные буферы освобождаются после использования
- Для больших матриц рекомендуется использовать 64-битную сборку
- MPI-реализация дублирует матрицу B на всех процессах, что требует дополнительной памяти


## 6. Экспериментальная установка
- **Оборудование/ОС**: Windows 11 , Intel Core i7-12700H (4 ядра, 8 потоков), 8 ГБ ОЗУ
- **Инструментарий**: MSVC 19.41.34120, CMake 3.28.1, сборка Release
- **Окружение**: Количество процессов задаётся через `mpiexec -n N` (2, 4, 8 процессов)
- **Данные**: Генерация в тестах

## 7. Результаты и обсуждение

### 7.1 Корректность

Корректность реализации проверялась следующими способами:

**Функциональные тесты**: 30 тестовых случаев включают:
   - Матрицы различных размеров (от 1×1 до 100×100)
   - Специальные матрицы: нулевые, единичные, диагональные
   - Матрицы с граничными значениями (NaN, Inf, очень большие/малые числа)
   - Случайные матрицы с различными распределениями

**Сравнение с эталонным результатом**: 
   - Последовательная реализация алгоритма Фокса сравнивается с наивным умножением матриц
   - MPI-реализация сравнивается с последовательной реализацией
   - Допустимая погрешность: относительная ошибка ≤ 1e-8

**Инварианты и свойства**:
   - Проверка сохранения размерности результата
   - Тестирование свойств умножения (ассоциативность, дистрибутивность)
   - Обработка особых случаев: нулевая матрица × любая матрица = нулевая матрица

**Автоматизированная проверка**: Все тесты запускаются автоматически в CI/CD пайплайне при каждом коммите.

### 7.2 Производительность
Текущее время, ускорение и эффективность. Таблица примеров:

| Режим | Количество | Время, с | Ускорение | Эффективность |
|-------|------------|----------|-----------|---------------|
| seq   | 1          | 2.987    | 1.00      | N/A           |
| mpi   | 1          | 1.523    | 1.96      | 196.0%        |
| mpi   | 2          | 0.959    | 3.12      | 156.0%        |
| mpi   | 4          | 0.677    | 4.41      | 110.3%        |
| mpi   | 8          | 0.647    | 4.62      | 57.8%         |

## 8. Выводы

Реализация алгоритма Фокса с использованием MPI демонстрирует значительное ускорение по сравнению с последовательной версией. При 8 процессах достигается ускорение в 4.62 раза при умножении матриц. Максимальная эффективность наблюдается при 2 процессах (156%), что связано с оптимальным балансом между вычислениями и коммуникациями. Ускорение >100% на 1 процессе MPI связано с лучшей организацией вычислений в MPI-реализации по сравнению с последовательной

Основным ограничением является рост накладных расходов на коммуникацию при увеличении числа процессов, что снижает эффективность при 8 процессах до 57.8%. Также реализация требует полного хранения матрицы B на каждом процессе, что ограничивает максимальный размер обрабатываемых матриц доступной памятью. 

Алгоритм эффективен для средних и больших матриц, где вычислительная нагрузка преобладает над коммуникационными затратами.

## 9. Список литературы
1. [Открытая документация MPI](https://www.open-mpi.org/doc/) 
2. Лекции и практические занятия по предемту "Параллельное программирование" ННГУ
3. [cppreference.com](https://en.cppreference.com/)

