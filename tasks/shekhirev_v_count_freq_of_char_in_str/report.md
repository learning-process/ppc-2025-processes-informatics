# Отчет по лабораторной работе №1
## "Подсчет частоты символа в строке"

**Студент:** Шехирев Владислав Эдуардович
**Группа:** 3823Б1ФИ3
**Технология:** SEQ, MPI
**Вариант:** 23

### 1. Введение
**Мотивация:** Обработка строковых данных — частая задача в анализе данных (например, анализ ДНК, логов, текстов). Реализация поиска символа с помощью MPI позволяет изучить накладные расходы при распараллеливании задач с низкой вычислительной сложностью.

**Проблема:** Операция сравнения символов (`char == char`) выполняется процессором экстремально быстро. Основным ограничением при распараллеливании становится не скорость процессора, а пропускная способность памяти (Memory Bandwidth) и накладные расходы на передачу данных между процессами.

**Ожидаемый результат:** Ожидается, что на малых и средних объемах данных последовательная версия может работать быстрее параллельной из-за накладных расходов на коммуникацию (рассылку строки). Ускорение возможно только на экстремально больших данных.

### 2. Постановка задачи
**Задано:** Строка `str` произвольной длины и символ `target`.
**Требуется:** Определить количество вхождений символа `target` в строку `str`.

**Входные данные:** `std::string str`, `char target`.
**Выходные данные:** `int count` (количество вхождений).

### 3. Базовый алгоритм (Последовательный)
Последовательный алгоритм выполняет линейный проход по строке. Сложность алгоритма — `O(N)`.

Код ключевой функции:
```cpp
bool CharFreqSequential::RunImpl() {
  const auto &str = GetInput().str;
  char target = GetInput().target;

  int count = 0;
  for (char c : str) {
    if (c == target) {
      count++;
    }
  }

  GetOutput() = count;
  return true;
}
```

### 4. Схема распараллеливания
**Декомпозиция:** Использована геометрическая декомпозиция (блочное разбиение).
1.  **Рассылка данных:** Корневой процесс (rank 0) рассылает длину строки и содержимое строки всем процессам с помощью `MPI_Bcast`.
2.  **Вычисление границ:** Каждый процесс вычисляет свой диапазон индексов `[start_index, end_index)` на основе своего ранга и общего количества процессов. Обработка остатка от деления реализована равномерно (первые процессы берут на 1 элемент больше).
3.  **Локальный подсчет:** Процесс считает вхождения символа только в своем диапазоне.
4.  **Агрегация:** Результаты суммируются на корневом процессе с помощью `MPI_Reduce` (операция `MPI_SUM`).

### 5. Experimental Setup
*   **Hardware:** Intel Core i5, выделeно 8GB RAM.
*   **Environment:** Docker Container (Ubuntu 22.04) running on Windows 11 (WSL 2).
*   **Toolchain:** GCC 14.2.0, OpenMPI 4.1.x.
*   **Data:** Строка длиной 10,000,000 символов (10 MB).

### 6. Результаты
#### 6.1 Корректность
Корректность алгоритмов проверена функциональными тестами (Google Test).
Покрыты сценарии:
*   Обычная строка.
*   Отсутствие искомого символа.
*   Пустая строка.
*   Строка, полностью состоящая из искомых символов.
*   Искомый символ на границах диапазонов процессов.

Все тесты пройдены успешно.

Вот обновленные секции отчета с твоими **реальными** данными.

Обрати внимание:
1.  Размер данных теперь **50,000,000**.
2.  Время взято точно из твоих логов (строки `task_run`).
3.  **MPI (4) (0.011s)** работает в 2 раза быстрее, чем **MPI (1) (0.023s)** — это значит, распараллеливание работает! Но оно все равно не догоняет "чистый" **SEQ (0.004s)** из-за тяжелой операции `MPI_Bcast` (рассылка 50 МБ памяти).

Скопируй и замени эти секции в `report.md`.

### 5. Experimental Setup
*   **Hardware:** Intel Core i5, выделeно 8GB RAM.
*   **Environment:** Docker Container (Ubuntu 22.04) running on Windows 11 (WSL 2).
*   **Toolchain:** GCC 14.2.0, OpenMPI 4.1.x.
*   **Data:** Строка длиной **50,000,000 символов (50 MB)**.

### 6. Результаты
#### 6.1 Корректность
Корректность алгоритмов проверена функциональными тестами (Google Test).
Покрыты сценарии:
*   Обычная строка.
*   Отсутствие искомого символа.
*   Пустая строка.
*   Строка, полностью состоящая из искомых символов.
*   Искомый символ на границах диапазонов процессов.

Все тесты пройдены успешно.

#### 6.2 Производительность
Замеры проводились с использованием `ppc_perf_tests`.

| Mode | Count | Time, s | Speedup | Efficiency |
| :--- | :--- | :--- | :--- | :--- |
| seq | 1 | 0.00451 | 1.00 | 100% |
| mpi | 1 | 0.02360 | 0.19 | 19% |
| mpi | 4 | 0.01115 | 0.40 | 10% |

*   **seq (1)** — последовательная версия.
*   **mpi (1)** — параллельная версия на 1 процессе.
*   **mpi (4)** — параллельная версия на 4 процессах.

**Speedup** рассчитан как $T_{seq} / T_{mpi}$.

### 7. Выводы
По результатам эксперимента видно, что параллельная реализация на 4-х процессах работает **быстрее**, чем на одном процессе MPI (0.011s против 0.023s), что подтверждает работоспособность распараллеливания.

Однако, по сравнению с чистой последовательной версией, наблюдается замедление (Speedup = 0.40).

**Причины:**
1.  **Низкая вычислительная сложность:** Время обработки 50 млн символов одним ядром составляет всего ~4.5 мс.
2.  **Накладные расходы MPI:** Время, затрачиваемое на `MPI_Bcast` (рассылку 50 МБ данных четырем процессам), превышает время самого поиска. Копирование памяти между процессами занимает больше времени, чем проход по массиву.
3.  **Memory Bound:** Задача упирается в пропускную способность памяти.

**Заключение:**
Параллелизация данной задачи методом дублирования данных (`MPI_Bcast`) неэффективна для ускорения относительно последовательного кода на общих объемах памяти. Она демонстрирует внутреннее ускорение (MPI 4 vs MPI 1), но проигрывает SEQ из-за накладных расходов на инициализацию и передачу данных.

### 8. Литература
1.  Стандарт MPI 3.1.
2.  Лекции по курсу "Параллельное программирование".


