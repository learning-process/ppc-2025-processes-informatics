    
# Отчет по лабораторной работе
## "Подсчет частоты символа в строке"

- **Студент:** Шехирев Владислав
- **Группа:** 3823Б1ФИ3
- **Технология:** SEQ, MPI
- **Вариант:** 23

  

### 1. Введение
**Мотивация:** Обработка больших объемов текстовых данных — востребованная задача в таких областях, как биоинформатика (анализ генома), обработка логов и поиск паттернов. Распараллеливание этой задачи позволяет сократить время обработки за счет одновременного сканирования разных частей массива.

**Проблема:** Операция сравнения символов является элементарной и выполняется процессором очень быстро. Основным "узким местом" при распараллеливании становится пропускная способность памяти (Memory Bandwidth) и накладные расходы на передачу данных между процессами.

**Ожидаемый результат:** Ожидается получение ускорения на больших объемах данных при условии минимизации накладных расходов на коммуникацию (пересылку данных) между процессами.

### 2. Постановка задачи
**Задано:** Строка `str` произвольной длины и искомый символ `target`.
**Требуется:** Определить количество вхождений символа `target` в строку `str`.

**Входные данные:** `std::string str`, `char target`.
**Выходные данные:** `int count` (число вхождений).

### 3. Базовый алгоритм (Последовательный)
Последовательный алгоритм выполняет линейный проход по строке.
Сложность алгоритма: `O(N)`.

Реализация основана на стандартном алгоритме `std::count`, который часто оптимизирован компилятором с использованием векторных инструкций (SIMD).

```cpp
int count = std::count(str.begin(), str.end(), target);
```

### 4. Схема распараллеливания
Используется **геометрическая декомпозиция данных** (блочное разбиение). Входная строка логически разделяется на непрерывные блоки, и каждый процесс обрабатывает свой диапазон индексов.

**Алгоритм MPI-версии:**
1.  **Инициализация:** Каждый процесс получает доступ к входным данным.
2.  **Расчет границ:** Каждый процесс (`rank`) вычисляет свой диапазон индексов `[start_index, end_index)` на основе общего размера строки и количества процессов.
    *   Если длина строки `N` не делится нацело на число процессов `P`, остаток равномерно распределяется между первыми `N % P` процессами.
3.  **Локальные вычисления:** Процесс выполняет подсчет символов в своем диапазоне, используя оптимизированную функцию `std::count`.
    *   *Оптимизация:* Явная пересылка подстрок (`MPI_Scatter`) исключена для устранения накладных расходов на копирование памяти, так как в рамках SMP-системы (Shared Memory) эффективнее читать данные по индексам.
4.  **Агрегация:** Локальные результаты суммируются в глобальный результат на корневом процессе с помощью коллективной операции `MPI_Reduce` (флаг `MPI_SUM`).

### 5. Экспериментальная установка
*   **Hardware:** Intel Core i5, 8GB RAM.
*   **Environment:** Docker Container (Ubuntu 22.04) running on Windows 11 (WSL 2).
*   **Toolchain:** GCC 14.2.0, OpenMPI 4.1.x.
*   **Data:** Строка длиной **200,000,000 символов**.

### 6. Результаты
#### 6.1 Корректность
Корректность алгоритмов проверена набором функциональных тестов (Google Test).
Протестированы сценарии:
*   Типичная строка с искомыми символами.
*   Отсутствие искомого символа.
*   Пустая строка.
*   Строка, состоящая только из искомых символов.
*   Граничные случаи (символ в начале/конце блока).
*   Длинная строка (проверка корректности обработки остатка от деления).

Все тесты пройдены успешно.

#### 6.2 Производительность
Замеры проводились с использованием `ppc_perf_tests` на объеме данных 200 млн элементов.

| Mode | Count | Time, s | Speedup | Efficiency |
| :--- | :--- | :--- | :--- | :--- |
| seq | 1 | 0.03544 | 1.00 | 100% |
| mpi | 2 | 0.03658 | 0.97 | 48.5% |
| mpi | 4 | 0.01490 | **2.38** | 59.5% |

*   **Speedup (Ускорение):** $T_{seq} / T_{mpi}$
*   **Efficiency (Эффективность):** $Speedup / Count$

### 7. Выводы
В ходе лабораторной работы удалось достичь **существенного ускорения (в 2.38 раза)** параллельной версии относительно последовательной.

**Анализ результатов:**
1.  **Успешное распараллеливание:** Распределение вычислительной нагрузки на 4 процесса позволило сократить время обработки с 35 мс до 15 мс.
2.  **Роль оптимизации памяти:** Ключевым фактором успеха стал отказ от явной пересылки данных (`MPI_Bcast`/`Scatter`) внутри узла. Для задач класса Memory Bound (ограниченных скоростью памяти) лишнее копирование данных убивает производительность. Прямой доступ к данным по индексам позволил избежать этого узкого места.
3.  **Векторизация:** Использование `std::count` вместо ручного цикла позволило задействовать SIMD-инструкции процессора, обеспечив высокую скорость как последовательной, так и параллельной версии.

**Заключение:**
Задача подсчета символов эффективно распараллеливается с помощью MPI при условии минимизации коммуникационных расходов и работы с большими объемами данных (сотни мегабайт), достаточными для того, чтобы загрузить вычислительные ядра работой.

### 8. Литература
1.  Message Passing Interface (MPI) Forum. MPI: A Message-Passing Interface Standard.
2.  Лекционные материалы курса "Параллельное программирование для кластерных систем".
3.  Документация OpenMPI: [https://www.open-mpi.org/](https://www.open-mpi.org/)



