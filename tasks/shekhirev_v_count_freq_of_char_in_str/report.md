# Отчет по лабораторной работе №1
## "Подсчет частоты символа в строке"

**Студент:** Шехирев Владислав Эдуардович
**Группа:** 3823Б1ФИ3
**Технология:** SEQ, MPI
**Вариант:** 23

### 1. Введение
**Мотивация:** Обработка строковых данных — частая задача в анализе данных (например, анализ ДНК, логов, текстов). Реализация поиска символа с помощью MPI позволяет изучить накладные расходы при распараллеливании задач с низкой вычислительной сложностью.

**Проблема:** Операция сравнения символов (`char == char`) выполняется процессором экстремально быстро. Основным ограничением при распараллеливании становится не скорость процессора, а пропускная способность памяти (Memory Bandwidth) и накладные расходы на передачу данных между процессами.

**Ожидаемый результат:** Ожидается, что на малых и средних объемах данных последовательная версия может работать быстрее параллельной из-за накладных расходов на коммуникацию (рассылку строки). Ускорение возможно только на экстремально больших данных.

### 2. Постановка задачи
**Задано:** Строка `str` произвольной длины и символ `target`.
**Требуется:** Определить количество вхождений символа `target` в строку `str`.

**Входные данные:** `std::string str`, `char target`.
**Выходные данные:** `int count` (количество вхождений).

### 3. Базовый алгоритм (Последовательный)
Последовательный алгоритм выполняет линейный проход по строке. Сложность алгоритма — `O(N)`.

Код ключевой функции:
```cpp
bool CharFreqSequential::RunImpl() {
  const auto &str = GetInput().str;
  char target = GetInput().target;

  int count = 0;
  for (char c : str) {
    if (c == target) {
      count++;
    }
  }

  GetOutput() = count;
  return true;
}
```

### 4. Схема распараллеливания
**Декомпозиция:** Использована геометрическая декомпозиция (блочное разбиение).
1.  **Рассылка данных:** Корневой процесс (rank 0) рассылает длину строки и содержимое строки всем процессам с помощью `MPI_Bcast`.
2.  **Вычисление границ:** Каждый процесс вычисляет свой диапазон индексов `[start_index, end_index)` на основе своего ранга и общего количества процессов. Обработка остатка от деления реализована равномерно (первые процессы берут на 1 элемент больше).
3.  **Локальный подсчет:** Процесс считает вхождения символа только в своем диапазоне.
4.  **Агрегация:** Результаты суммируются на корневом процессе с помощью `MPI_Reduce` (операция `MPI_SUM`).

### 5. Experimental Setup
*   **Hardware:** Intel Core i5, выделeно 8GB RAM.
*   **Environment:** Docker Container (Ubuntu 22.04) running on Windows 11 (WSL 2).
*   **Toolchain:** GCC 14.2.0, OpenMPI 4.1.x.
*   **Data:** Строка длиной 10,000,000 символов (10 MB).

### 6. Результаты
#### 6.1 Корректность
Корректность алгоритмов проверена функциональными тестами (Google Test).
Покрыты сценарии:
*   Обычная строка.
*   Отсутствие искомого символа.
*   Пустая строка.
*   Строка, полностью состоящая из искомых символов.
*   Искомый символ на границах диапазонов процессов.

Все тесты пройдены успешно.

#### 6.2 Производительность
Замеры проводились с использованием `ppc_perf_tests`.

| Mode | Count | Time, s | Speedup | Efficiency |
| :--- | :--- | :--- | :--- | :--- |
| seq | 1 | 0.00144 | 1.00 | 100% |
| mpi | 1 | 0.00455 | 0.31 | 31% |
| mpi | 4 | 0.00200 | 0.72 | 18% |

*   **mpi (1)** — запуск параллельной программы на 1 процессе (демонстрирует накладные расходы на инициализацию MPI без сетевого обмена).
*   **mpi (4)** — запуск на 4 процессах.

### 7. Выводы
По результатам эксперимента видно, что параллельная реализация на 4-х процессах работает **медленнее** последовательной (Speedup = 0.72).

**Причины отсутствия ускорения:**
1.  **Низкая вычислительная сложность:** Время обработки 10 млн символов одним ядром составляет всего ~1.4 мс. Это слишком мало, чтобы перекрыть накладные расходы.
2.  **Накладные расходы MPI:** Время, затрачиваемое на `MPI_Bcast` (рассылку 10 МБ данных четырем процессам) и инициализацию среды, превышает выигрыш от разделения задачи.
3.  **Memory Bound:** Задача упирается в скорость чтения из оперативной памяти, а не в вычисления процессора. Добавление потоков лишь увеличивает конкуренцию за доступ к памяти.

**Заключение:**
Параллелизация данной задачи методом дублирования данных (`MPI_Bcast`) неэффективна для малых и средних объемов данных. Она может иметь смысл только при использовании `MPI_Scatter` (распределение частей массива без дублирования) и на объемах данных, превышающих сотни мегабайт.

### 8. Литература
1.  Стандарт MPI 3.1.
2.  Лекции по курсу "Параллельное программирование".
