# Отчет по лабораторной работе
## "Подсчет частоты символа в строке"

- **Студент:** Шехирев Владислав
- **Группа:** 3823Б1ФИ2
- **Технология:** SEQ, MPI
- **Вариант:** 23

### 1. Введение
**Мотивация:** Обработка больших объемов текстовых данных — востребованная задача в таких областях, как биоинформатика (анализ генома), обработка логов и поиск паттернов. Распараллеливание этой задачи позволяет сократить время обработки за счет одновременного сканирования разных частей массива.

**Проблема:** Операция сравнения символов является элементарной и выполняется процессором очень быстро. Основным "узким местом" при распараллеливании становится пропускная способность памяти (Memory Bandwidth) и накладные расходы на передачу данных между процессами.

**Ожидаемый результат:** На системе с общей памятью (один компьютер) ожидается, что накладные расходы на копирование данных средствами MPI (распределение задачи) могут превысить выигрыш от распараллеливания вычислений.

### 2. Постановка задачи
**Задано:** Строка `str` произвольной длины и искомый символ `target`.
**Требуется:** Определить количество вхождений символа `target` в строку `str`.

**Входные данные:** `std::string str`, `char target`.
**Выходные данные:** `int count` (число вхождений).

### 3. Базовый алгоритм (Последовательный)
Последовательный алгоритм выполняет линейный проход по строке.
Сложность алгоритма: `O(N)`.

```cpp
int count = 0;
for (char c : str) {
    if (c == target) count++;
}
```

### 4. Схема распараллеливания
Используется **геометрическая декомпозиция данных** (блочное разбиение).

**Алгоритм MPI-версии:**
1.  **Инициализация:** Данные (строка) изначально создаются и находятся только на корневом процессе (`rank 0`).
2.  **Расчет границ:** Корневой процесс вычисляет размеры блоков для каждого процесса-работника. Если длина строки не делится нацело, остаток равномерно распределяется между первыми процессами.
3.  **Распределение данных (`MPI_Scatterv`):**
    *   Используется функция `MPI_Scatterv`, которая "нарезает" исходную строку и отправляет каждому процессу **только его часть**.
    *   Это позволяет алгоритму работать на распределенных кластерах, где у каждого узла своя ограниченная память, но на одной машине это создает накладные расходы на копирование памяти.
4.  **Локальные вычисления:** Каждый процесс подсчитывает вхождения символа в своем локальном буфере, используя оптимизированную функцию `std::count` (которая может использовать SIMD-инструкции).
5.  **Агрегация:** Локальные результаты суммируются в глобальный результат на корневом процессе с помощью `MPI_Reduce` (операция `MPI_SUM`).

### 5. Экспериментальная установка
*   **Hardware:** Intel Core i5, 8GB RAM.
*   **Environment:** Docker Container (Ubuntu 22.04) running on Windows 11 (WSL 2).
*   **Toolchain:** GCC 14.2.0, OpenMPI 4.1.x.
*   **Data:** Строка длиной **1,000,000,000 символов (1 GB)**.

### 6. Результаты
#### 6.1 Корректность
Корректность алгоритмов проверена набором функциональных тестов (Google Test).
Протестированы сценарии: обычная строка, отсутствие искомого символа, пустая строка, строка только из искомых символов, граничные случаи. Все тесты пройдены успешно.

#### 6.2 Производительность
Замеры проводились с использованием `ppc_perf_tests`.

| Mode | Count | Time, s | Speedup | Efficiency |
| :--- | :--- | :--- | :--- | :--- |
| seq | 1 | 0.09743 | 1.00 | 100% |
| mpi | 1 | 1.18918 | 0.08 | 8% |
| mpi | 2 | 0.71070 | 0.14 | 7% |
| mpi | 4 | 0.50994 | 0.19 | 4.8% |
| mpi | 8 | 0.52885 | 0.18 | 2.2% |

*   **seq (1)** — последовательная версия (Zero-copy).
*   **mpi (1)** — параллельная версия на 1 процессе (демонстрирует чистые накладные расходы на копирование 1 ГБ данных внутри `MPI_Scatterv` без сети).
*   **Speedup** рассчитан как $T_{seq} / T_{mpi}$.

### 7. Выводы

Результаты экспериментов показывают следующее:

1.  **Влияние копирования памяти:**
    Реализация через `MPI_Scatterv` (требование для распределенных систем) на одной машине приводит к существенному замедлению. Последовательная версия обрабатывает 1 ГБ данных за ~0.1 с, тогда как MPI тратит ~1.2 с только на подготовку и копирование буферов. Это подтверждает, что задача является **Memory Bound**.

2.  **Внутренняя масштабируемость MPI:**
    Несмотря на проигрыш последовательной версии, внутри самой MPI-реализации наблюдается ускорение при добавлении процессов:
    *   **1 -> 2 процесса:** время сократилось с 1.19 с до 0.71 с.
    *   **2 -> 4 процесса:** время сократилось с 0.71 с до 0.51 с.
    Это доказывает, что распараллеливание вычислительной части (`std::count`) работает корректно и частично компенсирует накладные расходы на передачу данных.

3.  **Точка насыщения (8 процессов):**
    При переходе с 4 на 8 процессов производительность перестала расти и даже немного снизилась (0.51 с -> 0.53 с). Это свидетельствует о **насыщении пропускной способности шины памяти** и увеличении накладных расходов планировщика ОС на переключение контекста.

**Заключение:**
Для задач с низкой вычислительной плотностью использование MPI на одной машине нецелесообразно из-за доминирования расходов на копирование данных. Данный алгоритм предназначен для работы с гигантскими объемами данных на реальных кластерах, где сеть и объем памяти узлов становятся критическими факторами.

### 8. Литература
1.  Message Passing Interface (MPI) Forum. MPI: A Message-Passing Interface Standard.
2.  Лекционные материалы курса "Параллельное программирование".
3.  Документация OpenMPI: [https://www.open-mpi.org/](https://www.open-mpi.org/)