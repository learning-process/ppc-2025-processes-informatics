# Параллельное вычисление двойного интеграла методом Симпсона с использованием MPI

**Дисциплина:** Параллельное программирование  
**Преподаватель:** Нестеров Александр Юрьевич, Оболенский Арсений Андреевич  
**Студент:** Спичек Денис Игоревич  
**Группа:** 3823Б1ФИ1  
**Вариант:** 9  

---

## Введение

Распараллеливание является ключевым подходом для ускорения вычислений в наукоемких задачах.  
В данной работе рассматривается реализация алгоритма вычисления двойного интеграла

\[\iint f(x, y)\,dx\,dy\]

на прямоугольной области \([0, 1] 	imes [0, 1]\) с использованием составной формулы Симпсона.  
Реализация выполнена как в последовательной, так и в параллельной форме с применением технологии **MPI (Message Passing Interface)**.

**Цель работы** — реализовать и сравнить эффективность последовательного и параллельного алгоритмов вычисления двойного интеграла, а также подтвердить корректность MPI-версии.

---

## Постановка задачи

Необходимо вычислить двойной интеграл от функции

\[f(x, y) = x^2 + y^2
\]

на области \([0, 1] 	imes [0, 1]\) с использованием составной формулы Симпсона.

---

## Аналитическое решение

Аналитическое значение интеграла:

\[\iint_{[0,1]^2} (x^2 + y^2)\,dx\,dy = rac{2}{3}  pprox 0.666\ldots\]

Для целей тестирования результат округляется до ближайшего целого числа:

\[	ext{round}\left(rac{2}{3}
ight) = 1\]

---

## Требуется

- Реализовать последовательную версию вычисления (формула Симпсона для двойного интеграла);
- Реализовать параллельную версию с использованием MPI;
- Провести сравнение времени выполнения и подтвердить корректность вычислений.

---

## Описание алгоритма

Двойной интеграл

\[\iint_R f(x, y)\,dx\,dy\]

на прямоугольной области  
\(R = [a, b] 	imes [c, d]\)  
с использованием составной формулы Симпсона при \(N\) разбиениях по каждой координате (где \(N\) — чётное) вычисляется по формуле:

\[\iint_R f(x, y)\,dx\,dy  pprox
rac{h_x h_y}{9}
\sum_{i=0}^{N} \sum_{j=0}^{N} w_i w_j f(x_i, y_j)
\]

где:

- \(h_x = h_y = h = rac{1}{N}\) (для области \([0, 1] 	imes [0, 1]\));
- весовые коэффициенты \(w_k\) определяются как:
  - \(w_0 = 1\);
  - \(w_N = 1\);
  - \(w_k = 4\), если \(k\) нечётное;
  - \(w_k = 2\), если \(k\) чётное и \(k 
eq 0, N\).

---

## 1. Последовательный алгоритм (`ops_seq.cpp`)

Последовательный алгоритм выполняет прямой расчёт двойной суммы, используя два вложенных цикла и функцию `GetSimpsonWeight` для определения весовых коэффициентов \(w_i\) и \(w_j\).

Итоговый результат вычисляется по формуле:

\[	ext{result} = rac{	ext{sum} \cdot h \cdot h}{9.0}\]

---

## 2. Параллельный алгоритм (`ops_mpi.cpp`)

В параллельной версии используется распределение работы по внешнему циклу (по индексу \(i\)).

Основные этапы:

1. **Инициализация MPI** и определение ранга процесса (`rank`) и общего числа процессов (`size`);
2. **Распространение данных:** число разбиений \(N\) рассылается всем процессам с помощью `MPI_Bcast`;
3. **Распределение нагрузки:** каждый процесс вычисляет локальную сумму `local_sum` для непересекающихся значений индекса \(i\) с шагом, равным числу процессов (`size`):

   ```cpp
   // Distribute rows (i) among processes
   for (int i = rank; i <= n; i += size) {
       // ... внутренний цикл по j ...
   }
   ```

4. **Сбор результатов:** частичные суммы собираются и суммируются в глобальную сумму `global_sum` с помощью `MPI_Allreduce` с операцией `MPI_SUM`;
5. **Финальный результат:** каждый процесс вычисляет итоговое значение интеграла:

\[	ext{result} = rac{	ext{global\_sum} \cdot h \cdot h}{9.0}\]

---

## Описание программной реализации (MPI-версия)

Реализация выполнена на языке **C++** с использованием библиотеки **MPI** в классе `SpichekDSimpsonIntegralMPI`.

Ключевые особенности реализации:

- используется блочно-циклическое распределение работы по внешнему индексу \(i\), обеспечивающее хорошую балансировку нагрузки;
- обмен данными минимизирован и сводится к одной коллективной операции `MPI_Allreduce`;
- для обработки весовых коэффициентов применяется вспомогательная функция `GetSimpsonWeight`.

---

## Результаты экспериментов

### Условия экспериментов

- Входной параметр: число разбиений \(N = 10000\);
- Среда выполнения: Windows, MS-MPI;
- Оборудование: локальная рабочая станция;
- Методика: измерение времени встроенными средствами тестового фреймворка, усреднённые значения для режима *pipeline*.

---

### Результаты замеров времени (pipeline)

| Кол-во процессов (MPI) | Время MPI (сек) | Время SEQ (сек) | Ускорение (отн. MPI-1) | Примечание |
|------------------------|-----------------|------------------|------------------------|------------|
| 1 | 0.0978 | 0.0972 | 1.00× | Базовое время MPI/SEQ |
| 2 | 0.0501 | 0.0996 | 1.95× | Значительное ускорение |
| 3 | 0.0373 | 0.1052 | 2.62× | Хорошая масштабируемость |
| 4 | 0.0269 | 0.1095 | 3.64× | Лучшее время выполнения |

---

## Анализ результатов

### Сравнение MPI и SEQ

- В задаче вычисления двойного интеграла сложность алгоритма составляет \(O(N^2)\), что обеспечивает высокий выигрыш от распараллеливания;
- Для одного MPI-процесса время выполнения сопоставимо с последовательной версией;
- Начиная с двух процессов MPI-версия демонстрирует значительное ускорение.

### Масштабируемость MPI

- При увеличении числа процессов с 1 до 4 время выполнения уменьшилось с 0.0978 с до 0.0269 с;
- Достигнуто ускорение ≈ 3.64, что близко к идеальному ускорению в 4 раза.

---

## Подтверждение корректности

Функциональные тесты для различных значений \(N\) (10, 20, 40) и для обоих режимов (SEQ и MPI) завершились со статусом **[ PASSED ]**.

Результаты параллельных вычислений полностью совпадают с последовательной реализацией и аналитическим значением \( pprox 0.666\) (округлённым до 1).

---

## Выводы

- Реализованы последовательная и параллельная (MPI) версии алгоритма вычисления двойного интеграла методом Симпсона;
- Параллельная реализация демонстрирует хорошую масштабируемость и ускорение до 3.64 раза на 4 процессах;
- Показано, что для вычислительно сложных задач преимущества MPI перевешивают накладные расходы на коммуникации;
- Эффективно использованы коллективные операции `MPI_Bcast` и `MPI_Allreduce`.
