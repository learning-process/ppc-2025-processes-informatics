# <Сумма элементов вектора>

- Student: Лукин Иван Антонович, group 3823Б1ФИ3
- Technology: SEQ | MPI
- Variant: 1

## 1. Introduction
Мотивация: Исследовать эффективность распараллеливания простой операции через MPI

Проблема: Сложение векторов имеет низкую арифметическую интенсивность - затраты на коммуникации могут превысить выгоду от параллельных вычислений

Ожидаемый результат: MPI-версия может уступать последовательной из-за преобладания накладных расходов распределения данных над скоростью вычислений

## 2. Problem Statement
На вход приходит вектор произвольного размера.
Задача - вычислить сумму его элементов (целые числа)

## 3. Baseline Algorithm (Sequential)
Базовый (последовательный) алгоритм проходит по всему вектору и складывает его элементы с помощью std::accumulate

## 4. Parallelization Scheme
Параллельный алгоритм реализует распределенное суммирование элементов вектора с использованием MPI. Корневой процесс распределяет данные между всеми процессами с помощью операции Scatterv, обеспечивая равномерное распределение элементов с учетом возможного остатка (Всем по целой части, остаток распределяется между первыми reminder процессами). Каждый процесс независимо вычисляет локальную сумму своих элементов. Затем с помощью операции Allreduce все частичные суммы складываются, и итоговый результат становится доступен всем процессам одновременно. Для случая, когда количество процессов превышает размер вектора, алгоритм предусматривает специальную обработку, где только root-процесс выполняет последовательное суммирование, а результат рассылается остальным процессам через Broadcast

## 5. Experimental Setup
- Hardware/OS: Intel I5-13420H, 8 cores, 16GB RAM, Win10 OS
- Toolchain: Microsoft Visual C++ (MSVC), MSBuild 17.14.23, Release
- Environment: PPC_NUM_PROC
- Data: Вектор на 20'000'000 элементов, элементы составляют арифметическую прогрессию от 1 до vector.size с шагом 1

## 6. Results and Discussion

### 6.1 Correctness
Корректность реализаций проверена функциональными тестами. Они проверяют, может ли параллельный алгоритм обрабатывать все сценарии распределения ресурсов между процессами: на вход поступают векторы размером от 1 до 11.

### 6.2 Performance
Это тесты производительности на векторе из 1 млн элементов. Видно, что выигрыш от распараллеливания затмевается расходами на коммуникацию, причем при масштабировании это все более заметно.

| Mode        | Count | Time, s | Speedup | Efficiency |
|-------------|-------|---------|---------|------------|
| seq         | 1     | 0.0004  | 1.00    | N/A        |
| mpi         | 4     | 0.00068 | 0.6     | 15.0%      |
| mpi         | 8     | 0.0013  | 0.31    | 4.0%       |

Это тесты производительности на векторе из 20 млн элементов. При увеличении размера входного вектора увеличивается выигрыш от распараллеливания при вычислениях, но в то же время растут накладные расходы на коммункацию. Масштабирование параллелизации до 8 процессов усугубляет нагрузку, осуществляемую Scatterv.

| Mode        | Count | Time, s | Speedup | Efficiency |
|-------------|-------|---------|---------|------------|
| seq         | 1     | 0.01    | 1.00    | N/A        |
| mpi         | 4     | 0.022   | 0.45    | 11.3%      |
| mpi         | 8     | 0.025   | 0.4     | 5.0%       |


## 7. Conclusions
Реализация распределенного суммирования векторов с использованием MPI показала свою принципиальную неэффективность для данной задачи. Основная проблема заключается в катастрофическом преобладании коммуникационных затрат над вычислительными, что особенно проявляется при работе с большими объемами данных. С ростом размера вектора и количества процессов накладные расходы на распределение данных через операцию Scatterv становятся настолько значительными, что полностью нивелируют преимущества параллельных вычислений. Низкая арифметическая интенсивность операции сложения векторов делает этот тип задач непригодным для эффективного распараллеливания с использованием MPI-подхода, что подтверждается резким падением эффективности до 5-11% при работе с крупными массивами данных.


## 8. References
1. Лекции по параллельному программированию ННГУ
2. Стандарт MPI