# <Сумма элементов вектора>

- Student: Лукин Иван Антонович, group 3823Б1ФИ3
- Technology: SEQ | MPI
- Variant: 1

## 1. Introduction
Мотивация: Исследовать эффективность распараллеливания простой операции через MPI

Ожидаемый результат: Ускорение MPI версии алгоритма даже при условии того, что сложение - очень быстрая операция. Такие ожидания связаны с особенностью постановки задачи - исходный вектор известен сразу на всех процессах.

## 2. Problem Statement
На вход приходит вектор произвольного размера.
Задача - вычислить сумму его элементов (целые числа)

## 3. Baseline Algorithm (Sequential)
Базовый (последовательный) алгоритм проходит по всему вектору и складывает его элементы с помощью std::accumulate

## 4. Parallelization Scheme
Параллельный алгоритм реализует распределенное суммирование элементов вектора с использованием MPI. Сначала процессы определяют общий размер вектора и количество доступных процессов. Каждый процесс независимо вычисляет границы своей части данных на основе его ранга и общего количества процессов, обеспечивая равномерное распределение элементов с учетом возможного остатка. Первые reminder процессов получают на один элемент больше, где reminder представляет собой остаток от деления размера вектора на количество процессов. Каждый процесс вычисляет локальную сумму элементов в своей части вектора с помощью std::accumulate. Затем с помощью операции MPI_Allreduce с операцией MPI_SUM все частичные суммы складываются, и итоговый результат становится доступен всем процессам одновременно. Для случая, когда количество процессов превышает размер вектора, алгоритм предусматривает специальную обработку, где только root-процесс выполняет последовательное суммирование всего вектора, а результат рассылается остальным процессам через MPI_Bcast.

## 5. Experimental Setup
- Hardware/OS: Intel I5-13420H, 8 cores, 16GB RAM, Win10 OS
- Toolchain: Microsoft Visual C++ (MSVC), MSBuild 17.14.23, Release
- Environment: PPC_NUM_PROC
- Data: Вектор на 20'000'000 элементов, элементы составляют арифметическую прогрессию от 1 до vector.size с шагом 1

## 6. Results and Discussion

### 6.1 Correctness
Корректность реализаций проверена функциональными тестами. Они проверяют, может ли параллельный алгоритм обрабатывать все сценарии распределения ресурсов между процессами: на вход поступают векторы размером от 1 до 11.

### 6.2 Performance
Это тесты производительности на векторе из 1 млн элементов. 

| Mode        | Count | Time, s | Speedup | Efficiency |
|-------------|-------|---------|---------|------------|
| seq         | 1     | 0.00019 | 1.00    | N/A        |
| mpi         | 4     | 0.00035 | 0.54    | 13.5%      |
| mpi         | 8     | 0.00041 | 0.46    | 6.0%       |

Это тесты производительности на векторе из 20 млн элементов. 

| Mode        | Count | Time, s | Speedup | Efficiency |
|-------------|-------|---------|---------|------------|
| seq         | 1     | 0.006   | 1.00    | N/A        |
| mpi         | 4     | 0.0022  | 2.7     | 68.2%      |
| mpi         | 8     | 0.0035  | 1.71    | 21.4%      |

Можно увидеть, что выигрыш от распараллеливания заметен только на больших данных. Это связано с тем, что затраты на счет при малых данных малы относительно затрат на создание процессов и их коммуникацию.

Задача эффективнее на 4 процессах, так как на 8 повышаются накладные расходы - создание большего количества процессов, опреация редукции между ними.

## 7. Conclusions
Реализация распределенного суммирования с использованием MPI демонстрирует ограниченную эффективность для данной задачи. Основная проблема заключается в преобладании коммуникационных затрат над вычислительными, особенно при работе с малыми объемами данных. Низкая арифметическая интенсивность операции сложения делает эту задачу малопригодной для эффективного распараллеливания на небольших векторах.

На больших объемах данных достигается положительный эффект от параллелизации, однако с существенными ограничениями масштабируемости. Рост количества процессов приводит к резкому падению эффективности из-за увеличения накладных расходов на коммуникации. Таким образом, MPI-реализация может быть целесообразна только для обработки крупных векторов при умеренном количестве процессов, где вычислительная нагрузки компенсирует коммуникационные издержки.


## 8. References
1. Лекции по параллельному программированию ННГУ
2. Стандарт MPI