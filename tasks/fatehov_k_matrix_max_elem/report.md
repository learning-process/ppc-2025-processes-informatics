# Максимальное значение элементов матрицы

- Студент: Фатехов Камиль Гаярович, группа 3823Б1ФИ3
- Технология: SEQ | MPI
- Вариант: 13

## 1. Введение
**Цель лабораторной**: 
Разработка последовательной и параллельной версии поиска максимального значения элементов матрицы с последующим сравнением этих двух версий.

**Задачи лабораторной**:  
    - Реализовать последовательную версию поиска максимального значения элементов матрицы.  
    - Изучить основы MPI и реализовать параллельную версию программы.  
    - Сравнить эти производительность и эффективность этих реализаций
## 2. Постановка задачи
**Задача**: Определить максимальное значение элементов матрицы

**Входные данные**  
    Данные представлены в виде `tuple(rows, columns, matrix)`, где:  
    - `rows` - количество строк матрицы (*size_t*)  
    - `columns` - количество столбцов матрицы (*size_t*)  
    - `matrix` - непосредственно сама матрица (*vector*)

**Выходные данные**
На выходе получается единичное значение типа *double* - максимальный элемент в матрице.  

**Ограничения**:  
    - `rows <= 10000`  
    - `cols <= 10000` 
 

## 3. Базовый алгоритм (Последовательный)

 **Реализован следующий код**:
```cpp 
bool FatehovKMatrixMaxElemSEQ::RunImpl() {
  auto &data = GetInput();
  size_t rows = std::get<0>(data);
  size_t columns = std::get<1>(data);
  std::vector<double>& matrix = std::get<2>(data);
  double max = matrix[0];
  for (size_t i = 0; i < rows * columns; i++) {
    max = std::max(matrix[i], max);
  }
  GetOutput() = max;
  return true;
}
```

**Алгоритм работы**:  
1. Получаем входные данные: количество строк (rows), количество столбцов (columns) и ссылку на матрицу, записанную в виде одномерного вектора, хранящего элементы построчно

2. Инициализируем максимум: присваиваем переменной max значение первого элемента матрицы

3. Последовательно перебираем элементы: через цикл проходим по всем элементам матрицы

4. Сравниваем и обновляем результат: если элемент больше чем текущий максимумом, обновляем значение max

5. Возвращаем результат: записываем найденное максимальное значение в выходные данные  

## 4. Схема распараллеливания

**Описание**:  
- Код реализует параллельный поиск максимального элемента в матрице с использованием MPI. Данные распределяются блоками по процессам, каждый процесс находит локальный максимум, а затем все процессы объединяют свои результаты чтобы найти общий максимум.

**Алгоритм работы**:  

1. **Инициализация MPI и получение размеров матрицы**

```cpp
int world_rank = 0;
int world_size = 0;
MPI_Comm_rank(MPI_COMM_WORLD, &world_rank);
MPI_Comm_size(MPI_COMM_WORLD, &world_size);

size_t rows = 0, columns = 0;
if (world_rank == 0) {
  auto &data = GetInput();
  rows = std::get<0>(data);
  columns = std::get<1>(data);
}
```

2. **Рассылка размеров матрицы всем процессам**
```cpp
MPI_Bcast(&rows, 1, MPI_UNSIGNED_LONG, 0, MPI_COMM_WORLD);
MPI_Bcast(&columns, 1, MPI_UNSIGNED_LONG, 0, MPI_COMM_WORLD);
```

3. **Распределение данных**  

*Схема распределения*:  

Всего элементов: `N = rows × columns `   
Базовое количество на процесс: `elems_per_proc = N / world_size`  
Остаток: `remainder = N % world_size`   

*Вычисление размеров блоков*:  

```cpp
std::vector<int> send_counts(world_size);
std::vector<int> displacements(world_size);

for (int i = 0; i < world_size; ++i) {
    send_counts[i] = static_cast<int>(elems_per_proc) + (std::cmp_less(i, remainder) ? 1 : 0);
    displacements[i] = (i == 0) ? 0 : (displacements[i - 1] + send_counts[i - 1]);
}
```

*Пример (4 процесса, 10 элементов)*:

Процесс 0: send_counts[0]=3, displacements[0]=0  
Процесс 1: send_counts[1]=3, displacements[1]=3  
Процесс 2: send_counts[2]=2, displacements[2]=6  
Процесс 3: send_counts[3]=2, displacements[3]=8  

4. **Распределение данных через MPI_Scatterv**

```cpp
std::vector<double> local_data(send_counts[world_rank]);
const std::vector<double> *full_matrix_ptr = nullptr;

if (world_rank == 0) {
  auto &data = GetInput();
  full_matrix_ptr = &std::get<2>(data);
}

MPI_Scatterv((world_rank == 0) ? (*full_matrix_ptr).data() : nullptr,  send_counts.data(), displacements.data(),
               MPI_DOUBLE, local_data.data(), send_counts[world_rank], MPI_DOUBLE, 0, MPI_COMM_WORLD);

```
5. **Локальные вычисления**
```cpp
double local_max = -std::numeric_limits<double>::max();
for (const auto &value : local_data) {
    local_max = std::max(value, local_max);
}
```

6. **Объединение локальных максимумов**
```cpp
double global_max = NAN;
MPI_Allreduce(&local_max, &global_max, 1, MPI_DOUBLE, MPI_MAX,MPI_COMM_WORLD);
```

*Схема связи*:
```
P0: local_max₀ ──────┐  
P1: local_max₁ ──────┤─ Allreduce(MPI_MAX) ─→ global_max (во всех процессах)  
P2: local_max₂ ──────┤  
P3: local_max₃ ──────┘  
```
7. **Запись результата**

```cpp
GetOutput() = global_max;
```
## 5. Детали реализации

### **Генерация тестовых данных**

Для создания тестовой матрицы используется линейный конгруэнтный генератор (ЛКГ) - алгоритм для генерации псевдослучайных чисел.

**Алгоритм ЛКГ**:

```cpp
for (size_t i = 0; i < total; ++i) {
      state = (a * state + c) % m;
      double value = ((static_cast<double>(state) / m) * 2000.0) - 1000.0;
      matrix.push_back(value);

      max_val = std::max(value, max_val);
    }
```

где:

- `state` - текущее состояние генератора  
- `a` - множитель (1664525)
- `c` - приращение (1013904223)
- `m` - модуль (2²² = 4194304)

**Процесс генерации**: 

1. Начинаем с начального состояние `state = 42`
2. Для каждого элемента матрицы:  
    - Вычисляем новое состояние по формуле ЛКГ  
    - Преобразуем в число от -1000 до 1000
    - Сохраняем в матрице
    - Сравниваем с текущим максимумом

**Преимущества для тестирования**:

- Генерирует одинаковые данные на всех процессах

- Быстрый и простой алгоритм

- Предсказуемый результат для проверки правильности

Таким способом создается матрица 5000×10000 элементов (50 миллионов чисел) с известным максимальным значением для проверки работы алгоритма.


## 6. Экспериментальная среда
- Hardware/OS: AMD RYZEN 5 5600 6-Core Processor, 12-Threads, 16GB, Ubuntu (DevContainer/WSL 2.0)
- Toolchain: GCC 14.1.0, cmake version 3.31.1, Release
- Data: происходит генерация данных через функцию, описанную в пункте 5

## 7. Результаты и обсуждение

### 7.1 Корректность
**Модульные тесты**

В коде реализовано 3 тестовых случая:

- Тест 1: Матрица 3×4 с положительными числами (максимум = 12)

- Тест 2: Матрица 3×3 с отрицательными числами (максимум = -2)

- Тест 3: Матрица 5×5 со смешанными значениями (максимум = 4123412)

**Сравнение с результатом**

- Результат алгоритма сравнивается с истинным значением, которое было вычислено заранее  (`expected_result_`)

**Условие корректности**
 - Параллельная MPI-версия и последовательная SEQ-версия дают идентичные результаты

 - Все процессы в MPI-реализации получают одинаковый глобальный максимум

Таким образом, корректность подтверждается совпадением результатов с эталонными значениями, успешным прохождением модульных тестов и идентичностью результатов разных реализаций алгоритма.

### 7.2 Производительность
- При тестировании производительности генерируется матрица 5000×10000 элементов
- Диапазон значений: [-1000, 1000]
- Генерация данных: линейный конгруэнтный генератор
- Во время генерации вычисляется и сохраняется ожидаемый максимум

| **Режим** | **Количество процессов** | **Время, с** | **Ускорение** | **Эффективность** |
|-------------|-------|---------|---------|------------|
| seq         | 1     | 0.0509   | 1.00    | N/A       |
| mpi         | 2     | 0.2713   | 0.19    | 9.5%      |
| mpi         | 4     | 0.1886   | 0.27    | 6.75%     |
| mpi         | 6     | 0.1687   | 0.30    | 5%       |

**Анализ результатов**:

1. **SEQ значительно быстрее MPI**:
   - SEQ версия: 0.0509 сек
   - Лучшая MPI версия (6 процессов): 0.1687 сек
   - SEQ в **3.3 раза быстрее** MPI с 6 процессами

2. **Отрицательное ускорение**:
   - Ускорение < 1.0 для всех MPI конфигураций
   - MPI с 2 процессами: ускорение 0.19 (в 5 раз медленнее SEQ)

3. **Низкая эффективность**:
   - Максимальная эффективность: 9.5% (2 процесса)
   - Эффективность падает с ростом числа процессов

**Причины низкой производительности MPI**:

1. **Высокие накладные расходы коммуникации**:
   - `MPI_Bcast`: передача размеров матрицы
   - `MPI_Scatterv`: распределение данных
   - `MPI_Allreduce`: сбор результатов

2. **Простота вычислений**:
   - Поиск максимума: 1 сравнение на элемент
   - Вычислительная сложность: O(n)
   - Отношение вычислений к коммуникациям невыгодное


## 8. Заключение

В ходе лабораторной работы были реализованы последовательная и параллельная версии алгоритма поиска максимального элемента в матрице.

**Основные выводы**:

1. **SEQ версия показала лучшую производительность**:
   - Время выполнения: 0.0509 сек
   - Использует локальность данных и кэш процессора
   - Отсутствуют накладные расходы на коммуникацию

2. **MPI версия демонстрирует отрицательное ускорение**:
   - Лучший результат (6 процессов): 0.1687 сек (в 3.3 раза медленнее SEQ)
   - Низкая эффективность (5-9.5%)
   - Накладные расходы MPI превышают выигрыш от параллелизации


## 9. Источники
[Линейный конгруэнтный генератор](https://www.tutorialspoint.com/cplusplus-program-to-implement-the-linear-congruential-generator-for-pseudo-random-number-generation)



