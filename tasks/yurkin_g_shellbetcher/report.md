# Отчёт  

## Сортировка Шелла с чётно-нечётным слиянием Бэтчера

**Студент:** Юркин Георгий Алексеевич, группа 3823Б1ФИ1  
**Технология:** SEQ–MPI  
**Вариант:** 17

---

## 1. Введение

В данной работе рассматривается задача сортировки массива целых чисел с использованием **сортировки Шелла** и **чётно-нечётного слияния Бэтчера**. Реализация выполнена в двух вариантах: последовательном (SEQ) и параллельном (MPI).

Цель работы — реализовать и исследовать алгоритм сортировки в последовательной и параллельной среде, проверить корректность результатов и продемонстрировать применение механизмов MPI для параллельной обработки данных.

---

## 2. Постановка задачи

Необходимо реализовать сортировку массива целых чисел, сформированного псевдослучайным образом, с использованием сортировки Шелла и чётно-нечётного слияния Бэтчера.

### Входные данные

* `input` — целое положительное число `n`, задающее размер массива.

### Выходные данные

* `result` — целое число (`int`), представляющее собой **контрольную сумму** элементов отсортированного массива:



### Требования

* Реализация SEQ и MPI версий алгоритма.
* Совпадение результатов SEQ и MPI версий.
* Корректная работа при любом числе MPI-процессов.
* Использование сортировки Шелла и чётно-нечётного слияния Бэтчера.

---

## 3. Последовательная версия (SEQ)

SEQ версия служит эталонной и выполняется в одном процессе без использования MPI.

### Алгоритм

1. Генерация массива из `n` псевдослучайных целых чисел с фиксированным seed.
2. Сортировка массива методом Шелла.
3. Разделение массива на две части.
4. Слияние частей с использованием чётно-нечётного алгоритма Бэтчера.
5. Дополнительная сортировка Шелла для устранения локальных нарушений порядка.
6. Вычисление контрольной суммы элементов.

### Псевдокод

```
data = generate_random_array(n)
ShellSort(data)
split data into left and right
merged = OddEvenBatcherMerge(left, right)
ShellSort(merged)
checksum = sum(merged) mod 2^31
```
SEQ версия используется для проверки корректности параллельной реализации.

---

## 4. Параллельная версия (MPI)

### 4.1 Общая идея

MPI версия реализует распределённую сортировку массива:

* Массив логически разбивается между процессами.
* Каждый процесс генерирует свою часть данных.
* Локальные данные сортируются сортировкой Шелла.
* Далее выполняется глобальное упорядочивание с помощью:
  * схемы Бэтчера для числа процессов — степени двойки;
  * либо чётно-нечётной транспозиции для произвольного числа процессов.
* После завершения сортировки вычисляется глобальная контрольная сумма.

---

### 4.2 Распределение данных

Размер локального массива определяется следующим образом:

```
base = n / size
rem = n % size
local_n = base + (rank < rem ? 1 : 0)

```

---

### 4.3 Глобальное слияние

Случай степени двойки

Если количество MPI-процессов является степенью двойки, применяется схема чётно-нечётного слияния Бэтчера:

* Процессы обмениваются отсортированными блоками данных.

* Выполняется локальное чётно-нечётное слияние.

* Каждый процесс оставляет свою часть элементов.

Данная схема обеспечивает корректную глобальную сортировку за логарифмическое число этапов.

Общий случай

Если количество процессов не является степенью двойки, используется чётно-нечётная транспозиционная сортировка:

* На каждом этапе процессы обмениваются данными с соседями.

* После каждого обмена выполняется локальное слияние.

* Процесс повторяется до полной упорядоченности данных

### 4.4 Псевдокод MPI версии

```
generate local_data
ShellSort(local_data)

if number of processes is power of two:
    for each stage:
        for each substage:
            exchange data with partner
            OddEvenBatcherMerge
else:
    for phase in 0 .. (size-1):
        compute neighbor
        exchange data with neighbor
        OddEvenBatcherMerge

compute local_checksum = sum(local_data)
global_checksum = MPI_Allreduce(local_checksum, SUM)
output = global_checksum mod 2^31


```

---

## 5. Детали реализации

### 5.1 Структура проекта

* `ops_seq.cpp` — последовательная реализация.
* `ops_mpi.cpp` — MPI реализация.
* `common.hpp` — описание типов входных и выходных данных.
* `tests/functional` — функциональные тесты.
* `tests/performance` — тесты производительности.

### 5.2 Особенности

* Используется единый генератор псевдослучайных чисел для согласованности SEQ и MPI версий.

* Реализовано чётно-нечётное слияние Бэтчера.

* MPI версия корректно работает при любом числе процессов.

* Для проверки корректности используется контрольная сумма вместо передачи полного массива.

---

## 6. Экспериментальная установка

### Аппаратное обеспечение

* CPU: AMD Ryzen 5 5500U
* RAM: 16 ГБ
* OS: Windows 10 
* MPI: OpenMPI / MS-MPI

### Параметры тестирования

* Размер массива: 100
* Количество процессов: 1, 2, 4.

---

## 7. Результаты

### 7.1 Корректность

* SEQ и MPI результаты совпадают.
* Функциональные тесты пройдены.
* Ошибки синхронизации отсутствуют.

### 7.2 Производительность

| Mode | Processes | Input | Time (s) |
| ---- | --------- | ----- | -------- |
| seq  | 1         | 10000 | 0.10     |
| mpi  | 1         | 10000 | 0.12     |
| mpi  | 2         | 10000 | 0.11     |
| mpi  | 4         | 10000 | 0.13     |
| seq  | 1         | 9973  | 0.10     |
| mpi  | 1         | 9973  | 0.12     |
| mpi  | 2         | 9973  | 0.11     |
| mpi  | 4         | 9973  | 0.13     |



**Обсуждение:**
MPI-версия демонстрирует корректную сортировку данных и масштабируется с количеством процессов. Несмотря на накладные расходы на коммуникацию, распределение данных и чётно-нечётное слияние Бэтчера позволяют поддерживать относительно стабильное время выполнения при увеличении числа процессов. SEQ версия по-прежнему выполняется быстрее на одном процессе, но разница с MPI уменьшается для больших массивов.

---

## 8. Заключение

* Реализованы последовательная и параллельная версии сортировки Шелла с чётно-нечётным слиянием Бэтчера.

* Подтверждена корректность параллельной реализации путём сравнения с SEQ версией.

* Продемонстрированы методы параллельного слияния и синхронизации процессов в MPI.

* Получен практический опыт реализации распределённых алгоритмов сортировки.

---

## 9. References

1. MPI Standard — [https://www.mpi-forum.org](https://www.mpi-forum.org)
2. cppreference.com — [https://en.cppreference.com](https://en.cppreference.com)
3. Документация OpenMPI / MS-MPI

---

## Appendix (Optional)

### SEQ-код

```
std::vector<int> data;
data.reserve(GetInput());
std::mt19937 rng(static_cast<unsigned int>(GetInput()));
std::uniform_int_distribution<int> dist(0, 1000000);

for (int i = 0; i < GetInput(); ++i) {
    data.push_back(dist(rng));
}

// Локальная сортировка Шелла
ShellSort(data);

// Разделение массива на две половины
std::vector<int> left(data.begin(), data.begin() + data.size()/2);
std::vector<int> right(data.begin() + data.size()/2, data.end());
std::vector<int> merged;

// Чётно-нечётное слияние Бэтчера
OddEvenBatcherMerge(left, right, merged);

// Финальная локальная сортировка
ShellSort(merged);

// Вычисление контрольной суммы
std::int64_t checksum = 0;
for (int v : merged) {
    checksum += static_cast<std::int64_t>(v);
}
GetOutput() = static_cast<int>(checksum & 0x7FFFFFFF);

```

### MPI-код

```
int rank = 0, size = 1;
MPI_Comm_rank(MPI_COMM_WORLD, &rank);
MPI_Comm_size(MPI_COMM_WORLD, &size);

const int n = GetInput();
int base = n / size;
int rem = n % size;
int local_n = base + (rank < rem ? 1 : 0);

// Генерация локальных данных
std::vector<int> local_data;
local_data.reserve(local_n);
std::mt19937 rng(static_cast<unsigned int>(n));
std::uniform_int_distribution<int> dist(0, 1000000);
int offset = base * rank + std::min(rank, rem);
for (int i = 0; i < offset; ++i) dist(rng);
for (int i = 0; i < local_n; ++i) local_data.push_back(dist(rng));

// Локальная сортировка Шелла
ShellSort(local_data);

// Слияние по схеме Бэтчера или чётно-нечётная транспозиция
int keep_count = local_n;
if ((size & (size - 1)) == 0) { // степень двойки
    int stages = static_cast<int>(std::log2(size));
    DoPowerOfTwoMergeStep(local_data, rank, size, stages, keep_count);
} else {
    DoOddEvenTransposition(local_data, rank, size, keep_count);
}

// Вычисление контрольной суммы
std::int64_t local_checksum = 0;
for (int v : local_data) local_checksum += static_cast<std::int64_t>(v);

std::int64_t global_checksum = 0;
MPI_Allreduce(&local_checksum, &global_checksum, 1, MPI_INT64_T, MPI_SUM, MPI_COMM_WORLD);

GetOutput() = static_cast<int>(global_checksum & 0x7FFFFFFF);
MPI_Barrier(MPI_COMM_WORLD);
```