# Нахождение числа нарушений упорядоченности соседних элементов вектора (SEQ + MPI)

- **Студент:** Лифанов Кирилл Максимович, группа 3823Б1ФИ2  
- **Технология:** SEQ-MPI  
- **Вариант:** №6 

---

## 1. Введение

Цель работы — реализовать алгоритм подсчёта соседних инверсий в массиве целых чисел, а затем выполнить распараллеливание вычислений с использованием MPI.

Соседняя инверсия — это пара соседних элементов (a[i], a[i+1]), нарушающая порядок неубывания:
если  a[i] > a[i+1], то существует adjacent inversion.

Такие инверсии являются важным индикатором нарушений сортированности и используются в анализе массивов, проверке устойчивости сортировок и диагностике данных.

Параллелизация по MPI позволяет распределить фрагменты массива между процессами и ускорить подсчёт на больших данных.

---

## 2. Постановка задачи

Пусть задан вектор целых чисел v длины n.

Требуется вычислить:
count = |{ i | v[i] > v[i+1], 0 ≤ i < n-1 }|


Ограничения и требования:

- размер вектора должен быть ≥ 2;

- все значения должны корректно помещаться в тип int;

- результат SEQ и MPI вариантов должен совпадать;

- входной вектор не должен быть пустым.

---

## 3. Базовый алгоритм (последовательная версия)

Инициализируется счётчик count = 0.

В цикле от 0 до n−2 сравниваются пары (v[i], v[i+1]).

Если v[i] > v[i+1], счётчик увеличивается.

Итоговое значение возвращается как результат.

---

## 4. Схема распараллеливания (MPI)

Для параллелизации используется модель распределения данных (data parallelism): массив разбивается на почти равные части, и каждый процесс анализирует свой фрагмент.

---

### 4.1 Декомпозиция данных

Пусть:

n — размер массива
p — число MPI-процессов


Тогда:

base = n / p
rem  = n % p


Каждый процесс получает:

local_size = base + (rank < rem ? 1 : 0)


Стартовый индекс:

local_begin = rank * base + min(rank, rem)
Используется MPI_Scatterv, что позволяет отправлять фрагменты разной длины.

---
### 4.2 Локальный подсчет
Каждый процесс считает инверсии только внутри своего блока:
```cpp
for (i = 0; i + 1 < local_size; ++i)
    if block[i] > block[i+1]:
        local_count++

```
---

### 4.3 Учёт границ между блоками

Если процесс не нулевой и у блока есть элементы, то дополнительно идёт проверка:
если последний элемент блока (rank−1) > первый элемент блока (rank):
    → это тоже соседняя инверсия
---

### 4.4 Сбор результата

MPI_Reduce суммирует локальные значения на процессе 0.

MPI_Bcast рассылает итог всем процессам (что требуется тестовым фреймворком PPC).

Значение заносится в GetOutput().
---

## 5. Детали реализации проекта

### 5.1 Структура проекта:

common/include/common.hpp	определение входных/выходных типов
seq/include/ops_seq.hpp/.cpp	последовательная реализация
mpi/include/ops_mpi.hpp/.cpp	MPI-реализация
tests/functional/main.cpp	функциональные тесты
tests/performance/main.cpp	тесты производительности

---

## 6. Экспериментальная среда

- Окружение: DevContainer / Ubuntu 24.04

- CPU: Intel Core i5-12450H

- RAM: 16 GB

- MPI: OpenMPI 4.1.6

- Компилятор: GCC 14, C++23

- Сборка: Release mode

---

## 7. Результаты и обсуждение

### 7.1 Проверка корректности

В функциональных тестах (`functional/main.cpp`) используются входные файлы:

Были выполнены следующие тесты:

- монотонно возрастающие массивы;

- полностью убывающие (проверка максимального количества инверсий);

- массивы с повторяющимися элементами;

- отрицательные значения;

- зигзагообразные массивы;

- случайные данные;

- массивы размера 1 и 2;

- неравномерные и короткие блоки для MPI.

SEQ и MPI показывают строго идентичные результаты.

---

### 7.2 Производительность

Результаты измерения производительности (данные тестировались локально, длина вектора - 100000000):

| Режим | Количество процессов |    Время, с  | Ускорение  | Эффективность |
|-------|----------------------|--------------|------------|---------------|
| SEQ   | 1                    | 0.0850194422 | 1.00       | N/A           |
| MPI   | 2                    | 0.0623924212 | 1.37       | 68%           |
| MPI   | 4                    | 0.0470028635 | 1.77       | 44%           |

---

## 8. Заключение

Можем сделать вывод:

- параллелизация эффективна на больших данных;
- скорость ограничена накладными расходами Scatterv/Reduce;

---

## 9. Источники

- Куликов Г. — “Параллельное программирование”
- Peter S. Pacheco — Parallel Programming with MPI
- Kumar V. — “Introduction to Parallel Computing” (2nd edition)