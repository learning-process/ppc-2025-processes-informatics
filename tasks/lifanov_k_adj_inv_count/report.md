# Нахождение числа нарушений упорядоченности соседних элементов вектора (SEQ + MPI)

**Студент:** Лифанов Кирилл Максимович, группа 3823Б1ФИ2  
**Технология:** SEQ + MPI  
**Вариант:** №6

---

## 1. Введение

Цель работы — разработать и исследовать алгоритм подсчёта *соседних инверсий* в массиве целых чисел, а также выполнить его распараллеливание с использованием технологии MPI.

**Соседняя инверсия** — пара элементов `a[i], a[i+1]`, нарушающая порядок неубывания:

```
если a[i] > a[i+1] → существует adjacent inversion
```

Подобные инверсии являются количественной характеристикой «разупорядоченности» данных и используются при анализе массивов, диагностике ошибок сортировки и оценке качества структур данных. Параллелизация на основе MPI позволяет распределять вычисления между несколькими процессами, что ускоряет обработку больших объёмов данных.

---

## 2. Постановка задачи

Задано: вектор целых чисел `v` длины `n`.

Требуется вычислить:

```
count = |{ i | v[i] > v[i+1], 0 ≤ i < n−1 }|
```

### Ограничения:

- `n ≥ 2`;
- входной вектор не должен быть пустым;
- все значения помещаются в тип `int`;
- результаты SEQ и MPI реализаций должны совпадать.

---

## 3. Последовательный алгоритм (SEQ)

1. Инициализация счётчика:
   ```
   count = 0
   ```
2. Проход по массиву от `0` до `n−2`:
   ```
   если v[i] > v[i+1], увеличить count
   ```
3. Результат возвращается как итоговое значение.

Алгоритм имеет линейную асимптотику: **O(n)**.

---

## 4. Параллельная схема по MPI

Параллелизация основана на *разделении данных* между процессами.

### 4.1 Декомпозиция массива

Пусть:

- `n` — длина массива,
- `p` — число процессов.

Тогда:

```
base = (n - 1) / p
rem  = (n - 1) % p
```

Каждый процесс получает:

```
local_size = base + (rank < rem ? 1 : 0)
local_begin = rank * base + min(rank, rem)
```

Используется `MPI_Scatterv`, что позволяет передавать блоки разной длины.

---

### 4.2 Локальный подсчёт инверсий

Каждый процесс считает инверсии *только внутри своего фрагмента*:

```cpp
for (i = 0; i + 1 < local_size; ++i)
    if block[i] > block[i+1]:
        local_count++;
```

---

### 4.3 Учёт границ между блоками

Для процессов `rank > 0`:

```
если последний элемент предыдущего блока > первый элемент текущего:
    это тоже соседняя инверсия
```

Так корректно учитываются «стыки» между частями массива.

---

### 4.4 Сбор результатов

- `MPI_Reduce` суммирует все локальные значения на процессе 0.
- `MPI_Bcast` транслирует итог всем процессам (требование фреймворка PPC).

---

## 5. Структура проекта

```
tasks/lifanov_k_adj_inv_count
├── common
│   └── include
│       └── common.hpp
├── data
│   └── tests.json
├── mpi
│   ├── include
│   │   └── ops_mpi.hpp
│   └── src
│       └── ops_mpi.cpp
├── seq
│   ├── include
│   │   └── ops_seq.hpp
│   └── src
│       └── ops_seq.cpp
├── tests
│   ├── functional
│   │   └── main.cpp
│   └── performance
│       └── main.cpp
├── info.json
├── report.md
└── settings.json
```

---

## 6. Экспериментальная среда

- ОС: DevContainer / Ubuntu 24.04  
- Компилятор: GCC 14 (C++23)  
- MPI: OpenMPI 4.1.6  
- CPU: Intel Core i5‑12450H  
- RAM: 16 GB  
- Режим сборки: **Release**

---

## 7. Результаты

### 7.1 Корректность

Функциональные тесты проверяли:

- возрастающие массивы;
- полностью убывающие;
- массивы с повторяющимися элементами;
- отрицательные числа;
- случайные данные;
- короткие массивы (1, 2);
- корректность учёта границ между блоками;
- равенство результатов SEQ и MPI.

Все тесты успешно пройдены.

---

## 7.2 Производительность

Длина массива: `100000000`.

| Режим | Процессы | Время (с)        | Ускорение | Эффективность |
|-------|----------|------------------|-----------|---------------|
| SEQ   |   1      | **0.0221287**    | 1.00      | N/A           |
| MPI   |   2      | **0.0166971**    | 1.33      | 66%           |
| MPI   |   4      | **0.0127035**    | 1.74      | 43%           |

### Интерпретация

- ускорение наблюдается, но эффект ограничивается накладными расходами MPI;
- масштабирование замедляется из-за `Scatterv`, `Reduce` и синхронизаций;
- оптимальный режим — **2–4 процесса**, дальнейшее увеличение малоэффективно.

---

## 8. Заключение

В ходе работы:

- разработан последовательный и параллельный (MPI) алгоритм подсчёта соседних инверсий;
- достигнуто ускорение до 1.74× при использовании 4 процессов;
- SEQ и MPI реализации полностью согласованы по результатам;
- показано, что прирост производительности ограничивается затратами на коммуникации.

Работа демонстрирует принципы эффективного распараллеливания линейных алгоритмов и особенности MPI-программирования.

---

## 9. Источники

1. Куликов Г. — «Параллельное программирование».  
2. Peter S. Pacheco — *Parallel Programming with MPI*.  
3. Kumar V. — *Introduction to Parallel Computing*, 2nd edition.

