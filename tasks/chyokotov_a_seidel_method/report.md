# <Итеративный метод Гаусса-Зейделя>

- Student: Чёкотов Алексей Павлович, группа 3823Б1ФИ2
- Technology: SEQ | MPI
- Variant: 19

## 1. Introduction
При увеличении размеров матриц до тысяч строк и столбцов последовательные вычисления становятся вычислительно затратными, что создает потребность в эффективных стратегиях распараллеливания.
Ожидается, что MPI-реализация обеспечит ускорение по сравнению с последовательной версией при сохранении высокой эффективности использования вычислительных ресурсов.

## 2. Problem Statement
Необходимо найти решение матричного уравнения **Ax = b** итеративным методом Гаусса-Зейделя.
Входные данные: Матрица и вектор: pair\<vector\<vector\<double>>, vector\<double>>
Выходные данные: Массив vector\<double>

Входные данные должны представлять собой квадратную невырожденную матрицу размера n×n и вектор правой части размером n, где матрица обладает строгим диагональным преобладанием (модуль каждого диагонального элемента больше суммы модулей остальных элементов той же строки) для гарантии сходимости метода Гаусса-Зейделя. 
Также все строки матрицы должны иметь одинаковую длину, а размер вектора должен совпадать с размером матрицы.

## 3. Baseline Algorithm (Sequential)
1. Получаем входные данные.
2. Запускаем цикл, ограниченный 1000 итерациями.
- Для каждого i берем значение b[i] в цикле.
- Для всех других переменных j (кроме i-й) берём текущее значение х[j], умножаем на коэффициент matrix[i][j] и вычитаем.
- Делим на диагональный А[i][i].
- Сразу же обновляем значение х[i] этим новым результатом.
3. Проверяем сходимость текущего результата, завершаем цикл при получении необходимой точности решения.

## 4. Parallelization Scheme
1. Получаем входные данные и количество процессов.
2. Нулевой процесс рассылает построчно матрицу А и вектор b другим процессам.
3. Каждый процесс высчитывает свои значения участка вектора x.
4. Вычисляем для каждого процесса: сколько значений он вычислил и какое смещение должно быть в итоговом векторе.
5. Собираем полученный вектор х со всех процессов для последующих вычислений.
6. Вычисляем сходимость текущего результата на каждом процессе и собираем значение ошибки.
7. Завершаем цикл при получении необходимого уровня сходимости или при достижении максимального числа итераций.

## 5. Implementation Details
Структура проекта:
- mpi версия: tasks\chyokotov_a_seidel_method\mpi\
- Последовательная версия: tasks\chyokotov_a_seidel_method\seq\
- Функциональные тесты: tasks\chyokotov_a_seidel_method\tests\functional\
- Производительность: tasks\chyokotov_a_seidel_method\tests\performance\

Для рассылки строк матрицы используется MPI_Send(), для приема данных MPI_Recv().
Для сбора и объеденения данных всех процессов используется MPI_Allgatherv() и MPI_AllReduce().

Кроме потребления памяти на входные и выходные данные в mpi версии используются:
- Массивы у каждого процесса для вычисления минимальных значений в своих столбцах.
- Два массива для вычисления количества строк, в которых процесс проводил вычисления, и смещения в итоговом векторе.

## 6. Experimental Setup
- CPU - AMD Ryzen 5 5500U with Radeon Graphics(2.10 GHz)
- 6 cores, 12 treads.
- OS: Windows 11, 25Н2 version
- compiler: clang version 21.1.0 build type Release
- Data: Генерация матрицы 5000*5000.

## 7. Results and Discussion

### 7.1 Correctness
Функциональные тесты для проверки на корректность:
- Пустая матрица
- Матрица 1*1
- Матрица 2*2
- Матрица с отрицательными числами
- Матрица с дробными числами

### 7.2 Performance
Тест на производительность выполнялся на матрице размером 5000*5000 и векторе размера 5000.

|   Mode  | Count | Time, s | Speedup | Efficiency |
|---------|-------|---------|---------|------------|
|   seq   |   1   | 1.6793  |  1.00   |    N/A     |
|   mpi   |   2   | 2.4296  |  0.69   |   34.6%    |
|   mpi   |   4   | 3.0129  |  0.56   |   13.9%    |
|   mpi   |   6   | 3.7545  |  0.45   |    7.5%    |

Наблюдается замедление с ростом числа процессов, а эффективность постепенно снижается из-за роста наклодных расходов для коммуникации между процессами. На каждой итерации необходимо синхронизировать вычисленные данные для последующей итерации что замедляет выполнение.

## 8. Conclusions
Были реализованы SEQ и MPI версии алгоритма и в результате сравнения их работы выяснилось, что решение плохо масштабируется для задачи.

## 9. References
1. [Документация OpenMPI](https://www.open-mpi.org/doc/)
2. [стандарт с++](https://www.open-std.org)