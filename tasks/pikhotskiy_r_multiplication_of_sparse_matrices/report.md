# Умножение разреженных матриц. Элементы типа double. Формат хранения матрицы – строковый (CRS).

- Student: Пихотский Роман Владимирович, group 3823Б1ФИ1
- Technology: SEQ | MPI
- Variant: 4

## 1. Введение

Задача умножения разреженных матриц является важной в научных вычислениях, где матрицы часто содержат большое количество нулевых элементов. Использование формата CRS (Compressed Row Storage) позволяет эффективно хранить и обрабатывать такие матрицы, экономя память и вычислительные ресурсы. В данной работе реализовано умножение разреженных матриц с горизонтальной схемой распределения: матрица $A$ делится между процессами по строкам, матрица $B$ транспонируется и рассылается всем процессам для эффективного доступа к столбцам.

## 2. Постановка задачи

**Формальная постановка:** для разреженных матриц $A \in \mathbb{R}^{m \times k}$ и $B \in \mathbb{R}^{k \times n}$ в формате CRS вычислить $C = A \times B$.

**Входные данные:**
- две разреженные матрицы в формате CRS, каждая содержит:
  - `rows`, `cols` — размерности матрицы;
  - `values` — массив ненулевых значений типа `double`;
  - `col_indices` — индексы столбцов для каждого ненулевого значения;
  - `row_ptr` — указатели на начало каждой строки (размер = rows + 1).

**Выходные данные:**
- результирующая матрица $C$ в формате CRS.

**Ограничения и допущения:**
- $m, k, n > 0$, выполняется совместимость размеров: `A.cols == B.rows`;
- размеры массивов `row_ptr` соответствуют заявленным размерностям;
- MPI-версия работает при любом числе процессов.

## 3. Последовательная версия

SEQ-вариант использует транспонирование матрицы $B$ для эффективного вычисления скалярных произведений строк $A$ и столбцов $B$.

- **Validation:** проверка совместимости размеров ($A.cols = B.rows$), положительности размерностей, корректности формата CRS (размер `row_ptr`).
- **PreProcessing:** сохранение матрицы $A$, транспонирование матрицы $B$ в формат CRS.
- **Run:** для каждой пары (строка $i$ матрицы $A$, столбец $j$ матрицы $B$) вычисляется скалярное произведение с использованием merge-подобного алгоритма пересечения отсортированных индексов столбцов.
- **PostProcessing:** отсутствует.

### Алгоритм умножения

```pseudocode
for i in 0..A.rows-1:
    for j in 0..B.cols-1:
        sum = 0
        a_idx = A.row_ptr[i]
        bt_idx = B_T.row_ptr[j]
        while a_idx < A.row_ptr[i+1] and bt_idx < B_T.row_ptr[j+1]:
            if A.col_indices[a_idx] == B_T.col_indices[bt_idx]:
                sum += A.values[a_idx] * B_T.values[bt_idx]
                a_idx++; bt_idx++
            else if A.col_indices[a_idx] < B_T.col_indices[bt_idx]:
                a_idx++
            else:
                bt_idx++
        if |sum| > eps:
            C.values.push(sum)
            C.col_indices.push(j)
    C.row_ptr[i+1] = C.values.size()
```

## 4. Параллельная версия (MPI)

### 4.1. Распределение данных

- Матрица $A$ и транспонированная $B^T$ рассылаются всем процессам через `MPI_Bcast`.
- Строки матрицы $A$ распределяются между процессами: `base_rows = m / P`, первые `m mod P` рангов получают на одну строку больше.
- Каждый процесс вычисляет свой диапазон строк результата.

### 4.2. Локальное вычисление

- Каждый процесс умножает свои `local_rows` строк матрицы $A$ на транспонированную $B^T$.
- Используется тот же merge-подобный алгоритм пересечения индексов, что и в SEQ-версии.

### 4.3. Сбор и распространение результата

- Локальные результаты собираются на ранге 0 через `MPI_Gatherv`:
  - количество ненулевых элементов (`nnz`);
  - значения и индексы столбцов;
  - указатели строк с корректировкой смещений.
- Итоговая матрица рассылается всем процессам через `BroadcastSparseMatrix`.

### 4.4. Псевдокод MPI-версии

```pseudocode
RunImpl():
    rank, P = MPI_Comm_rank/size
    
    base_rows = A.rows / P
    extra_rows = A.rows % P
    my_start_row = rank * base_rows + min(rank, extra_rows)
    my_num_rows = base_rows + (rank < extra_rows ? 1 : 0)
    
    local_result = SparseMatrixCRS(my_num_rows, B.cols)
    
    for local_i in 0..my_num_rows-1:
        global_i = my_start_row + local_i
        for j in 0..B.cols-1:
            sum = merge_multiply(A.row[global_i], B_T.row[j])
            if |sum| > eps:
                local_result.values.push(sum)
                local_result.col_indices.push(j)
        local_result.row_ptr[local_i+1] = local_result.values.size()
    
    GatherResults(local_result, my_num_rows)
    BroadcastSparseMatrix(result, root=0)
    return true
```

## 5. Детали реализации

### 5.1. Файловая структура проекта

`tasks/pikhotskiy_r_multiplication_of_sparse_matrices/`:
- `common/include/common.hpp` — структура `SparseMatrixCRS`, типы `InType`/`OutType`, вспомогательные функции `DenseToCRS`, `CRSToDense`, `TransposeCRS`, `CompareSparseMatrices`.
- `seq/include/ops_seq.hpp`, `seq/src/ops_seq.cpp` — последовательный таск: валидация, транспонирование $B$, merge-умножение.
- `mpi/include/ops_mpi.hpp`, `mpi/src/ops_mpi.cpp` — MPI-реализация с `BroadcastSparseMatrix`, горизонтальным распределением строк, локальным умножением и `GatherResults`.
- `tests/functional/main.cpp` — 12 функциональных тестов (единичные, диагональные, прямоугольные матрицы, различная разреженность).
- `tests/performance/main.cpp` — перф-тест: матрицы $300 \times 300$ с плотностью 10%.

### 5.2. Ключевые классы и функции

- `SparseMatrixCRS` — структура для хранения разреженной матрицы в формате CRS.
- `ComputeRowColProduct` — вычисление скалярного произведения строки $A$ и столбца $B$ (через транспонированную $B^T$).
- `TransposeCRS` — транспонирование CRS-матрицы за $O(nnz)$.
- `DenseToCRS` / `CRSToDense` — конвертация между плотным и разреженным форматами.
- `CompareSparseMatrices` — сравнение матриц с допуском $\varepsilon = 10^{-9}$.
- `BroadcastSparseMatrix` — рассылка CRS-матрицы через MPI (`MPI_Bcast` для размеров, значений, индексов и указателей).
- `GatherResults` — сбор локальных результатов через `MPI_Gather` и `MPI_Gatherv` с корректировкой смещений `row_ptr`.

### 5.3. Использование памяти

- **SEQ:** хранит матрицы $A$, $B^T$ и результат $C$; дополнительная память — только для транспонирования.
- **MPI:** каждый ранг хранит полные $A$ и $B^T$ (для доступа к любой строке), локальный результат (`local_rows` строк); ранг 0 дополнительно хранит собранный результат.

## 6. Экспериментальное окружение

**Аппаратная платформа / ОС:**
- CPU: AMD Ryzen 5 5500U (6 cores, 12 threads, 2.1 GHz);
- RAM: 12 GB DDR4;
- ОС: Windows 10 + WSL (Ubuntu).

**Инструменты разработки:**
- Компилятор: `g++ 13.3.0`;
- MPI: `OpenMPI`;
- Сборка: `CMake`.

**Параметры тестирования:**
- число процессов: 6 (`mpirun -n 6 ...`);
- размер матриц: $300 \times 300$;
- плотность ненулевых элементов: 10%.

## 7. Результаты и обсуждение

### 7.1. Корректность

- 12 функциональных тестов покрывают:
  - базовые случаи: единичный элемент ($1 \times 1$), единичная матрица, нулевая матрица;
  - диагональные и разреженные матрицы с различными паттернами;
  - прямоугольные матрицы: $2 \times 3 \cdot 3 \times 2$, $3 \times 2 \cdot 2 \times 3$;
  - векторные операции: строка $\times$ столбец, столбец $\times$ строка;
  - матрицы с разреженностью 20-30%.
- Эталон вычисляется наивным плотным умножением с последующей конвертацией в CRS.
- Допускается погрешность $10^{-9}$.

### 7.2. Производительность

| Mode | Time, s | Speedup | Efficiency |
|------|---------|---------|------------|
| seq, pipeline | 0.003920 | 1.00 | N/A |
| mpi, pipeline | 0.001133 | 3.46 | 57.7% |
| seq, task_run | 0.002896 | 1.00 | N/A |
| mpi, task_run | 0.000791 | 3.66 | 61.0% |

**Интерпретация:**
- MPI-версия показывает ускорение в 3.5-3.7 раза при использовании нескольких процессов.
- Эффективность ~60% объясняется накладными расходами на рассылку полных матриц $A$ и $B^T$ всем процессам, а также сбор результатов.
- Для больших матриц с высокой разреженностью ожидается лучшее масштабирование, так как доля вычислений относительно коммуникаций возрастает.
- Режимы `pipeline` и `task_run` показывают схожие результаты.
 
## 8. Заключение

Реализованы SEQ и MPI-варианты умножения разреженных матриц в формате CRS с горизонтальной схемой распределения. MPI-версия распределяет строки матрицы $A$ между процессами, использует транспонированную $B^T$ для эффективного доступа к столбцам и собирает результат через `MPI_Gatherv`. Достигнуто ускорение в ~3.5 раза.

**Потенциальные улучшения:**
- Распределение только необходимых строк $A$ каждому процессу вместо рассылки полной матрицы.
- Использование неблокирующих коммуникаций для перекрытия вычислений и передачи данных.
- Адаптивная балансировка нагрузки с учётом количества ненулевых элементов в строках.

## 9. Источники

1. Microsoft MPI : документация [Электронный ресурс] // Microsoft Learn. – URL: https://learn.microsoft.com/ru-ru/message-passing-interface/microsoft-mpi (дата обращения: 26.12.2025).

2. Сысоев А. В. Курс лекций по параллельному программированию