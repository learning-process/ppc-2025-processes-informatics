# Нахождение наиболее отличающихся по значению соседних элементов вектора

- Студент: Круглова Алёна Витальевна, группа 3823Б1ФИ2
- Технология: SEQ | MPI 
- Вариант: 8

## 1. Введение
Задача нахождения максимальной разницы между соседними элементами вектора является типичной операцией анализа данных, применяемой в финансовых индикаторах, обработке сигналов и системах мониторинга. Несмотря на кажущуюся простоту, она хорошо иллюстрирует типовые приёмы распараллеливания: разбиение данных и коллективные операции над частичными результатами. В рамках лабораторной работы реализованы последовательная и параллельная версии алгоритма и проведён их сравнительный анализ.

## 2. Постановка задачи
**Формальная постановка:** Для заданного входного вектора $V$ из вещественных чисел (std::vector<float>) длиной `N`, требуется найти максимальную абсолютную разность между любыми двумя соседними элементами. 

**Входные данные:** Вектор `V` (std::vector<float>) произвольной длины.

**Выходные данные:** Вещественное число (float) — значение максимальной разности.

**Ограничения:**
- `N ≥ 0`, алгоритм должен корректно работать и при `N < 2`
- MPI‐версия должна поддерживать произвольное количество процессов

## 3. Последовательная версия(Baseline)
Последовательная версия проходит по массиву один раз и сравнивает каждый элемент с предыдущим:
```cpp
float max_diff = std::abs(vec[1] - vec[0]);
for (size_t i = 1; i < vec.size(); ++i) {
    float diff = std::abs(vec[i] - vec[i - 1]);
    max_diff = std::max(diff, max_diff);
}
```
- временная сложность: `O(N)`
- память: `O(1)` дополнительная

## 4. Параллельная версия
### 4.1. Разделение данных
 Массив делится на `size` блоков с равномерным распределением остатка:
- `base = N / size`
- `rem = N % size`
- процесс `i` получает: `base + (i < rem ? 1 : 0)` элементов
- для корректного вычисления разницы между блоками добавляется один элемент перекрытия, если блок не упирается в конец массива
 
### 4.2. Взаимодействие процессов
1. **Синхронизация параметров**  
   Процесс 0 определяет размер входного вектора $N$ и выполняет широковещательную рассылку с использованием `MPI_Bcast`.
2. **Распределение данных между процессами**  
   Формируются массивы `sendcounts` и `displs` с учетом перекрытия соседних блоков. Распределение данных осуществляется посредством `MPI_Scatterv`.
3. **Выполнение локальных вычислений**  
   Каждый процесс вычисляет максимальную разность в своем блоке данных функцией `LocalMaxDiff`. Перекрытие блоков гарантирует корректный учет граничных элементов.
4. **Редукция результатов**  
   Локальные максимумы собираются на корневом процессе операцией `MPI_Reduce` с операцией `MPI_MAX` для определения глобального максимума.
5. **Распространение результата**  
   Итоговое значение глобального максимума рассылается всем процессам с помощью `MPI_Bcast`.

### 4.3. Псевдокод алгоритма
```pseudocode
MPI_Init()
Получить rank и size

Если rank == 0: n = длина массива
MPI_Bcast(n)
Если n < 2: вернуть 0

Если rank == 0: вычислить sendcounts и displs с учётом перекрытия
MPI_Scatterv(массив → local_vec)

local_max = max |local_vec[i] - local_vec[i-1]|

MPI_Reduce(local_max → global_max, MAX)
MPI_Bcast(global_max)

return global_max
```
## 5. Детали реализации

### 5.1. Файловая структура проекта
kruglova_a_max_diff_adjacent/  
├── common/include/common.hpp  
├── seq/include/ops_seq.hpp  
├── seq/src/ops_seq.cpp  
├── mpi/include/ops_mpi.hpp  
├── mpi/src/ops_mpi.cpp  
├── tests/functional/main.cpp  
├── tests/performance/main.cpp  
└── data/  

### 5.2. Ключевые классы и функции
- **InType** = `std::vector<float>` — входной вектор
- **OutType** = `float` — выходное значение (максимальная разность)
- **`RunImpl()`** — реализация основного алгоритма
- **`LocalMaxDiff()`** — локальный поиск максимума
- **`ValidationImpl()`**,**`PreProcessingImpl()`**, **`PostProcessingImpl()`** — инфраструктурные методы
- **Тестовые классы:**
    - **`KruglovaAMaxDiffAdjacentSEQ`** — последовательная версия
    - **`KruglovaAMaxDiffAdjacentMPI`** — параллельная MPI версия

### 5.3. Использование памяти

**В SEQ версии**хранится весь массив.
**В MPI версии:**
- процесс 0 хранит весь массив и служебные структуры
- остальные процессы — только локальные участки

## 6. Экспериментальное окружение
Экспериментальные исследования проводились на вычислительной системе с процессором **AMD Ryzen 5 5500U**, содержащим 6 вычислительных ядер с поддержкой одновременной многопоточности. Конфигурация платформы включает **16** гигабайт оперативной памяти DDR4 и твердотельный накопитель объемом 512 ГБ под управлением операционной системы **Windows 10**.

Вычислительный комплекс использует среду разработки Microsoft Visual Studio 2019 со встроенным компилятором C++. Параллельные вычисления организованы посредством реализации **Microsoft MPI версии 10.0.12498.5**, а сборка проекта осуществляется с помощью системы **CMake 3.30.4** в режиме **release**.

Тестовые сценарии исследуют масштабируемость при различных конфигурациях: выполнение в одном процессе, распределение на 2, 4 и 6 процессов. Тестирование корректности и производительности проводилось на массиве из **4 000 000** элементов.

## 7. Результаты

### 7.1 Корректность
Корректность подтверждена с помощью Google Test путем сравнения результата MPI версии с результатом эталонной последовательной версии (KruglovaAMaxDiffAdjacentSEQ). Проверка проходит для всех тестовых сценариев, включая случай `N<2` и большие сгенерированные данные.

### 7.2 Производительность
Время выполнения для вектора длиной 4 000 000 символов:

| Mode        | Count | Time, s | Speedup | Efficiency |
|-------------|-------|---------|---------|------------|
| seq         | 1     | 0.0098  | 1.00    | N/A        |
| mpi         | 2     | 0.0083  | 1.18    | 59.0%      |
| mpi         | 4     | 0.0067  | 1.46    | 36.5%      |
| mpi         | 6     | 0.0063  | 1.55    | 25.8%      |

**Анализ результатов:**
Проведённые измерения показывают, что MPI-версия демонстрирует ускорение относительно последовательной реализации, достигая 1.55× при 6 процессах. Однако эффективность снижается с ростом числа процессов — с 59% до 25.8%, что связано с доминированием операций над вычислениями.
Поскольку задача вычислительно простая и имеет низкую вычислительную плотность, накладные расходы на `MPI_Scatterv` и `MPI_Reduce` существенно ограничивают масштабируемость. В итоге MPI-реализация даёт умеренное ускорение, но её эффективность быстро падает при увеличении числа процессов

## 8. Заключение
Разработаны и протестированы последовательная и параллельная (MPI) реализации алгоритма поиска максимальной разности между соседними элементами вектора.
Успешно реализована стратегия **блочного разделения с нахлестом (Halo)**, обеспечивающая корректный расчет граничных разностей. Для сбора результата использована эффективная коллективная операция `MPI_Reduce` с `MPI_MAX`.
Анализ производительности подтверждает, что данная задача ограничена пропускной способностью памяти и накладными расходами на коммуникацию, что является важным выводом при проектировании параллельных систем.

## 9. Источники
1. Microsoft MPI : документация [Электронный ресурс] // Microsoft Learn. – URL: https://learn.microsoft.com/ru-ru/message-passing-interface/microsoft-mpi (дата обращения: 12.11.2025).
2. Сысоев А. В. Курс лекций по параллельному программированию
3. Нестеров А.Ю и Оболенский А.А Практические занятия