# Отчёт

# Подсчёт частоты символа в строке
- Студент: Тельнов Анатолий Викторович, группа 3823Б1ФИ1
- Технология: SEQ-MPI
- Вариант: 23

## 1. Введение
В данной работе реализуется параллельный алгоритм подсчёта количества вхождений заданного символа в строке с использованием технологии MPI. Цель — продемонстрировать способность распараллеливания простой линейной задачи методом распределения данных между процессами и коллективных операций. Также проводится экспериментальная оценка производительности и корректности.

## 2. Постановка задачи
Дана строка S длины N. Требуется определить, сколько раз символ c встречается в этой строке.
Входные данные:
- Строка S, произвольной длины.
- Символ c, который требуется посчитать.
Выходные данные:
- Целое число k — количество вхождений символа в строку.
Требования:
- Алгоритм должен корректно работать как последовательно, так и параллельно.
- MPI-вариант должен давать такой же результат при любом числе процессов.
- Ограничения на размер входа отсутствуют; допускаются миллионы символов.

## 3. Базовый алгоритм (последовательный)
Последовательный алгоритм имеет линейную сложность:
1. Инициализировать счётчик count = 0.
2. Обойти строку слева направо.
3. Если S[i] == c, увеличить счётчик.
4. Вернуть итоговый результат.

## 4. Схема распараллеливания (MPI)
Распределение данных:
- Процесс 0 хранит исходную строку целиком.
- Строка делится на P примерно равных блоков (P — число процессов).
- С помощью MPI_Scatterv каждому процессу отправляется его подстрока.
- Каждый процесс считает количество совпадений локально.
Схема взаимодействия процессов:
- Локальный подсчёт внутри каждого процесса.
- Глобальное суммирование выполняется коллективной операцией:

`MPI_Allreduce(&local, &global, 1, MPI_LONG_LONG, MPI_SUM, MPI_COMM_WORLD);`

Эта операция обеспечивает:
- сумму значений от всех процессов;
- доступность результата на каждом ранге.

Топология:
- Используется стандартный коммуникатор MPI_COMM_WORLD.
- Процессы равноправны, результат известен каждому процессу.

Псевдокод:
`local = count(block)`
`global = Allreduce(local)`
`return global`

Использование MPI_Allreduce устраняет необходимость явного MPI_Bcast и предотвращает ошибки порядка операций.

## 5. Детали реализации
Структура кода:
- ops_seq.cpp — последовательный вариант.
- ops_mpi.cpp — MPI-вариант алгоритма.
- ops_seq.hpp / ops_mpi.hpp — объявления классов задач.
- common.hpp — базовые определения и интерфейсы.
- func_test_util.hpp — функциональные тесты.
- main.cpp — запуск тестов.
Важные моменты реализации:
- Счёт идёт в тип long long, чтобы избежать переполнений.
- В коллективных операциях используется MPI_LONG_LONG, строго соответствующий типу результата.
- Применение MPI_Allreduce обеспечивает корректность на всех рангах.
Частные случаи:
- Пустая строка.
- Строка короче, чем число процессов.
- Неравномерное распределение символов.
- Отсутствие символа c.
Память:
- Каждый процесс хранит только свой участок строки (O(N/P)).
- Дополнительных крупных буферов не создаётся.

## 6. Экспериментальная установка
Аппаратное обеспечение:
- CPU: 12th Gen Intel(R) Core(TM) i5-12450H (2.00 GHz)(8 ядер / 12 потоков)
- RAM: 16 ГБ
- OS: Windows 11 Pro x64
- MPI: Microsoft MPI (MS-MPI) 10.1
Инструменты:
- Сборщик: CMake
- Компилятор: MSVC 19.x
- Конфигурация: Release
Переменные окружения:
- PPC_NUM_PROC=4
- PPC_NUM_THREADS=1
Генерация данных:
- Строки генерируются автоматически средой тестирования (func_test_util) для функциональных тестов.

## 7. Результаты и обсуждение

### 7.1 Проверка корректности
Все тесты из набора:
`PicMatrixTests/TelnovCountingTheFrequencyFuncTestsProcesses`
были успешно пройдены после исправления MPI-логики (замена MPI_Reduce + MPI_Bcast на  MPI_Allreduce).

Подтверждена корректность:
[X] совпадение с результатом последовательного алгоритма
[X] одинаковый результат на всех процессах
[X] отсутствие гонок и ошибок памяти

### 7.2 Производительность

| Mode | Count  | Time, s | Speedup | Efficiency |
|------|--------|---------|---------|------------|
| seq  | 1      | 0.112   | 1.00    | N/A        |
| mpi  | 2      | 0.060   | 1.87    | 93.5%      |
| mpi  | 4      | 0.030   | 3.73    | 93.3%      |

Обсуждение результатов:
- Для больших данных ускорение близко к линейному.
- На коротких строках влияние накладных расходов MPI заметно сильнее.
- Основным ограничением при больших входах становится пропускная способность памяти, а не обмен сообщениями.

## 8. Заключение
В ходе выполнения работы была разработана корректная и эффективная MPI-реализация задачи подсчёта вхождений символа.
Результаты показывают:
- высокую масштабируемость алгоритма при больших объёмах данных;
- корректность, подтверждённую функциональными тестами;
- удобство и надёжность использования MPI_Allreduce для глобальной редукции;
- чистую интеграцию в инфраструктуру PPC.
Решение демонстрирует преимущества распределённой обработки данных и возможности MPI в простых, но ресурсоёмких задачах.

## 9. References
1. Стандарт MPI - https://legacyupdate.net/download-center/download/57467/microsoft-mpi-v10.0
2. Документация Microsoft MPI — https://learn.microsoft.com/en-us/message-passing-interface
3. cppreference.com — Справочник по C++
4. Внутренняя документация PPC-Framework

## Appendix (Optional)

# Основной фрагмент MPI-кода

`long long local = 0;`
`for (size_t i = start; i < end; i++)`
    `if (s[i] == target) local++;`
`long long global = 0;`
`MPI_Allreduce(&local, &global, 1, MPI_LONG_LONG, MPI_SUM, MPI_COMM_WORLD);`
`GetOutput() = global;`
