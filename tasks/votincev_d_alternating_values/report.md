# Отчет по реализации алгоритма нахождения числа чередований знаков значений соседних элементов вектора

**Дисциплина:** Параллельное программирование  
**Преподаватель:** Нестеров Александр Юрьевич и Оболенский Арсений Андреевич  
**Студент:** Вотинцев Дмитрий Сергеевич 3823Б1ФИ3
**Вариант:** 5

## Введение
В рамках данной работы был реализован алгоритм поиска чередований знаков значений соседних элементов вектора: последовательная и параллельная реализация.

## Постановка задачи
Дан вектор v из N элементов. Необходимо найти число чередований знаков между соседними элементами.
Рассмотрим элементы v[i-1] и v[i] , i = 1,2,...,N.
Если v[i-1] >= 0  и  v[i] < 0 - то это чередование
Если v[i-1] < 0  и  v[i] >= 0 - то это чередование
(0 считается беззнаковым, грубо говоря +0)

## Описание алгоритма

Последовательная версия:
проходимся по всему массиву, сравниваем соседние элементы на чередование, увеличиваем счетчик чередований (если между соседями знак чередуется). 


## Описание схемы параллельного алгоритма
Пусть у нас N процессов, вектор размера M, тогда алгоритм такой:
0-й процесс делит массив на N частей. 
Части выделяются следующим образом:
Сначала считаем минимум, который будет каждый процесс обрабатывать:
base = M/N  
Затем вычисляем остаток от деления:
remain = M % N
Первые remain процессов заберут по 1 элементу из массива, остальные отработают только минимум.
Однако так работать не будет, вот пример:
Пусть 2 процесса и вектор {0, -1, 2, -3}
Тогда:
0й процесс возьмет 0 -1  -> всего 1 чередование
1й процесс возьмет 2 -3  -> всего 1 чередование
И ответ будет 2 - что неверно, ведь чередований 3.
Поэтому нужно ещё цеплять правого соседа:
0й возьмет 0 -1 2
1й возьмёт 2 -3 ?
Да, если просто брать правого соседа - то последний процесс всегда будет заходит за память, поэтому последний процесс будет брать на 1 элемент меньше.

Итого кратко:
вычисляем минимальную часть M/N;
первые remain процессов получают +1 элемент вектора к обработке;
всем процессам добавляем +1 чтобы цепляли правого соседа;
последнему процессу делаем -1 чтобы он не выходил за память.

Но что подразумевается под "0й процесс делит на части вектор и распределяет" ?
Можно сделать вот как: 
дается исходный вектор v
0й процесс делит его на части (прям поэлементно);
0й процесс распряделяет ЧАСТИ вектора другим процессам;
другие процессы создают внутри себя вектора, которые являются частью исходного.

Это и была моя первая реализация алгоритма - простая, но давала ускорение в 0.5.
То есть SEQ работал быстрее MPI всегда.
Проблема была не в делении на части (ведь она оптимальная) - а в памяти.
Проблема передачи памяти заключалась в том, что 0й процесс посылал части вектора - по сути копирование.
Но можно посылать индекс начала + кол-во элементов для обработки - что намного лучше и действительно дает ускорение.


## Результаты экспериментов и выводы

Функциональные тесты содержат 6 тестов:
Тест1: отсутствие чередований
Тест2: правильная обработка 0
Тест3: максимум чередований
Тест4: 1 элемент
Тест5: пустой вектор
Тест6: случайные значения

Тест на производительность - данные генерируются как в Тест3 (максимум чередований).
То есть так: 0 -1 2 -3 4 -5 ...
Почему именно так? Потому что в таком случае при запуске на разном количестве процессов делиться вектор на части будет по-разному.
Это самый "проблемный" случай, если вектор делится неправильно, поэтому его я и выбрал.

Ниже таблица с результатами Perf тестов.

В таблице представлено: n - размер вектора, время выполнения SEQ и MPI версии для 4-х процессов (в секундах):

| Размер данных (n) | SEQ версия (с) | MPI версия (с) | Ускорение |
|-------------------|----------------|----------------|-----------|
| 100               | 0.000000       | 0.0013         | 0.00×     |
| 10 000            | 0.000006       | 0.0018         | 0.003×    |
| 1 000 000         | 0.0039         | 0.0059         | 0.66×     |
| 10 000 000        | 0.821          | 0.457          | 1,862×    |

При замерах (Perf тестах) была получена данная таблица.
Можно заметить, что на 4 процессах ускорение не 4, как хотелось бы,а всего 1,862x.
С увеличением размера данных будет повышаться эффективность MPI версии, но идеальное ускорение 4x недостижимо. Для достижения ускорения 4х процессы должны посылать Recv, Send так, чтобы время ожидания было минимальным, сама посылка данных должна происходить мгновенно и т.д. - но это невозможно.


Вообще, до этого таблица была такой:

| Размер данных (n) | SEQ версия (с) | MPI версия (с) | Ускорение |
|-------------------|----------------|----------------|-----------|
| 100               | 0.000000       | 0.0013         | 0.00×     |
| 10 000            | 0.000006       | 0.0018         | 0.003×    |
| 1 000 000         | 0.0039         | 0.0059         | 0.66×     |
| 10 000 000        | 0.0330         | 0.0068         | 4.85×     |

Я замерял с помощью MPI_Wtime и std::chrono.
Даже если в MPI версии выводить результат только 0-м процессом,то вывод такой примерно:
MPI was working: 0.152748
MPI was working: 0.0263676
MPI was working: 0.0264405
MPI was working: 0.026304
MPI was working: 0.0263244
MPI was working: 0.0223208

Что интересно - SEQ версия выводит тоже не 1 раз:
SEQ was working:0.130335 seconds
SEQ was working:0.122825 seconds
SEQ was working:0.106342 seconds
SEQ was working:0.102996 seconds
SEQ was working:0.102507 seconds
.. и так ещё может строк 10-15

И если смотреть по 1-му значению вывода MPI версии и SEQ, то первая таблица даже как будто верна. А если смотреть на другие - то как раз и ускорение 4.85x (как во 2й таблице).
Мне показалось такое ускорение странным, поэтому я мерил по гугл тестам (сколько они писали ms).
С одной стороны, MPI и std::chrono очень разные, что могло и повлиять.
С другой стороны - почему настолько разные?
(как я замерял - осталось закомментированным в коде)
(не исключается возможность, что я мерил не так, как нужно было)


## Заключение
В результате проделанной работы были реализованы версии MPI, SEQ алгоритма нахождения числа чередований знаков значений соседних элементов вектора.
Было так же показано, что MPI версия работает быстрее SEQ на больших значениях, а на маленьких значениях - лучше использовать SEQ.


## Литература
1. Лекции Сысоева Александра Владимировича
2. Практические занятия Нестерова Александра Юрьевича и Оболенского Арсения Андреевича
3. Интернет

## Приложения (код параллельной реализации)

```
bool VotincevDAlternatingValuesMPI::RunImpl() {
  int all_swaps = 0;

  int process_n = 0;
  MPI_Comm_size(MPI_COMM_WORLD, &process_n);  // получаю кол-во процессов

  int proc_rank = 0;
  MPI_Comm_rank(MPI_COMM_WORLD, &proc_rank);  // получаю ранк процесса

  // если процессов больше, чем размер вектора
  const int vector_size = static_cast<int>(vect_data_.size());
  process_n = std::min(vector_size, process_n);

  if (proc_rank == 0) {
    all_swaps = ProcessMaster(process_n);  // главный процесс (распределяет + считает часть)
  } else {
    ProcessWorker();  // процессы-рабочие (только считают)
  }

  SyncResults(all_swaps);  // посылаю результат всем процессам
  return true;
}

```