# Отчет по реализации алгоритма нахождения числа чередований знаков значений соседних элементов вектора

**Дисциплина:** Параллельное программирование  
**Преподаватель:** Нестеров Александр Юрьевич и Оболенский Арсений Андреевич  
**Студент:** Вотинцев Дмитрий Сергеевич 3823Б1ФИ3
**Вариант:** 5

## Введение
В рамках данной работы был реализован алгоритм поиска чередований знаков значений соседних элементов вектора: последовательная и параллельная реализация.

## Постановка задачи
Дан вектор v из N элементов. Необходимо найти число чередований знаков между соседними элементами.
Рассмотрим элементы v[i-1] и v[i] , i = 1,2,...,N.
Если v[i-1] >= 0  и  v[i] < 0 - то это чередование
Если v[i-1] < 0  и  v[i] >= 0 - то это чередование
(0 считается беззнаковым, грубо говоря +0)

## Описание алгоритма

Последовательная версия:
проходимся по всему массиву, сравниваем соседние элементы на чередование, увеличиваем счетчик чередований (если между соседями знак чередуется). 


## Описание схемы параллельного алгоритма
Пусть у нас N процессов, вектор размера M, тогда алгоритм такой:
0-й процесс делит массив на N частей. 
Части выделяются следующим образом:
Сначала считаем минимум, который будет каждый процесс обрабатывать:
base = M/N
Затем вычисляем остаток от деления:
remain = M % N
Первые remain процессов заберут по 1 элементу из массива, остальные отработают только минимум.
Однако так работать не будет, вот пример:
Пусть 2 процесса и вектор {0, -1, 2, -3}
Тогда:
0й процесс возьмет 0 -1  -> всего 1 чередование
1й процесс возьмет 2 -3  -> всего 1 чередование
И ответ будет 2 - что неверно, ведь чередований 3.
Поэтому нужно ещё цеплять правого соседа:
0й возьмет 0 -1 2
1й возьмёт 2 -3 ?
Да, если просто брать правого соседа - то последний процесс всегда будет заходит за память, поэтому последний процесс будет брать на 1 элемент меньше.

Итого кратко:
вычисляем минимальную часть M/N;
первые remain процессов получают +1 элемент вектора к обработке;
всем процессам добавляем +1 чтобы цепляли правого соседа;
последнему процессу делаем -1 чтобы он не выходил за память.

Но что подразумевается под "0й процесс делит на части вектор и распределяет" ?
Можно сделать вот как: 
дается исходный вектор v
0й процесс делит его на части (прям поэлементно);
0й процесс распряделяет ЧАСТИ вектора другим процессам;
другие процессы создают внутри себя вектора, которые являются частью исходного.

Это и была моя первая реализация алгоритма - простая, но давала ускорение в 0.5.
То есть SEQ работал быстрее MPI всегда.
Проблема была не в делении на части (ведь она оптимальная) - а в памяти.
Проблема передачи памяти заключалась в том, что 0й процесс посылал части вектора - по сути копирование.
Но можно посылать индекс начала + кол-во элементов для обработки - что намного лучше и действительно дает ускорение.


## Результаты экспериментов и выводы
В таблице представлено: n - размер вектора, время выполнения SEQ и MPI версии для 4-х процессов (в секундах):

| Размер данных (n) | SEQ версия (с) | MPI версия (с) | Ускорение |
|-------------------|----------------|----------------|-----------|
| 100               | 0.000000       | 0.0013         | 0.00×     |
| 10 000            | 0.000006       | 0.0018         | 0.003×    |
| 1 000 000         | 0.0039         | 0.0059         | 0.66×     |
| 10 000 000        | 0.025568       | 0,0062703      | 4,077×    |

Само ускорение в 4,077 раз - выглядит достаточно неправдоподобно, нереалистично.
Ведь для 4-х процессов максимальное теоретическое ускорение это 4x.
Всё может быть связано с кешированием, возможно, компилятор векторизует параллельную версию, а версию SEQ не может (потому что нетривиально ходим по ветору, компилятор такое не любит). Возможно, SEQ версия не попадала в кеш (сначала работала MPI программа, потом запустился SEQ и пошли кеш промахи). Но Perf тесты показали такое ускорение. 
Алгоритм достаточно простой и хорошо распараллеливается - это основная причина ускорения.
Можно сделать выводы из экспериментов:
-При больших данных MPI версия дает существенное ускорение и показывает преимущество над SEQ.
-При небольших данных SEQ версия работает быстрее (за счет MPI операций, которые замедляют работу MPI версии, что существенно при маленьких размерах вектора)

 
## Заключение
В результате проделанной работы были реализованы версии MPI, SEQ алгоритма нахождения числа чередований знаков значений соседних элементов вектора.
Было так же показано, что MPI версия работает быстрее SEQ на больших значениях, а на маленьких значениях - лучше использовать SEQ.


## Литература
1. Лекции Сысоева Александра Владимировича
2. Практические занятия Нестерова Александра Юрьевича и Оболенского Арсения Андреевича
3. Интернет

## Приложения (код параллельной реализации)

# Код оптимальной реализации (передача индексов + размера)
```
bool VotincevDAlternatingValuesMPI::RunImpl() {
  // double start_time = 0, end_time = 0;
  // start_time = MPI_Wtime();

  int allSwaps = 0;
  // получаю кол-во процессов
  int ProcessN;
  MPI_Comm_size(MPI_COMM_WORLD, &ProcessN);

  // получаю ранг процесса
  int ProcRank;
  MPI_Comm_rank(MPI_COMM_WORLD, &ProcRank);

  // если процессов больше, чем элементов
  if (static_cast<int>(v.size()) < ProcessN) {
    ProcessN = static_cast<int>(v.size());
  }

  int partSize;
  if (ProcRank == 0) {
    int base = v.size() / ProcessN;    // минимум на обработку
    int remain = v.size() % ProcessN;  // остаток (распределим)

    int startId = 0;
    for (int i = 1; i < ProcessN; i++) {
      partSize = base;
      if (remain) {  // если есть остаток - то распределяем между первыми
        partSize++;
        remain--;
      }

      partSize++;  // цепляем правого соседа, 0-й будет последним - поэтому он будет последний кусок считать

      // Вместо пересылки данных - пересылаем индексы начала и конца
      int indices[2] = {startId, startId + partSize};
      MPI_Send(&indices[0], 2, MPI_INT, i, 0, MPI_COMM_WORLD);

      startId += partSize - 1;

      // вычисляю для последнего
      if (i == (ProcessN - 1)) {
        partSize = base + remain;
      }
    }

    // 0й процесс считает свою часть
    for (size_t j = startId + 1; j < v.size(); j++) {
      if ((v[j - 1] < 0 && v[j] >= 0) || (v[j - 1] >= 0 && v[j] < 0)) {
        allSwaps++;
      }
    }

    // получаю посчитанные swap от процессов
    for (int i = 1; i < ProcessN; i++) {
      int tmp;
      // что , сколько, тип, кому, тег, коммуникатор
      MPI_Recv(&tmp, 1, MPI_INT, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);
      allSwaps += tmp;
    }

    // на этом этапе 0-й процесс сделал всю работу
  }

  for (int i = 1; i < ProcessN; i++) {
    if (ProcRank == i) {
      // получаем индексы вместо данных
      int indices[2];
      MPI_Recv(indices, 2, MPI_INT, 0, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);

      int start_index = indices[0];
      int end_index = indices[1];

      int swapCount = 0;
      // обрабатываем свой диапазон из глобального вектора v
      for (int j = start_index + 1; j < end_index; j++) {
        if ((v[j - 1] < 0 && v[j] >= 0) || (v[j - 1] >= 0 && v[j] < 0)) {
          swapCount++;
        }
      }

      MPI_Send(&swapCount, 1, MPI_INT, 0, 0, MPI_COMM_WORLD);
    }
  }

  // только 0й процесс владеет правильным результатом, остальные - его формируют
  // если остальным посылать - будет проблема, на 20 процессах уже не работает
  if (ProcRank == 0) {
    GetOutput() = allSwaps;
  }

  
  MPI_Barrier(MPI_COMM_WORLD);
  // end_time = MPI_Wtime();

  // if (ProcRank == 0) {
  //   std::cout << "MPI_was_working:" << (end_time - start_time) << "\n";
  // }

  MPI_Bcast(&allSwaps, 1, MPI_INT, 0, MPI_COMM_WORLD);
  GetOutput() = allSwaps;
  MPI_Barrier(MPI_COMM_WORLD);

  // ========== пересылка, но не через Bcast
  //
  // if (ProcRank == 0) {
  //   // отправляем всем процессам корректный результат
  //   for (int i = 1; i < ProcessN; i++) {
  //     MPI_Send(&allSwaps, 1, MPI_INT, i, 0, MPI_COMM_WORLD);
  //   }
  //   // сами устанавливаем значение
  //   GetOutput() = allSwaps;
  // }
  // for (int i = 1; i < ProcessN; i++) {
  //   if (ProcRank == i) {
  //     int allSw;
  //     MPI_Recv(&allSw, 1, MPI_INT, 0, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);
  //     GetOutput() = allSw;
  //   }
  // }
  //
  // ========== пересылка, но не через Bcast

  return true;
}
```

# Код неоптимальной реализации (с передачей кусков векторов)
```
bool VotincevDAlternatingValuesMPI::RunImpl() {
  int allSwaps = 0;
  // получаю кол-во процессов
  int ProcessN;
  MPI_Comm_size(MPI_COMM_WORLD, &ProcessN);

  // получаю ранг процесса
  int ProcRank;
  MPI_Comm_rank(MPI_COMM_WORLD, &ProcRank);

  int partSize;
  if (ProcRank == 0) {
    int base = v.size() / ProcessN;    // минимум на обработку
    int remain = v.size() % ProcessN;  // остаток (распределим)

    int startId = 0;
    for (int i = 1; i < ProcessN; i++) {
      partSize = base;
      if (remain) {  // если есть остаток - то распределяем между первыми
        partSize++;
        remain--;
      }

      partSize++;  // цепляем правого соседа, 0-й будет последним - поэтому он будет последний кусок считать

      // отправляем всем процессам их размер
      MPI_Send(&partSize, 1, MPI_INT, i, 0, MPI_COMM_WORLD);
      // std::cout << "Id: " << startId << '\n';

      // отправлем всем прцоессам их кусок вектора
      MPI_Send(v.data() + startId, partSize, MPI_DOUBLE, i, 0, MPI_COMM_WORLD);
      startId += partSize - 1;

      // вычисляю для последнего
      if (i == ProcessN - 1) {
        partSize = base + remain;
      }
    }

    for (size_t j = startId + 1; j < v.size(); j++) {
      if ((v[j - 1] < 0 && v[j] >= 0) || (v[j - 1] >= 0 && v[j] < 0)) {
        allSwaps++;
      }
    }

    // получаю посчитанные swap от процессов
    for (int i = 1; i < ProcessN; i++) {
      int tmp;
      // что , сколько, тип, кому, тег, коммуникатор
      MPI_Recv(&tmp, 1, MPI_INT, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);
      allSwaps += tmp;
    }

    // на этом этапе 0-й процесс сделал всю работу
  }

  for (int i = 1; i < ProcessN; i++) {
    if (ProcRank == i) {
      std::vector<double> vectData;
      // отправляю partSize процессам кроме последнего
      MPI_Recv(&partSize, 1, MPI_INT, 0, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);

      vectData.resize(partSize);
      MPI_Recv(vectData.data(), partSize, MPI_DOUBLE, 0, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);

      int swapCount = 0;
      for (size_t j = 1; j < vectData.size(); j++) {
        if ((vectData[j - 1] < 0 && vectData[j] >= 0) || (vectData[j - 1] >= 0 && vectData[j] < 0)) {
          swapCount++;
        }
      }

      MPI_Send(&swapCount, 1, MPI_INT, 0, 0, MPI_COMM_WORLD);
    }
  }

  MPI_Barrier(MPI_COMM_WORLD);

  if (ProcRank == 0) {
    // отправляем всем процессам корректный результат
    for (int i = 1; i < ProcessN; i++) {
      MPI_Send(&allSwaps, 1, MPI_INT, i, 0, MPI_COMM_WORLD);
    }
    // сами устанавливаем значение
    GetOutput() = allSwaps;
  }
  for (int i = 1; i < ProcessN; i++) {
    if (ProcRank == i) {
      int allSw;
      MPI_Recv(&allSw, 1, MPI_INT, 0, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);
      GetOutput() = allSw;
    }
  }
  MPI_Barrier(MPI_COMM_WORLD);
  return true;
} 
```

