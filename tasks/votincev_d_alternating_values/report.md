# Отчет по реализации алгоритма нахождения числа чередований знаков значений соседних элементов вектора

**Дисциплина:** Параллельное программирование  
**Преподаватель:** Нестеров Александр Юрьевич и Оболенский Арсений Андреевич  
**Студент:** Вотинцев Дмитрий Сергеевич 3823Б1ФИ3
**Вариант:** 5

## Введение
В рамках данной работы был реализован алгоритм поиска чередований знаков значений соседних элементов вектора: последовательная и параллельная реализация.

## Постановка задачи
Дан вектор v из N элементов. Необходимо найти число чередований знаков между соседними элементами.
Рассмотрим элементы v[i-1] и v[i] , i = 1,2,...,N.
Если v[i-1] >= 0  и  v[i] < 0 - то это чередование
Если v[i-1] < 0  и  v[i] >= 0 - то это чередование
(0 считается беззнаковым, грубо говоря +0)

## Описание алгоритма

Последовательная версия:
проходимся по всему массиву, сравниваем соседние элементы на чередование, увеличиваем счетчик чередований (если между соседями знак чередуется). 


## Описание схемы параллельного алгоритма
Пусть у нас N процессов, размер вектор M, тогда алгоритм такой:
0-й процесс делит массив на равные N частей. 
Части вычисляются следующим образом:
Сначала считаем минимум, который будет каждый процесс обрабатывать:
base = M/N
Затем вычисляем остаток от деления:
remain = M % N
Первые remain процессов заберут по 1 элементу из массива, остальные отработают только минимум.
Однако так работать не будет, вот пример:
Пусть 2 процесса и вектор 0 -1 2 -3
Тогда:
0й процесс возьмет 0 -1  -> всего 1 чередование
1й процесс возьмет 2 -3  -> всего 1 чередование
И ответ будет 2 - что неверно, ведь чередований 3.
Поэтому нужно ещё цеплять правого соседа:
0й возьмет 0 -1 2
1й возьмёт 2 -3 ?
Да, если просто брать правого соседа - то последний процесс всегда будет заходит за память, поэтому последний процесс будет брать на 1 элемент меньше.

Итого кратко:
вычисляем минимальную часть M/N;
первые remain процессов получают +1 элемент вектора к обработке;
всем процессам добавляем +1 чтобы цепляли правого соседа;
последнему процессу делаем -1 чтобы он не выходил за память.

Но что подразумевается под "0й процесс делит на части вектор и распределяет" ?
Можно сделать вот как: 
дается исходный вектор v
0й процесс делит его на части (прям поэлементно);
0й процесс распряделяет ЧАСТИ вектора другим процессам;
другие процессы создают внутри себя вектора, которые являются частью исходного.

Это и была моя первая реализация алгоритма - простая, но давала ускорение в 0.5.
То есть SEQ работал быстрее MPI всегда.
Проблема была не в делении на части (ведь она оптимальная) - а в памяти.
Проблема передачи памяти заключалась в том, что 0й процесс посылал части вектора - по сути копирование.
Но можно посылать индекс начала + кол-во элементов для обработки - что намного лучше.


## Результаты экспериментов и выводы
В таблице представлено n - размер вектора, время выполнения SEQ и MPI версии для 4-х процессов:

| Размер данных (n) | SEQ версия (с) | MPI версия (с) | Ускорение |
|-------------------|----------------|----------------|-----------|
| 100               | 0.000000       | 0.0013         | 0.00×     |
| 10 000            | 0.000006       | 0.0018         | 0.003×    |
| 1 000 000         | 0.0039         | 0.0059         | 0.66×     |
| 10 000 000        | 0.0330         | 0.0068         | 4.85×     |

Само ускорение в 4.85 раз - выглядит достаточно неправдоподобно, как будто такого не может быть.
Но тесты показали так. Алгоритм достаточно простой и хорошо распараллеливается - это основная причина.

 
## Заключение
В результате проделанной работы были реализованы версии MPI, SEQ алгоритма нахождения числа чередований знаков значений соседних элементов вектора.
Было так же показано, что MPI версия превосходит SEQ на больших значениях.


## Литература
1. Лекции Сысоева Александра Владимировича
2. Практические занятия Нестерова Александра Юрьевича и Оболенского Арсения Андреевича


## Приложения (код параллельной реализации)

# Код неоптимальной реализации (с передачей кусков векторов)
```
bool VotincevDAlternatingValuesMPI::RunImpl() {
  int allSwaps = 0;
  // получаю кол-во процессов
  int ProcessN;
  MPI_Comm_size(MPI_COMM_WORLD, &ProcessN);

  // получаю ранг процесса
  int ProcRank;
  MPI_Comm_rank(MPI_COMM_WORLD, &ProcRank);

  int partSize;
  if (ProcRank == 0) {
    int base = v.size() / ProcessN;    // минимум на обработку
    int remain = v.size() % ProcessN;  // остаток (распределим)

    int startId = 0;
    for (int i = 1; i < ProcessN; i++) {
      partSize = base;
      if (remain) {  // если есть остаток - то распределяем между первыми
        partSize++;
        remain--;
      }

      partSize++;  // цепляем правого соседа, 0-й будет последним - поэтому он будет последний кусок считать

      // отправляем всем процессам их размер
      MPI_Send(&partSize, 1, MPI_INT, i, 0, MPI_COMM_WORLD);
      // std::cout << "Id: " << startId << '\n';

      // отправлем всем прцоессам их кусок вектора
      MPI_Send(v.data() + startId, partSize, MPI_DOUBLE, i, 0, MPI_COMM_WORLD);
      startId += partSize - 1;

      // вычисляю для последнего
      if (i == ProcessN - 1) {
        partSize = base + remain;
      }
    }

    for (size_t j = startId + 1; j < v.size(); j++) {
      if ((v[j - 1] < 0 && v[j] >= 0) || (v[j - 1] >= 0 && v[j] < 0)) {
        allSwaps++;
      }
    }

    // получаю посчитанные swap от процессов
    for (int i = 1; i < ProcessN; i++) {
      int tmp;
      // что , сколько, тип, кому, тег, коммуникатор
      MPI_Recv(&tmp, 1, MPI_INT, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);
      allSwaps += tmp;
    }

    // на этом этапе 0-й процесс сделал всю работу
  }

  for (int i = 1; i < ProcessN; i++) {
    if (ProcRank == i) {
      std::vector<double> vectData;
      // отправляю partSize процессам кроме последнего
      MPI_Recv(&partSize, 1, MPI_INT, 0, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);

      vectData.resize(partSize);
      MPI_Recv(vectData.data(), partSize, MPI_DOUBLE, 0, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);

      int swapCount = 0;
      for (size_t j = 1; j < vectData.size(); j++) {
        if ((vectData[j - 1] < 0 && vectData[j] >= 0) || (vectData[j - 1] >= 0 && vectData[j] < 0)) {
          swapCount++;
        }
      }

      MPI_Send(&swapCount, 1, MPI_INT, 0, 0, MPI_COMM_WORLD);
    }
  }

  MPI_Barrier(MPI_COMM_WORLD);

  if (ProcRank == 0) {
    // отправляем всем процессам корректный результат
    for (int i = 1; i < ProcessN; i++) {
      MPI_Send(&allSwaps, 1, MPI_INT, i, 0, MPI_COMM_WORLD);
    }
    // сами устанавливаем значение
    GetOutput() = allSwaps;
  }
  for (int i = 1; i < ProcessN; i++) {
    if (ProcRank == i) {
      int allSw;
      MPI_Recv(&allSw, 1, MPI_INT, 0, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);
      GetOutput() = allSw;
    }
  }
  MPI_Barrier(MPI_COMM_WORLD);
  return true;
} 
```

# Код оптимальной реализации (передача индексов + размера)
```
bool VotincevDAlternatingValuesMPI::RunImpl() {

  double start_time = 0, end_time = 0;

  start_time = MPI_Wtime();
  int allSwaps = 0;
  // получаю кол-во процессов
  int ProcessN;
  MPI_Comm_size(MPI_COMM_WORLD, &ProcessN);

  // получаю ранг процесса
  int ProcRank;
  MPI_Comm_rank(MPI_COMM_WORLD, &ProcRank);

  int partSize;
  if (ProcRank == 0) {
    int base = v.size() / ProcessN;    // минимум на обработку
    int remain = v.size() % ProcessN;  // остаток (распределим)

    int startId = 0;
    for (int i = 1; i < ProcessN; i++) {
      partSize = base;
      if (remain) {  // если есть остаток - то распределяем между первыми
        partSize++;
        remain--;
      }

      partSize++;  // цепляем правого соседа, 0-й будет последним - поэтому он будет последний кусок считать

      // Вместо пересылки данных - пересылаем индексы начала и конца
      int indices[2] = {startId, startId + partSize};
      MPI_Send(indices, 2, MPI_INT, i, 0, MPI_COMM_WORLD);
      // std::cout << "Id: " << startId << '\n';

      startId += partSize - 1;

      // вычисляю для последнего
      if (i == ProcessN - 1) {
        partSize = base + remain;
      }
    }

    for (size_t j = startId + 1; j < v.size(); j++) {
      if ((v[j - 1] < 0 && v[j] >= 0) || (v[j - 1] >= 0 && v[j] < 0)) {
        allSwaps++;
      }
    }

    // получаю посчитанные swap от процессов
    for (int i = 1; i < ProcessN; i++) {
      int tmp;
      // что , сколько, тип, кому, тег, коммуникатор
      MPI_Recv(&tmp, 1, MPI_INT, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);
      allSwaps += tmp;
    }

    // на этом этапе 0-й процесс сделал всю работу
    
  }

  for (int i = 1; i < ProcessN; i++) {
    if (ProcRank == i) {
      // получаем индексы вместо данных
      int indices[2];
      MPI_Recv(indices, 2, MPI_INT, 0, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);

      int start_index = indices[0];
      int end_index = indices[1];

      // корректируем конечный индекс если нужно
      if (end_index > v.size()) end_index = v.size();

      int swapCount = 0;
      // обрабатываем свой диапазон из глобального вектора v
      for (int j = start_index + 1; j < end_index; j++) {
        if ((v[j - 1] < 0 && v[j] >= 0) || (v[j - 1] >= 0 && v[j] < 0)) {
          swapCount++;
        }
      }

      MPI_Send(&swapCount, 1, MPI_INT, 0, 0, MPI_COMM_WORLD);
    }
  }

  MPI_Barrier(MPI_COMM_WORLD);
  end_time = MPI_Wtime();

  if (ProcRank == 0) {
    // отправляем всем процессам корректный результат
    for (int i = 1; i < ProcessN; i++) {
      MPI_Send(&allSwaps, 1, MPI_INT, i, 0, MPI_COMM_WORLD);
    }
    // сами устанавливаем значение
    GetOutput() = allSwaps;
  }
  for (int i = 1; i < ProcessN; i++) {
    if (ProcRank == i) {
      int allSw;
      MPI_Recv(&allSw, 1, MPI_INT, 0, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);
      GetOutput() = allSw;
    }
  }
  MPI_Barrier(MPI_COMM_WORLD);
  if(ProcRank == 0) {
    std::cout << "MPI_was_working_" << (end_time - start_time) << "\n";
  }
  
  return true;
}
```