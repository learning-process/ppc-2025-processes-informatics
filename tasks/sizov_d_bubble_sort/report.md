# Сортировка пузырьком (алгоритм чет-нечетной перестановки)

- Student: Сизов Дмитрий Игоревич, group 3823Б1ФИ2
- Technology: SEQ + MPI
- Variant: 21

## 1. Introduction
Цель работы — реализовать последовательную и параллельную (MPI) версии пузырьковой сортировки на основе алгоритма **Odd–Even Transposition Sort** и сравнить их корректность и производительность.  
Задача относится к классу алгоритмов сравнения, предполагающих попарные обмены соседних элементов.

Основной ожидаемый результат — корректная реализация сортировки, успешное прохождение функциональных и производительных тестов PPC и демонстрация ускорения MPI-версии на больших входных данных.

## 2. Problem Statement
Дано: массив целых чисел `InType = std::vector<int>`.

Требуется:
- Отсортировать массив по неубыванию.
- Предоставить две реализации: последовательную и MPI.
- Оба варианта должны корректно работать во всех тестах.

Формат ввода/вывода:
- Вход: `std::vector<int>`
- Выход: отсортированный `std::vector<int>`

Ограничения: пустой вход недопустим.

## 3. Baseline Algorithm (Sequential)
Последовательная версия использует классический odd–even sort:
Алгоритм odd–even sort работает серией фаз, в каждой из которых он сравнивает соседние элементы, но пары выбираются по-разному для чётных и нечётных фаз.

На чётной фазе просматриваются пары элементов, начинающиеся с чётных индексов: (0,1), (2,3), (4,5) и так далее.
Для каждой пары, если левый элемент больше правого, они меняются местами.

На нечётной фазе сравниваются пары, начинающиеся с нечётных индексов: (1,2), (3,4), (5,6) и т.д.
Здесь применяется то же правило обмена.

Эти фазы чередуются, и из-за регулярного сдвига сравниваемых пар вправо любые элементы, стоящие не на своём месте, постепенно перемещаются в нужную сторону. Алгоритм повторяет фазы до тех пор, пока в какой-то фазе не окажется, что обменов больше не требуется, что означает достижение полной упорядоченности.

Сложность: O(n²).

## 4. Parallelization Scheme (MPI)

### 4.1 Data Distribution
На этапе распределения данных вызывается ComputeScatterInfo, которая рассчитывает:
- counts[i] — сколько элементов получит процесс i;
- displs[i] — смещение блока в общем массиве.

Это обеспечивает почти равномерное распределение даже при n % p != 0.
MPI_Scatterv использует эти массивы, чтобы выдать каждому процессу его подмассив в локальный буфер.

### 4.2 Local Work
Каждый процесс локально сортирует свой подмассив с помощью std::ranges::sort(local).
На этом шаге каждый процесс имеет корректно отсортированный фрагмент, но глобальный порядок ещё не гарантирован, так как разные процессы могут содержать «перемешанные» диапазоны значений.

### 4.3 Neighbor Exchanges
Основная часть алгоритма — обмены между соседними процессами. В коде выполняется size фаз — по одной фазе на процесс.

На каждой фазе:
1. Определяется четность фазы: even_phase = (phase % 2 == 0).
2. Для каждого процесса вычисляется партнёр:
   - если чётная фаза и rank чётный, то партнёр rank+1,
   - если чётная фаза и rank нечётный, то партнёр rank-1,
   - если нечётная фаза и rank нечётный, то партнёр rank+1,
   - если нечётная фаза и rank чётный, то партнёр rank-1.
3. Если партнёр выходит за границы (partner < 0 или partner >= size), обмен пропускается.
4. Иначе процессы выполняют обмен данными через MPI_Sendrecv. Каждый процесс получает отсортированный блок соседа в recvbuf.
5. Затем оба процесса сливают local и recvbuf в единый отсортированный массив merged.
6. Далее каждый процесс выбирает свою половину результата:
   - если rank < partner, то он сохраняет первые local_n элементов,
   - если rank > partner, то он сохраняет последние local_n элементов.

Такой обмен гарантирует, что элементы постепенно перемещаются в нужную глобальную область — аналогично тому, как odd–even sort выталкивает большие числа вправо, а маленькие — влево.

### 4.4 Gathering & Broadcast
После всех фаз root-процесс выполняет:
1. Сбор данных в единый массив через MPI_Gatherv.
2. Финальную сортировку std::ranges::sort на root для гарантии корректного глобального порядка.
3. Рассылку итогового массива всем процессам через MPI_Bcast.

## 5. Implementation Details

### 5.1 Структура проекта
```
tasks/sizov_d_bubble_sort
├── common
│   └── include
│       └── common.hpp
├── data
│   ├── test1.txt
│   ├── test2.txt
│   ├── test3.txt
│   ├── ...
│   └── testN.txt
├── mpi
│   ├── include
│   │   └── ops_mpi.hpp
│   └── src
│       └── ops_mpi.cpp
├── seq
│   ├── include
│   │   └── ops_seq.hpp
│   └── src
│       └── ops_seq.cpp
├── tests
│   ├── functional
│   │   └── main.cpp
│   └── performance
│       └── main.cpp
├── info.json
├── report.md
└── settings.json

```

### 5.2 Data
Функциональные тесты используют набор входных файлов, расположенный в каталоге:
  tasks/sizov_d_bubble_sort/data/testN.txt
Каждый файл содержит две строки: неотсортированный массив и эталонный отсортированный результат.

### 5.3 Additional helper functions

В реализации используются несколько вспомогательных функций, размещённых в анонимном namespace внутри mpi/src/ops_mpi.cpp, а также служебная функция TrimString в модуле функциональных тестов.

- `ComputeScatterInfo(total, size, rem, counts, displs)`  
  Вычисляет размеры блоков (`counts`) и их смещения (`displs`) для `MPI_Scatterv`.  
  Функция корректно обрабатывает случаи, когда `total % size != 0`, распределяя "лишние" элементы между первыми процессами. Это гарантирует максимально равномерное разбиение входного массива.

- `ComputePartner(even_phase, even_rank, rank)`  
  Определяет соседа для обмена на текущей фазе odd–even сортировки.  
  Логика полностью соответствует алгоритму: на чётных фазах чётные процессы обмениваются с правым соседом, на нечётных — нечётные. Функция упрощает вычисление и делает код более читаемым.

- `OddEvenExchange(local, counts, rank, size, phase)`  
  Реализует одну фазу параллельной odd–even сортировки.  
  Выполняет отправку и приём блоков через `MPI_Sendrecv`, слияние двух отсортированных массивов и выбор корректной половины результата. Также содержит проверку граничных условий (например, отсуствие соседа справа или слева).

- `GatherResult(local, counts, displs, rank, total, output)`  
  Выполняет сбор результатов на root-процессе через `MPI_Gatherv`.  
  После получения всех блоков root выполняет финальную сортировку, обеспечивая устойчивость результата даже при возможных неточностях на стыках блоков.

- `TrimString(std::string& s)` (из функциональных тестов)  
  Удаляет пробельные символы (`\r`, `\n`, `\t`) по краям строки перед разбором тестовых данных.  
  Это предотвращает ошибки чтения чисел при наличии переносов строк в тестовых файлах.

## 6. Experimental Setup

|  Компонент |               Значение                 |
|------------|----------------------------------------|
|     CPU    | 12th Gen Intel(R) Core(TM) i5-12450H   |
|     RAM    |                 16 GB                  |
|     ОС     | OS: Ubuntu 24.04 (DevContainer / WSL2) |
| Компилятор | GCC 13.3.0 (g++), C++20, CMake, Release|
|     MPI    |        mpirun (Open MPI) 4.1.6         |

## 7. Results and Discussion

### 7.1 Correctness
Корректность подтверждена:
- многочисленными функциональными тестами,
- сравнением с `std::ranges::sort`,
- совпадением результатов MPI и SEQ.

### 7.2 Performance

| Режим | Количество процессов | Время, с     | Ускорение    | Эффективность   |
|-------|----------------------|--------------|--------------|-----------------|
| SEQ   | 1                    | 0.8406618595 | 1.000000000  | N/A             |
| MPI   | 2                    | 0.0016978002 | 495.1476973  | 24757.3848648%  |
| MPI   | 4                    | 0.0020235174 | 415.4458269  | 10386.1456726%  |
| MPI   | 8                    | 0.0026713558 | 314.6948301  | 3933.68537570%  |
| MPI   | 16                   | 0.0025910712 | 324.4456808  | 2027.78550500%  |
| MPI   | 32                   | 0.0017847040 | 471.0371353  | 1471.99104780%  |

Для запуска на CI производительные тесты используют автоматически сгенерированные данные, для SEQ и MPI размеры массивов разные:
- для SEQ: массив длиной 150 000, заполненный числами в убывающем порядке;
- для MPI: массив длиной 1 000 000, также заполненный убывающей последовательностью.
Для замеров результатов производительности и сравнения последовательной и параллельной версии использовался единый размер входных данных, а именно 150 000 элементов. Тесты производительности проводились локально на моём компьютере с указанными выше характеристиками. Результаты производительности усреднены по трём прогонам. Время, указанное в третьем столбце -- это время task_run.

Ускорение вычисляется по формуле:

\[
S(p) = \frac{T_{\text{seq}}}{T_p}
\]

Эффективность по:

\[
E(p) = \frac{S(p)}{p} \times 100\%
\]

Генерация данных выполняется непосредственно в файле:
  tasks/sizov_d_bubble_sort/tests/performance/main.cpp

### 7.3 Environment variables

Для запуска производительных тестов использовались стандартные переменные окружения PPC:

- **PPC_NUM_PROC** — задаёт количество MPI-процессов для измерений (в экспериментах: 1, 2, 4, 8, 16, 32).
- **PPC_PERF_MAX_TIME** — максимальное время выполнения одного теста (20 секунд).
- **PPC_IGNORE_TEST_TIME_LIMIT** — разрешает выполнение тестов без учёта ограничений по времени (использовалось при ручном запуске).
- **TMPDIR** — каталог для временных файлов, требуемый инфраструктурой PPC.

## 8. Conclusions
- Реализованы SEQ и MPI версии odd–even bubble sort.
- Обе версии успешно проходят тесты.
- MPI обеспечивает большое ускорение.
- Ограничение — квадратичная сложность и необходимость обменов на каждой фазе.

## 9. References
- Гергель В.П. Высокопроизводительные вычисления для многопроцессорных многоядерных систем. Серия «Суперкомпьютерное образование».
- Гергель В.П. Теория и практика параллельных вычислений. – М.: Интуит Бином. Лаборатория знаний, 2007.
- Wilkinson B., Allen M. Parallel programming: techniques and applications using networked workstations and parallel computers. – Prentice Hall, 1999.

## Appendix
```cpp
bool SizovDBubbleSortMPI::RunImpl() {
  int rank = 0;
  int size = 1;
  MPI_Comm_rank(MPI_COMM_WORLD, &rank);
  MPI_Comm_size(MPI_COMM_WORLD, &size);

  const int total = static_cast<int>(data_.size());
  const int rem = total % size;

  std::vector<int> counts(size);
  std::vector<int> displs(size);

  ComputeScatterInfo(total, size, rem, counts, displs);

  const int local_n = counts[rank];
  std::vector<int> local(local_n);

  MPI_Scatterv(data_.data(), counts.data(), displs.data(), MPI_INT, local.data(), local_n, MPI_INT, 0, MPI_COMM_WORLD);

  std::ranges::sort(local);

  for (int phase = 0; phase < size; ++phase) {
    OddEvenExchange(local, counts, rank, size, phase);
  }

  std::vector<int> global;
  GatherResult(local, counts, displs, rank, total, global);

  if (rank == 0) {
    GetOutput() = global;
  }

  return true;
}
```
