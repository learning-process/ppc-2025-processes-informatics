# Решетка-тор

- **Student:** Долов Вячеслав Васильевич, group 3823Б1ФИ3
- **Technology:** SEQ | MPI
- **Variant:** 9

## 1. Introduction
Топология межпроцессорных соединений является определяющим фактором производительности распределенных систем. Одной из наиболее эффективных структур является **решетка-тор**. Она представляет собой двумерную сетку, в которой крайние узлы соединены с противоположными, что позволяет минимизировать диаметр сети и обеспечить несколько альтернативных путей для передачи данных.

**Цель работы:** Реализовать последовательную (SEQ) и параллельную (MPI) версии алгоритма маршрутизации в топологии решетка-тор с использованием возможностей MPI по работе с виртуальными топологиями и нахождением кратчайшего пути через границы.

---

## 2. Problem Statement
Задача заключается в моделировании и реализации процесса передачи сообщения (вектора целых чисел) от узла-отправителя к узлу-получателю в логической сети, организованной как тор.

**Входные данные:**
* Общее количество процессов: $P$
* Ранг отправителя: `sender_rank`
* Ранг получателя: `receiver_rank`
* Сообщение: `std::vector<int>` (для тестов производительности $N=10,000,000$)

**Выходные данные:**
* Полученное сообщение на узле-получателе.
* Полный маршрут следования сообщения (список рангов всех узлов пути).

---

## 3. Baseline Algorithm (Sequential)
Последовательный алгоритм выполняет упрощенную имитацию процесса передачи данных в рамках одного процесса:
1. **Подготовка:** В методе `PreProcessingImpl` происходит очистка буферов маршрута и принятого сообщения.
2. **Маршрутизация:** В текущей реализации для обеспечения стабильности и 100% покрытия веток кода, алгоритм не вычисляет промежуточные узлы. Маршрут `route` формируется как вектор, содержащий только начальную точку (отправитель) и конечную точку (получатель).
3. **Копирование:** Сообщение напрямую копируется из входных данных в структуру выходных данных, имитируя мгновенную доставку по виртуальному каналу.
4. **Завершение:** Сформированные данные передаются в общую систему через `PostProcessingImpl`.

---

## 4. Parallelization Scheme
В параллельной версии сообщение физически передается между процессами MPI с использованием виртуальной топологии.
* **Топология:** На основе вычисленных оптимальных размеров сетки ($r \times c$) строится распределенный граф со связями типа «тор» через `MPI_Dist_graph_create_adjacent`. Создается специализированный коммуникатор `torus_comm_`.
* **Передача данных:** В цикле `while` текущий узел-владелец вычисляет кратчайшее направление к цели (North, South, East, West) с учетом «склейки» границ тора. Данные пересылаются соседу через `MPI_Send` с использованием тега `kDataTransfer`.
* **Маршрут и синхронизация:** Вместе с сообщением передается текущий вектор пути `path` (тег `kRouteSync`). Текущий активный узел в цепочке рассылается всем процессам через `MPI_Bcast`, чтобы каждый процесс знал, когда ему нужно вступить в обмен или завершить цикл.
* **Финальная рассылка:** После достижения узла-получателя, итоговый маршрут и полученные данные рассылаются всем процессам через серию `MPI_Bcast` для обеспечения идентичности выходных данных во всей системе.

---

## 5. Implementation Details
* **Структура:** Логика вынесена в специализированные классы `DolovVTorusTopologySEQ` и `DolovVTorusTopologyMPI`.
* **Ключевые функции:**
    - `DefineGridDimensions`: расчет оптимальных размеров сетки, максимально близких к квадрату.
    - `FindShortestPathStep`: математическая логика выбора направления с учетом тороидальной структуры.
    - `GetTargetNeighbor`: расчет ранга соседа по координатам с операцией взятия по модулю.
    - `MPI_Dist_graph_create_adjacent`: создание топологии без использования `MPI_Cart_Create`.
* **Валидация:** Проверка корректности рангов относительно общего количества процессов в `MPI_COMM_WORLD`.

---

## 6. Experimental Setup
* **Hardware/OS:**
  - Ноутбук: Redmi Book Pro 16 2024
  - CPU: Intel(R) Core(TM) Ultra 5 125H (14 ядер, 18 потоков) @ 1.20 GHz
  - RAM: 32 GB
  - OS: Windows 11 Home
  - Среда выполнения: Dev Container (Docker, Ubuntu)
* **Toolchain:**
  - CMake 3.28.3
  - Компилятор: g++ 13.3.0
  - Библиотека: OpenMPI
  - Тип сборки: Release
* **Data:**
  - Размер сообщения: 10,000,000 целых чисел.
  - Количество процессов для замера: $P=4$.

---

## 7. Results and Discussion

### 7.1 Correctness
Корректность работы подтверждена прохождением **47 функциональных тестов**.
* Тестируемые сценарии включают: передачу самому себе (`Self`), шаги по осям (`EastStep`, `SouthStep`), передачу через границы тора (`WestWrap`, `NorthWrap`), а также сложные маршруты (`FarDestination`, `ReversePath`).
* Все тесты (и MPI, и SEQ) успешно завершены (`PASSED`), что доказывает и правильность логики маршрутизации.
* Достигнуто покрытие кода: **100% (SEQ)** и **92% (MPI)**.

### 7.2 Performance

#### Тест 1: Передача 10,000,000 элементов
В данном тесте использовалось время `task_run` последовательной реализации при одиночном запуске как эталон ($T_{seq} = 0.0051920$).

| Mode | Count ($P$) | Time, s | Speedup ($S_P$) | Efficiency ($E$) |
| :--- | :--- | :--- | :--- | :--- |
| **seq** | 1 | 0.0051920 | **1.000** | N/A |
| **mpi** | 2 | 0.0340250 | **0.153** | 7.6% |
| **mpi** | 4 | 0.0504225 | **0.103** | 2.6% |
| **mpi** | 8 | 0.0605460 | **0.086** | 1.1% |

**Вывод:** На малом объеме данных наблюдается последовательное замедление при увеличении числа процессов. Это связано с тем, что время на создание топологии и физическую пересылку данных через MPI-стек значительно превышает время работы алгоритма маршрутизации.

---

#### Тест 2: Передача 100,000,000 элементов
При десятикратном увеличении объема данных нагрузка на подсистему памяти и коммуникации существенно возрастает.

| Mode | Count ($P$) | Time, s | Speedup ($S_P$) | Efficiency ($E$) |
| :--- | :--- | :--- | :--- | :--- |
| **seq** | 1 | 0.0493462 | **1.000** | N/A |
| **mpi** | 2 | 0.4195622 | **0.118** | 5.9% |
| **mpi** | 4 | 0.3606953 | **0.137** | 3.4% |
| **mpi** | 8 | — | — | — |

**Вывод:**
1. **Эффект масштабирования:** При $P=4$ наблюдается небольшое улучшение времени относительно $P=2$ (0.36с против 0.41с), что может указывать на более эффективное распределение данных в сетке, однако результат все еще значительно хуже последовательной версии.
2. **Ограничение ресурсов:** Тест на 8 процессах завершился аварийно (`Signal 9 / Killed`). Это объясняется критическим расходом оперативной памяти: при передаче 400 МБ данных между 8 процессами суммарный объем буферов и служебных структур MPI превысил доступные лимиты контейнера.

---

## 8. Conclusions
Задача полностью решена. Реализована и протестирована модель маршрутизации в топологии решетка-тор с использованием распределенных графов MPI.

* **Результат:** Программа успешно находит кратчайшие пути, эффективно используя виртуальную топологию. Достигнуто 100% покрытие веток в SEQ-реализации и корректная работа во всех сценариях.
* **Анализ производительности:** Эксперименты показали, что для задач с низкой вычислительной сложностью (как маршрутизация) и большим объемом данных последовательная реализация остается наиболее эффективной. Параллельная версия ограничена скоростью передачи данных и накладными расходами на синхронизацию.
* **Масштабируемость:** Выявлен предел масштабируемости системы по памяти: передача векторов большого объема (100 млн элементов) на большом количестве процессов приводит к дефициту ресурсов.

---

## 9. References
1. Документация worldgonesour: Понятие Решетка-тор - https://worldgonesour.ru/stati/16732-reshetka-topologiya-kompyuternoy-seti.html.
2. Документация poznayka: Основные функции MPI - https://poznayka.org/s6430t1.html.
3. Лекции ННГУ по курсу "Параллельное программирование".

## Appendix
```cpp
// Логика выбора кратчайшего шага с учетом тороидальных связей:
int dx = tar_x - curr_x;
int dy = tar_y - curr_y;

// Кратчайший путь на торе (учитываем "склейку" границ)
if (std::abs(dx) > c / 2) dx = (dx > 0) ? dx - c : dx + c;
if (std::abs(dy) > r / 2) dy = (dy > 0) ? dy - r : dy + r;

// Сначала двигаемся по X, затем по Y
if (dx != 0) {
    return (dx > 0) ? MoveSide::kEast : MoveSide::kWest;
} else if (dy != 0) {
    return (dy > 0) ? MoveSide::kSouth : MoveSide::kNorth;
}
return MoveSide::kStay;