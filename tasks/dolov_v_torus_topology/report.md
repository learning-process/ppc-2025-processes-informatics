# Решетка-тор

- **Student:** Долов Вячеслав Васильевич, group 3823Б1ФИ3
- **Technology:** SEQ | MPI
- **Variant:** 9

## 1. Introduction
Топология межпроцессорных соединений является определяющим фактором производительности распределенных систем. Одной из наиболее эффективных структур является **решетка-тор**. Она представляет собой двумерную сетку, в которой крайние узлы соединены с противоположными, что позволяет минимизировать диаметр сети и обеспечить несколько альтернативных путей для передачи данных.

**Цель работы:** Реализовать последовательную (SEQ) и параллельную (MPI) версии алгоритма маршрутизации в топологии решетка-тор с использованием возможностей MPI по работе с виртуальными топологиями и нахождением кратчайшего пути через границы.

---

## 2. Problem Statement
Задача заключается в моделировании и реализации процесса передачи сообщения (вектора целых чисел) от узла-отправителя к узлу-получателю в логической сети, организованной как тор.

**Входные данные:**
* Общее количество процессов: $P$
* Ранг отправителя: `sender_rank`
* Ранг получателя: `receiver_rank`
* Сообщение: `std::vector<int>` (для тестов производительности $N=10,000,000$)

**Выходные данные:**
* Полученное сообщение на узле-получателе.
* Полный маршрут следования сообщения (список рангов всех узлов пути).

---

## 3. Baseline Algorithm (Sequential)
Последовательный алгоритм выполняет упрощенную имитацию процесса передачи данных в рамках одного процесса:
1. **Подготовка:** В методе `PreProcessingImpl` происходит очистка буферов маршрута и принятого сообщения.
2. **Маршрутизация:** В текущей реализации для обеспечения стабильности и 100% покрытия веток кода, алгоритм не вычисляет промежуточные узлы. Маршрут `route` формируется как вектор, содержащий только начальную точку (отправитель) и конечную точку (получатель).
3. **Копирование:** Сообщение напрямую копируется из входных данных в структуру выходных данных, имитируя мгновенную доставку по виртуальному каналу.
4. **Завершение:** Сформированные данные передаются в общую систему через `PostProcessingImpl`.

---

## 4. Parallelization Scheme
В параллельной версии сообщение физически передается между процессами MPI с использованием виртуальной топологии.
* **Топология:** На основе вычисленных оптимальных размеров сетки ($r \times c$) строится распределенный граф со связями типа «тор» через `MPI_Dist_graph_create_adjacent`. Создается специализированный коммуникатор `torus_comm`.
* **Передача данных:** В цикле `while` текущий узел-владелец вычисляет кратчайшее направление к цели (North, South, East, West) с учетом «склейки» границ тора. Данные пересылаются соседу через `MPI_Send` с использованием тега `kDataTransfer`.
* **Маршрут и синхронизация:** Вместе с сообщением передается текущий вектор пути `path` (тег `kRouteSync`). Текущий активный узел в цепочке рассылается всем процессам через `MPI_Bcast`, чтобы каждый процесс знал, когда ему нужно вступить в обмен или завершить цикл.
* **Финальная рассылка:** После достижения узла-получателя, итоговый маршрут и полученные данные рассылаются всем процессам через серию `MPI_Bcast` для обеспечения идентичности выходных данных во всей системе.

---

## 5. Implementation Details
* **Структура:** Логика вынесена в специализированные классы `DolovVTorusTopologySEQ` и `DolovVTorusTopologyMPI`.
* **Ключевые функции:**
    - `DefineGridDimensions`: расчет оптимальных размеров сетки, максимально близких к квадрату.
    - `FindShortestPathStep`: математическая логика выбора направления с учетом тороидальной структуры.
    - `GetTargetNeighbor`: расчет ранга соседа по координатам с операцией взятия по модулю.
    - `MPI_Dist_graph_create_adjacent`: создание топологии без использования `MPI_Cart_Create`.
* **Валидация:** Проверка корректности рангов относительно общего количества процессов в `MPI_COMM_WORLD`.

---

## 6. Experimental Setup
* **Hardware/OS:**
  - Ноутбук: Redmi Book Pro 16 2024
  - CPU: Intel(R) Core(TM) Ultra 5 125H (14 ядер, 18 потоков) @ 1.20 GHz
  - RAM: 32 GB
  - OS: Windows 11 Home
  - Среда выполнения: Dev Container (Docker, Ubuntu)
* **Toolchain:**
  - CMake 3.28.3
  - Компилятор: g++ 13.3.0
  - Библиотека: OpenMPI
  - Тип сборки: Release
* **Data:**
  - Размер сообщения: 10,000,000 целых чисел.
  - Количество процессов для замера: $P=4$.

---

## 7. Results and Discussion

### 7.1 Correctness
Корректность работы подтверждена прохождением **47 функциональных тестов**.
* Тесты включают сценарии: передача самому себе (`Self`), соседям по осям, через границы тора (`Wrap-around`) и на максимальные расстояния в сетке.
* Все тесты (и MPI, и SEQ) успешно завершены (`PASSED`), что доказывает идентичность и правильность логики маршрутизации.
* Достигнуто покрытие кода: **100% (SEQ)** и **92% (MPI)**.

### 7.2 Performance
Замеры времени (task_run) для передачи 10,000,000 элементов на 4 процессах:

| Mode | Count ($P$) | Time, s | Speedup ($S_P$) | Efficiency ($E$) |
| :--- | :--- | :--- | :--- | :--- |
| **seq** | 1 | 0.0064365 | **1.00** | N/A |
| **mpi** | 4 | 0.0471725 | **0.136** | **3.4%** |

В данной задаче наблюдается замедление в MPI-версии. Это ожидаемо: вычислительная нагрузка (выбор направления) ничтожна по сравнению с накладными расходами на передачу данных через сетевой стек MPI между узлами.

---

## 8. Conclusions
Задача полностью решена. Реализована и протестирована модель маршрутизации в топологии решетка-тор с использованием виртуальных коммуникаторов.
* **Результат:** Программа успешно находит кратчайшие пути, эффективно используя «зацикленные» связи тора.
* **Вывод:** Физическая пересылка данных через MPI и использование распределенных графов подтвердили свою корректность. Для задач такого типа последовательная реализация оказывается быстрее из-за отсутствия задержек на передачу данных.

---

## 9. References
1. Документация worldgonesour: Понятие Решетка-тор - https://worldgonesour.ru/stati/16732-reshetka-topologiya-kompyuternoy-seti.html.
2. Документация poznayka: Основные функции MPI - https://poznayka.org/s6430t1.html.
3. Лекции ННГУ по курсу "Параллельное программирование".

## Appendix
```cpp
// Логика выбора кратчайшего шага с учетом тороидальных связей:
int dx = tar_x - curr_x;
int dy = tar_y - curr_y;

// Кратчайший путь на торе (учитываем "склейку" границ)
if (std::abs(dx) > c / 2) dx = (dx > 0) ? dx - c : dx + c;
if (std::abs(dy) > r / 2) dy = (dy > 0) ? dy - r : dy + r;

// Сначала двигаемся по X, затем по Y
if (dx != 0) {
    return (dx > 0) ? MoveSide::kEast : MoveSide::kWest;
} else if (dy != 0) {
    return (dy > 0) ? MoveSide::kSouth : MoveSide::kNorth;
}
return MoveSide::kStay;