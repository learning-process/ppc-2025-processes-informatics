# Гиперкуб

- **Студент:** Еремин Василий Евгеньевич, группа 3823Б1ФИ2
- **Технология:** SEQ | MPI
- **Вариант:** № 10

## 1. Введение

**Цель работы:** Реализация и сравнительный анализ последовательной и параллельной версий алгоритма с передачей данных через топологию гиперкуб

**Задачи:**
1. Реализовать последовательную версию алгоритма метода прямоугольников
2. Реализовать параллельную версию с использованием технологии MPI с передачей через топологию гиперкуб
3. Провести сравнительный анализ производительности и эффективности обеих реализаций

## 2. Постановка задачи

**Задача**: Требуется вычислить значение определенного интеграла методом прямоугольников

![Интеграл](data/int.png)

где:
- `f(x)` — интегрируемая функция
- `a`, `b` — нижний и верхний пределы интегрирования соответственно

**Описание метода решения:** Из трех методов (левых, правых и средних прямоугольников) был выбран *метод средних прямоугольников*.

Область интегрирования разбивается на `n` равных отрезков. Сложная площадь под кривой разбивается на множество простых прямоугольников - на каждом отрезке строится прямоугольник высотой, равной значению функции в середине отрезка. Общая площадь вычисляется как сумма площадей всех этих прямоугольников.

![Метод средних прямоугольников](data/square.png) 

**Входные данные:**
- `a` — нижний предел интегрирования (вещественное число)
- `b` — верхний предел интегрирования (вещественное число)  
- `n` — количество отрезков разбиения (целое число)
- `f(x)` — интегрируемая функция

**Выходные данные:**
- Приближенное значение интеграла (вещественное число)

**Ограничения:** 
- `-1 000 000 000 ≤ a < b ≤ 1 000 000 000`
- `0 < n ≤ 100 000 000`


## 3. Описание алгоритма (последовательного)
**Алгоритм последовательного вычисления:**
1. **Найдем шаг интегрирования:** `h = (b - a) / n`
2. **Вычисляем сумму значений функции:** через цикл находим площадь каждого прямоугольника по формуле `f(a + (i + 0.5) * h)` 
3. **Умножаем сумму на шаг:** `result = h * sum`

**Реализация на C++:**

```cpp
double rectangle_method(double a, double b, double n) {
  double result = 0.0;
  double h = (b - a) / n;

  for (int i = 0; i < n; i++) {
    result += f(a + ((i + 0.5) * h));
  }

  result *= h;
  return result;
}
```

## 4. Схема распараллеливания (Топология "Гиперкуб")
В данной работе реализована параллельная схема с использованием топологии n-мерного гиперкуба. Это позволяет оптимизировать коллективные операции (рассылку и сборку данных), сводя их сложность к $O(\log_2 P)$, где $P$ — количество процессов.
**Алгоритм параллельного вычисления:**

1. **Создание коммуникатора гиперкуба:** Поскольку гиперкуб строится только на $2^n$
 узлах, программа вычисляет максимально возможную степень двойки для текущего количества процессов. С помощью `MPI_Comm_split` создается отдельный коммуникатор `cube_comm`, в который входят только отобранные процессы. Остальные процессы исключаются из активной фазы расчета, но получают финальный результат в конце.
2. **Рассылка параметров (Broadcast по гиперкубу):** Вместо стандартного MPI_Bcast реализован кастомный алгоритм рассылки. Данные передаются от 0-го процесса к соседям по измерениям гиперкуба. На каждом шаге d (от старшего измерения к младшему) количество узлов, обладающих данными, удваивается.
3. **Локальное вычисление:** Внутри cube_comm каждый процесс вычисляет свою часть интеграла.
4. **Сборка результатов (AllReduce по гиперкубу):** Для получения общей суммы реализован алгоритм обмена по измерениям гиперкуба (от младшего к старшему). На каждой итерации процессы обмениваются текущими суммами с соседями, чей ранг отличается на 1 бит в соответствующем разряде (`neighbor = rank ^ (1 << d)`). Используется функция `MPI_Sendrecv` для предотвращения взаимоблокировок (deadlocks).

5. **Финальная синхронизация:** По завершении цикла гиперкуба каждый участвующий процесс владеет полной суммой. С помощью одного финального `MPI_Bcast` результат передается процессам, которые не вошли в структуру гиперкуба.

**Принцип разделения отрезков:**

```cpp
for (int i = cube_rank; i < n; i += hypercube_size) {
    local_result += f(a + ((i + 0.5) * step_size));
  }
```

**Схема распределения вычислений:**
- Процесс 0: обрабатывает отрезки 0, size, 2×size, ...
- Процесс 1: обрабатывает отрезки 1, size+1, 2×size+1, ...
- Процесс k: обрабатывает отрезки k, size+k, 2×size+k, ...

## 5. Детали реализации
**Ключевой фрагмент кода обмена:**

```cpp
// Сборка суммы (AllReduce) по измерениям гиперкуба
for (int d = 0; d < ndims; ++d) {
    int neighbor = cube_rank ^ (1 << d);
    double received_sum = 0.0;
    MPI_Sendrecv(&current_sum, 1, MPI_DOUBLE, neighbor, 0,
                 &received_sum, 1, MPI_DOUBLE, neighbor, 0,
                 cube_comm, MPI_STATUS_IGNORE);
    current_sum += received_sum;
}
```
**Дополнительные функции в тестах:**
- `InFunction(x)` - подынтегральная функция (что интегрируем)
- `Function(x)` - первообразная (аналитическое решение для проверки)


## 6. Экспериментальная среда
- Hardware/OS: Apple M1 Pro, 6 ядер производительности и 2 эффективности, 16 ГБ, Ubuntu 24.04.2 (DevContainer)
- Toolchain: GCC 13.3.0, C++20, CMake 3.28.3, build type Release
- Environment: PPC_NUM_PROC = 8

## 7. Результаты и обсуждение

### 7.1 Проверка корректности

**Методы верификации корректности:**

1. **Сравнение с аналитическим решением:**
   - Используется функция `Function(x)` - первообразная от `InFunction(x)`
   - Ожидаемый результат вычисляется по формуле Ньютона-Лейбница: `Function(b) - Function(a)`
   - Позволяет получить эталонное значение для сравнения

2. **Автоматизированное тестирование:**
   - Реализованы юнит-тесты для 5 различных тестовых случаев
   - Тестируются разные интервалы и количество разбиений
   - Проверяются как последовательная, так и MPI версии

3. **Допустимая погрешность:**
   ```cpp
   double tolerance = std::max(std::abs(expected_result_) * 0.01, 1e-8);
   ```
    - Относительная погрешность: 1%
    - Абсолютная погрешность: не менее 1e-8
    - Учитывает масштаб ожидаемого результата

**Тестовые случаи:**
 - **Разные интервалы:** [0.0, 1.0], [0.0, 2.0], [-10.0, 100.0], [1.0, 4.0], [-2.0, 3.0]
 - **Разное количество разбиений:** от 1,000 до 100,000
 - **Разные тестовые функции:** 
    - `x^2`
    - `sin(x)`
    - `e^x`
    - `x * e^x`
    - `x^3 - 4x + 1`
    - `e^x * (sin(x) + cos(x)) + 3x^2 * cos(x) - x^3 * sin(x)`

**Результат:** Все тесты проходят успешно, что подтверждает корректность обеих реализаций.

### 7.2 Производительность
- **Сложная тестовая функция:** `e^x * (x^2 * sin(x) + 2x * sin(x) + x^2 * cos(x)) + 4x^3 * cos(2x) - 2x^4 * sin(2x)`
- **Количество разбиений** `100 000 000`

**Полученные результаты:**

| **Режим** | **Количество процессов** | **Время, с** | **Speedup** | **Efficiency** |
|-----------|--------------------------|--------------|-------------|----------------|
| SEQ       | 1                        | 2.158        | 1.00        | N/A            |
| MPI       | 2                        | 1.074        | 2.31        | 116%           |
| MPI       | 4                        | 0.55         | 3.92        | 98%            |
| MPI       | 6                        | 0.64         | 3.37        | 56%            |
| MPI       | 8                        | 0.40         | 5.39        | 67%            |

**Анализ:** Использование топологии гиперкуба показало высокую эффективность. Небольшое снижение эффективности при переходе к 8 процессам объясняется увеличением количества стадий обмена $(log_2 8 = 3$ шага) и накладными расходами на создание нового коммуникатора через `MPI_Comm_split`. Однако логарифмическая природа алгоритма позволяет сохранять масштабируемость при значительном росте числа узлов.

## 8. Заключение
В работе реализован алгоритм численного интегрирования с использованием кастомных процедур обмена данными в топологии гиперкуба. Реализация через `MPI_Comm_split` позволила изолировать логику гиперкуба от общего числа процессов, что сделало программу устойчивой к запускам на произвольном количестве узлов. Эксперименты подтвердили, что ручная реализация коллективных операций по топологии гиперкуба не уступает по производительности стандартным методам MPI на данных вычислительных объемах.

## 9. Источники
1. [Wikipedia](https://ru.wikipedia.org/wiki/Метод_прямоугольников)
2. [Презентация по курсу](https://learning-process.github.io/parallel_programming_slides/slides/01-intro.pdf)

