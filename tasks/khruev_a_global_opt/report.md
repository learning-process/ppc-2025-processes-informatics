
# Многошаговая схема решения двумерных задач глобальной оптимизации. Распараллеливание по характеристикам.

**Студент:** Хруев Антон  
**Группа:** 3823Б1ФИ1  
**Технологии:** SEQ | MPI  
**Вариант:** 13

---

## 1. Введение

Целью данной работы является реализация алгоритма решения систем линейных алгебраических уравнений (СЛАУ) методом Гаусса–Жордана в двух вариантах:
- последовательном (SEQ),
- параллельном с использованием технологии MPI с распределением данных.

Метод Гаусса–Жордана позволяет привести расширенную матрицу системы уравнений к приведённому ступенчатому виду (единичная матрица слева), после чего решение извлекается напрямую из последнего столбца.

---

## 2. Постановка задачи

Рассматривается система линейных уравнений вида: $A x = b$.

### Входные данные
Расширенная матрица $[A|b]$ размера $n \times (m+1)$, где $n$ — количество уравнений, $m$ — количество неизвестных. В данной реализации рассматривается случай $n = m$ для получения единственного решения.

### Выходные данные
Вектор решения $x$ размера $m$. Если система не имеет решения или имеет бесконечное множество решений, возвращается пустой вектор.

---

## 3. Последовательный алгоритм (SEQ)

Последовательная версия реализована в классе `KhruevAGlobalOptSEQ`. Алгоритм включает:
1. **Выбор ведущего элемента:** Поиск максимального по модулю элемента в текущем столбце для минимизации погрешности и исключения деления на ноль.
2. **Прямой и обратный ход:** В отличие от классического метода Гаусса, метод Гаусса-Жордана обнуляет элементы как под, так и над главной диагональю одновременно.
3. **Нормализация:** Деление ведущей строки на опорный элемент.

---

## 4. Параллельная версия (MPI)

Параллельная версия реализована в классе `KhruevAGaussJordanMPI`. В реализации применена стратегия **горизонтальной декомпозиции матрицы**.


### Основные этапы
Логика была разделена на методы:

#### 1. Распределение данных (PreProcessing)
- Процесс 0 рассылает размеры матрицы через `MPI_Bcast`.
- Матрица распределяется между процессами с помощью **`MPI_Scatterv`**. Каждый процесс получает свой набор строк в зависимости от своего ранга.

#### 2. Вычислительное ядро (RunImpl)
Алгоритм выполняет $n$ итераций. В каждой итерации:
- **FindPivot:** Процессы локально ищут максимум в текущем столбце, затем через **`MPI_Allreduce`** с флагом `MPI_MAXLOC` находят глобальный максимум и процесс-владелец лучшей строки.
- **SwapRows:** Если ведущая строка находится не на нужном месте, выполняется физический обмен данными. Если строки принадлежат разным процессам, используется **`MPI_Sendrecv`**.
- **Eliminate:** - Процесс-владелец нормализует ведущую строку.
    - Строка рассылается всем участникам через **`MPI_Bcast`**.
    - Каждый процесс параллельно обновляет только свои локальные строки.



#### 3. Сбор результатов (PostProcessing)
- Части обработанной матрицы собираются обратно на Процесс 0 через **`MPI_Gatherv`**.
- Процесс 0 формирует итоговый вектор.
- Итоговый вектор рассылается всем процессам через `MPI_Bcast`, чтобы каждый ранг успешно прошел внутреннюю проверку валидности данных `CheckTestOutputData`.


---

## 5. Детали реализации
Основные файлы:

- ops_seq.cpp — последовательная реализация;
- ops_mpi.cpp — параллельная реализация с использованием MPI;
- common.hpp — определения типов данных;

Основные классы: KhruevAGaussJordanSEQ, KhruevAGaussJordanMPI
## 6. Аппаратное и программное обеспечение
- Hardware/OS: CPU - AMD Ryzen 3 7320U with Radeon Graphics, 4 ядра / 8 потоков; RAM - 8 GB; OS - Ubuntu 24.04.2 LTS
- Toolchain gcc 13.3.0, build type: Realese
- Data: тестовые данные задаеются в ручную

## 7. Результаты



### Функциональные тесты



Проверяются:
- системы размером 2×2 и 3×3,
- перестановка строк,
- дробные коэффициенты,
- диагональные и треугольные матрицы,
- корректность полученного решения.



---

### Тесты производительности

Результаты измерения производительности для матриц размером $400$:

#### Время выполнения 

Режим	|Процессы|	Время, c	|Ускорение	|Эффективность
---|---|---|---|---|
seq	|1	|0.0149538556|1.00	|100%
mpi	|2	|0.0086119634|1.74	|87%
mpi	|3	|0.0062087236|2,41	|80%
mpi	|4	|0.0056884682|2.67	|67%

Из таблицы можно видеть, что mpi реализация дает довольно хороший прирост производительности для больших матриц.

---

## 8. Выводы

В ходе выполнения работы:
- реализован последовательный алгоритм Гаусса–Жордана,
- реализована MPI-версия алгоритма,
- код успешно проходит функциональные и performance тесты.



---


## 9. Источники

1. Сысоев А. В. Лекции по параллельному программированию. — Н. Новгород: ННГУ, 2025 
2. Документация C++