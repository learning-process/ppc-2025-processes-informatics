# Вычисление многомерных интегралов методом Монте-Карло

- Студент: Гутянский Алексей Сергеевич, группа 3823Б1ФИ3
- Технология: SEQ | MPI
- Вариант: 10

## 1. Введение

Численное вычисление многомерных интегралов является важной задачей в вычислительной математике, физике, теории вероятностей и компьютерном моделировании. Классические квадратурные методы значительно усложняются при росте размерности пространства, а для некоторых интегралов вовсе не могут быть применены.

Скорость сходимости метода Монте-Карло не зависит от размерности пространства и определяется исключительно числом случайных выборок. Независимость выборок делает метод особенно удобным для параллельной реализации, так как вычисления в отдельных точках не требуют синхронизации между процессами.

В работе рассматриваются последовательная и параллельная (MPI) реализация алгоритма многомерного интегрирования методом Монте-Карло.

## 2. Постановка задачи

Требуется реализовать алгоритм численного вычисления многомерного интеграла методом Монте-Карло.

Рассматривается интеграл следующего вида:

$$
I = \int\limits_{a_1}^{b_1} \int\limits_{a_2}^{b_2} \dots \int\limits_{a_n}^{b_n}
f(x_1, x_2, \dots, x_n)\, dx_1 dx_2 \dots dx_n
$$

где:

- $ n $ - размерность пространства;
- $ f(x_1, x_2, \dots, x_n) $ - интегрируемая вещественная функция;
- $ a_i, b_i $ - нижняя и верхняя границы интегрирования по $ i $-й координате.

**Входные данные:**

- идентификатор интегрируемой функции;
- количество измерений $ n $;
- число случайных точек $ N $;
- массивы нижних и верхних границ интегрирования размерности $ n $.

**Выходные данные:**

- приближённое значение многомерного интеграла.

Требуется реализовать как последовательную, так и параллельную версии алгоритма с использованием технологии MPI и обеспечить корректность вычислений в пределах статистической погрешности метода.

## 3. Базовый алгоритм (Последовательный)

Метод Монте-Карло основан на статистической аппроксимации интеграла посредством равномерного случайного выбора точек в области интегрирования.

Обозначим объём области интегрирования как:

$$
V = \prod_{i=1}^{n} (b_i - a_i)
$$

Тогда значение интеграла аппроксимируется формулой:

$$
I \approx V \cdot \frac{1}{N} \sum_{k=1}^{N} f(\mathbf{x}_k)
$$

где $ \mathbf{x}_k = (x_{k_1}, x_{k_2}, \dots, x_{k_n}) $ - случайные точки, равномерно распределённые в области
$ [a_1, b_1] \times [a_2, b_2] \times \dots \times [a_n, b_n] $.

Последовательный алгоритм состоит из следующих этапов:

1. Вычисление объёма области интегрирования.
2. Генерация $ N $ случайных точек в $ n $-мерном пространстве.
3. Вычисление значения функции в каждой сгенерированной точке.
4. Накопление суммы значений функции.
5. Усреднение полученной суммы и домножение результата на объём области интегрирования.

Точность метода Монте-Карло определяется числом случайных точек и возрастает пропорционально $ O(N^{-1/2}) $, не завися от размерности пространства.

```cpp
std::random_device rd;
std::mt19937 gen(rd());
std::uniform_real_distribution<double> distr(0.0, 1.0);

size_t n_points = GetInput().n_points;
size_t n_dims = GetInput().n_dims;

double volume = 1.0;

for (size_t i = 0; i < n_dims; i++) {
    volume *= GetInput().upper_bounds[i] - GetInput().lower_bounds[i];
}

FunctionRegistry::IntegralFunction function = GetInput().GetFunction();

std::vector<double> random_point(n_dims);

double sum = 0.0;

for (size_t i = 0; i < n_points; i++) {
    for (size_t j = 0; j < n_dims; j++) {
        double lb = GetInput().lower_bounds[j];
        double rb = GetInput().upper_bounds[j];

        random_point[j] = lb + (distr(gen) * (rb - lb));
    }

    sum += function(random_point);
}

GetOutput() = volume * (sum / static_cast<double>(n_points));
```

## 4. Параллелизация

### 4.1. Схема разбиения данных

Параллельная версия основана на разбиении множества случайных точек между MPI-процессами.
Каждый процесс вычисляет частичную сумму значений функции для своего поднабора точек.

Число точек на процесс вычисляется как:

- базовый размер: $\lfloor N/P \rfloor$,
- первые $N mod P$ процессов получают на одну точку больше.

Таким образом, достигается равномерная загрузка процессов.

### 4.2. Распределение данными

Все входные данные задачи (идентификатор функции, размерность, количество точек, границы интегрирования) загружаются процессом с рангом 0.

Для передачи информации между процессами используется:

- упаковка данных с помощью _MPI_Pack_;
- рассылка единого буфера через _MPI_Bcast_;
- распаковка данных на остальных процессах через _MPI_Unpack_.

Такой подход позволяет избежать множественных вызовов _MPI_Bcast_ для каждого поля структуры задачи.

### 4.3. Вычисления

Производимые в параллельной версии вычисления совпадают с таковыми в последовательной версии. Каждый процесс вычисляет соответствующее ему число значений функции в случайных точках.

### 4.4. Сбор данных

После выполнения вычислений каждый процесс имеет сумму значений функции в своем подмножестве точек. Объединение результата (суммирование) выполняется с использованием _MPI_Reduce_.

Процесс с рангом 0 получает полную сумму и масштабирует ее в соответствии с границами интегрирования и количеством точек.

## 5. Детали реализации

### 5.1 Генерация случайных чисел

Для равномерной генерации точек в многомерном пространстве используются _std::mt19937_ и _std::uniform_real_distribution_ из стандартной библиотеки C++. Каждый процесс инициализирует генератор псевдослучайных чисел независимо.

### 5.2 Функции

Так как передача функции между MPI-процессами затруднительна, для обозначения функций используются числовые идентификаторы.

В системе реализован класс _FunctionRegistry_, позволяющий по числовому идентификатору получить объект _std::function_ и ожидаемую размерность. Если размерность функции задана нулем, функция допускает использование при произвольной размерности пространства.

Реализованы следующие функции:

- Функция 0 - Константная функция
  
  \[
  f(\mathbf{x}) = 1
  \]
  
  Произвольная размерность.

- Функция 1 - Линейная функция
  
  \[
  f(\mathbf{x}) = \sum_{i=1}^{n} x_i
  \]
  
  Произвольная размерность.

- Функция 2 - Произведение координат
  
  \[
  f(\mathbf{x}) = \prod_{i=1}^{n} x_i
  \]
  
  Произвольная размерность.

- Функция 3 - Квадратичная форма
  
  \[
  f(\mathbf{x}) = \sum_{i=1}^{n} x_i^2
  \]
  
  Произвольная размерность.

- Функция 4 - Гауссова функция
  
  \[
  f(\mathbf{x}) = e^{-\sum_{i=1}^{n} x_i^2}
  \]
  
  Произвольная размерность.

- Функция 5 - Тригонометрическая функция
  
  \[
  f(x, y) = \sin(x)\cos(y)
  \]
  
  Размерность строго равна 2.

- Функция 6 - Индикатор единичного гипершара
  
  \[
  f(\mathbf{x}) =
  \begin{cases}
  1, & \sum_{i=1}^{n} x_i^2 \le 1 \\
  0, & \text{иначе}
  \end{cases}
  \]
  
  Произвольная размерность.

### 5.3 Тестирование

Для функциональных тестов используется загрузка входных данных из текстовых файлов, расположенных в директории data/. Для тестирования корректности применяются многомерные интегралы различных размерностей от различных функций.

Для измерения производительности используется задача вычисления числа $\pi$. Задача решается через вычисление двумерного интеграла от функции-индикатора единичного гипершара. функция вычисляется в 8000000 точек.

## 6. Тестовое окружение

- Аппаратное обеспечение/Операционная система: Intel Core i5 14600KF, 6P+8E ядер, 64Gb Ddr5 5600Mhz, Windows 10, MS-MPI.
- Инструменты сборки: Cmake 4.2.0-rc4, Visual Studio 2022, MSVC, x64 Release.
- Переменные окружения: PPC_NUM_THREADS=PPC_NUM_PROC=1/2/3/4/6/8/10/12/14/16/18/20/22/24, PPC_PERF_MAX_TIME=10000.
- Данные: функциональные тесты небольшого размера, тест производительности с использованием 2-мерного интеграла и 8000000 точек.
- Дополнительно: в scripts/run_tests.py были отключены perf тесты для всех технологий, кроме seq и mpi.

## 7. Результаты

### 7.1 Корректность

Последовательная и параллельная версии программы были протестированы на различных наборах данных, включающих многомерные интегралы различных размерностей от различных функций на различных областях интегрирования.
Для тестирования были выбраны функции, точный ответ для которых может быть получен аналитически.
Во всех случаях полученные значения находились в пределах ожидаемой статистической погрешности метода Монте-Карло.

### 7.2 Производительность

| Режим        | Кол-во процессов | Время, сек | Ускорение | Эффективность параллелизма |
|-------------|-------|---------|---------|------------|
| seq | 1 | 0,104 | 1,00 | 100,0% |
| mpi | 2 | 0,052 | 2,02 | 100,96% |
| mpi | 3 | 0,034 | 3,06 | 101,96% |
| mpi | 4 | 0,026 | 4,00 | 100,0% |
| mpi | 6 | 0,018 | 5,83 | 97,22% |
| mpi | 8 | 0,018 | 5,78 | 72,22% |
| mpi | 10 | 0,016 | 6,50 | 65,0% |
| mpi | 12 | 0,015 | 6,93 | 57,78% |
| mpi | 14 | 0,012 | 8,67 | 61,90% |
| mpi | 16 | 0,012 | 8,67 | 54,17% |
| mpi | 18 | 0,010 | 10,50 | 58,33% |
| mpi | 20 | 0,011 | 9,45 | 47,27% |
| mpi | 22 | 0,013 | 8,00 | 36,36% |
| mpi | 24 | 0,013 | 8,00 | 33,33% |

До 4 процессов наблюдается почти линейное масштабирование; превышение 100% эффективности при 2–3 процессах объясняется неточностью замеров.

В диапазоне 6–12 процессов ускорение продолжает расти, но эффективность заметно падает из-за накладных расходов параллелизма.

Максимальное ускорение достигается при 18 процессах, после чего наблюдается ухудшение результатов.

## 8. Заключение

В рамках работы была реализована система численного вычисления многомерных интегралов методом Монте-Карло в последовательном и параллельном (MPI) вариантах.

Реализация демонстрирует естественную масштабируемость метода и простоту параллелизации за счет независимости выборок.

Полученные результаты подтверждают применимость метода Монте-Карло для задач многомерного интегрирования и эффективность использования MPI для ускорения вычислений.

## 9. Источники

1. Microsoft MPI : документация [Электронный ресурс] // Microsoft Learn. – URL: https://learn.microsoft.com/ru-ru/message-passing-interface/microsoft-mpi (дата обращения: 18.11.2025).
2. Курс лекций "Параллельная обработка данных" : Лаборатория Параллельных Информационных Технологий, НИВЦ МГУ [Электронный ресурс] // PARALLEL.RU. - URL: https://parallel.ru/vvv/mpi.html (дата обращения: 03.11.2025)
3. Сысоев А. В. Курс лекций по параллельному программированию.
4. Monte Carlo integration [Электронный ресурс] // Department of Mathematical Sciences, Chalmers University. — URL: https://utb.math.chalmers.se/Stat/Grundutb/CTH/tms150/1415/MC_20141008.pdf (дата обращения: 01.01.2026)