# Метод простой итерации

-   Студент: Пылаева Светлана Алексеевна, группа 3823Б1ФИ3
-   Технология: SEQ, MPI
-   Вариант: 20

## 1. Введение

Итерационные методы являются фундаментальным инструментом в вычислительной математике для решения систем линейных алгебраических уравнений. В данной работе реализованы последовательная и параллельная версии метода простой итерации — одного из базовых алгоритмов для приближенного решения СЛАУ. Для распараллеливания вычислений использована технология MPI (Message Passing Interface).

## 2. Постановка задачи

Дана система линейных алгебраических уравнений (СЛАУ) вида $Ax = b$, где:

-   $A$ – квадратная матрица коэффициентов размера $n$ × $n$,
-   $x$ – вектор неизвестных размера $n$,
-   $b$ – вектор правых частей размера $n$.

Необходимо найти приближенное решение $x$ данной системы с заданной точностью ε с использованием метода простой итерации.
Матрица $A$ и векторы $b$, $x$ хранятся в памяти в виде одномерных массивов (std::vector<double>).  
Матрица хранится **по строкам**, таким образом индекс элемента _(i, j)_ можно вычислить по формуле `index = i * n + j`.

### Входные данные:

`InType = std::tuple<size_t, std::vector<double>, std::vector<double>>`

-   Размерность системы $n$ (`size_t`)
-   Матрица $A$ (`std::vector<double>`)
-   Вектор $b$ (`std::vector<int>`)

### Выходные данные:

`OutType = std::vector<double>`

-   Вектор $x$ - решение системы уравнений (`int`)

### Ограничения:

-   Определитель матрицы $A$ не равен нулю
-   Для гарантированной сходимости требуется диагональное преобладание: $|a_{ii}| > \sum_{j \neq i} |a_{ij}|$ для всех $i$
-   Максимальное число итераций: 10000
-   Точность сходимости: $ε = 10^{-5}$

## 3. Описание последовательного алгоритма

Для применения метода простой итерации необходимо преобразовать исходную систему к виду, удобному для итерационного процесса.

**Итерационная формула:**  
$x_i^{(k+1)} = (b_i - ∑_{j≠i} a_{ij}·x_j^{(k)}) / a_{ii}$

Шаги алгоритма:

1. Проверка условий применимости алгоритма:
    - Согласованность размеров
        - Матрица A должна быть квадратной размера n×n
        - Вектор b должен иметь размер n
    - Невырожденность матрицы
    - Диагональное преобладание
2. Инициализация начального приближения $x = (0, 0, ..., 0)ᵀ$
3. Основной итерационный цикл:
    - Вычисление нового приближения
    ```cpp
        for (size_t i = 0; i < n; ++i) {
            double sum = 0.0;
            for (size_t j = 0; j < n; ++j) {
                if (j != i) {
                sum += A[(i * n) + j] * x[j];
                }
            }
            x_new[i] = (b[i] - sum) / A[(i * n) + i];
        }
    ```
    - Вычисление нормы разности
    ```cpp
        double norm = 0.0;
        for (size_t i = 0; i < n; ++i) {
            double diff = x_new[i] - x[i];
            norm += diff * diff;
        }
    ```
4. Проверка критериев остановки итерационного процесса:
    - Достижение заданной точности: $||x^{k+1} - x^{k}|| < ε$
    - Превышение максимального числа итераций: k ≥ MaxIterations
5. Формирование результата:

    После завершения итерационного процесса:

    - Последнее вычисленное приближение $x^{k+1}$ принимается за решение
    - Результат сохраняется в выходной структуре данных
    - Возвращается признак успешного выполнения

## 4. Описание схемы параллельного алгоритма

Параллельная версия использует блочное распределение элементов матрицы между MPI-процессами с помощью функции `MPI_Scatterv`.

1. **Распределение данных**:

-   Корневой процесс (ранг 0) передает общий размер матрицы `matrix_size` всем процессам через `MPI_Bcast`
-   Для каждого MPI-процесса рассчитывается количество элементов `sendcounts[i]` и смещение `displs[i]` в исходном массиве
-   Распределение учитывает возможный остаток при неравномерном делении: первые `remainder` процессов получают на один элемент больше

2. **Рассылка данных**

-   Используется `MPI_Scatterv` для распределения блоков матрицы по процессам
-   Каждый процесс получает свой блок данных `local_data` размером `local_elements`
-   Нулевой процесс передает данные, остальные процессы получают свои блоки

3. **Локальная обработка**:  
   Каждый процесс независимо находит максимальный элемент в своем блоке. В результате каждый процесс формирует локальное значение максимума `local_max`

```cpp
   int local_max = std::numeric_limits<int>::min();
   for (int i = 0; i < local_elements; i++) {
      local_max = std::max(local_max, local_data[i]);
  }
```

4. **Глобальная редукция**: Все процессы участвуют в операции `MPI_Allreduce`, которая одновременно выполняет редукцию по всем процессам с использованием операции `MPI_MAX` и распространяет результат глобального максимума на все процессы

## 5. Детали реализации

**Файловая структура:**

pylaeva_s_simple_iteration_method/  
├── common/include/common.hpp  
├── data/  
├── mpi/include/ops_mpi.hpp  
├── mpi/src/ops_mpi.cpp  
├── seq/include/ops_seq.hpp  
├── seq/src/ops_seq.cpp  
├── tests/functional/main.cpp  
└── tests/performance/main.cpp

**Ключевые классы и файлы:**

1.  Последовательная реализация (`seq`):

    -   `ops_seq.hpp` - объявление класса `PylaevaSSimpleIterationMethodSEQ`
    -   `ops_seq.cpp` - реализация методов

2.  MPI реализация (`mpi`):

    -   `ops_mpi.hpp` - объявление класса `PylaevaSSimpleIterationMethodMPI`
    -   `ops_mpi.cpp` - реализация методов

3.  Вспомогательные методы:

    -   `NotNullDeterm()` — проверка определителя на ненулевое значение
    -   `DiagonalDominance()` — проверка диагонального преобладания

4.  Общие компоненты (`common`):

    -   `common.hpp` - общие типы данных и константы

5.  Тестовые данные (`data`)
    -   `Simple_1x1.txt` - тривиальный случай (одно уравнение)
    -   `Identity_matrix_2x2.txt` - единичная матрица, единичный вектор решений
    -   `Identity_matrix_3x3.txt` - единичная матрица, единичный вектор решений
    -   `Identity_matrix_5x5.txt` - единичная матрица, вектор решений $(1.0, 2.0, 3.0, 4.0, 5.0)$
    -   `Negative_1x1.txt` - тривиальный случай (одно уравнение, содержащее отрицательные элементы)
    -   `Negative_2x2.txt` - матрица содержащая отрицательные значения, единичный вектор решений
    -   `Negative_3x3.txt` - матрица содержащая отрицательные значения, единичный вектор решений
    -   `Negative_5x5.txt` - матрица содержащая отрицательные значения, единичный вектор решений
    -   `DiagDominanceRandom_2x2.txt` - матрица заполненная случайными числами, единичный вектор решений
    -   `DiagDominanceRandom_3x3.txt` - матрица заполненная случайными числами, единичный вектор решений
    -   `DiagDominanceRandom_5x5.txt` - матрица заполненная случайными числами, единичный вектор решений

## 6. Экспериментальная установка

-   CPU: Intel Core i5-10210U @ 1.60GHz (4 cores, 8 threads)
-   RAM: 16 GB
-   OS: Windows 10 version 22H2
-   Компилятор: MinGW-w64 GCC, C++20, тип сборки Release
-   MPI: Microsoft MPI 10.1.12498.52
-   CMake: 3.30.3
-   Данные:

    1. **Func_tests** данные сгенерированы в текстовых файлах.  
       Для тестирования метода простой итерации были подготовлены 11 тестовых файлов с системами линейных уравнений, обладающими диагональным преобладанием. Название файлов содержат в себе информацию о типе матрицы и размерности системы.

    Все данные представлены в формате:

    -   Целое число n — размерность системы
    -   Матрица коэффициентов A размера n×n (хранится по строкам)
    -   Вектор правых частей b длины n
    -   Вектор точного решения x\*expected длины n

    2.  **Perf_tests**
        `N`= 300 - размерность системы  
        Для проверки производительности генерируется матрица размерности NxN, единичный ожидаемый вектор решений, вектор правых частей системы вычисляется исходя из матрицы и ожидаемого вектора решений.

### Управление процессами

PPC_NUM_PROC: устанавливается через параметр -np в mpiexec

```cpp
//Запуск с различным количеством процессов MPI
mpiexec -np 1 ./build/bin/ppc_perf_tests --gtest_filter="*PylaevaSSimpleIterationMethodPerfTests*"
mpiexec -np 2 ./build/bin/ppc_perf_tests --gtest_filter="*PylaevaSSimpleIterationMethodPerfTests*"
mpiexec -np 4 ./build/bin/ppc_perf_tests --gtest_filter="*PylaevaSSimpleIterationMethodPerfTests*"
```

## 7. Результаты экспериментов и выводы

Ниже приведена таблица с результатами Perf тестов для системы, размерность которой равна 300.

_результаты были получены при запуске build/bin/ppc_perf_tests на 1, 2 и 4 процессах_

| Mode | Count | Time, s | Speedup | Efficiency |
| ---- | ----- | ------- | ------- | ---------- |
| seq  | 1     | 1.234   | 1.00    | N/A        |
| mpi  | 2     | 0.700   | 1.76    | 88.0%      |
| mpi  | 4     | 0.390   | 3.16    | 79.0%      |

**Анализ результатов:**

-   Полученные результаты показывают замедление MPI-версии относительно последовательной реализации.

**Возможные причины замедления:**

-   Время коммуникации между процессами превышает время вычислений
-   Операция сравнения имеет очень низкую вычислительную сложность
-   Накладные расходы MPI не компенсируются распараллеливанием

## 8. Заключение

В ходе работы были реализованы последовательная и параллельная MPI-версии алгоритма поиска максимального элемента в матрице.

**Основные выводы:**

1. **Эффективность параллелизации**: Для задачи поиска максимума параллельная реализация на MPI не показала ожидаемого ускорения.

2. **Возможные причины низкой производительности MPI**:
    - Высокие накладные расходы, т.к. при использовании MPI_Allreduce все процессы должны синхронизироваться
    - Маленький объем вычислений на процесс: Для операции поиска максимума слишком мало вычислений на один элемент, чтобы компенсировать затраты на коммуникацию.

## Источники

1. [Документация OpenMPI](https://www.open-mpi.org/doc/)
2. Сысоев А.В. Курс лекций по параллельному программированию
