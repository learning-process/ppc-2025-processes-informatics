# Метод простой итерации

-   Студент: Пылаева Светлана Алексеевна, группа 3823Б1ФИ3
-   Технология: SEQ, MPI
-   Вариант: 20

## 1. Введение

Итерационные методы являются фундаментальным инструментом в вычислительной математике для решения систем линейных алгебраических уравнений. В данной работе реализованы последовательная и параллельная версии метода простой итерации — одного из базовых алгоритмов для приближенного решения СЛАУ. Для распараллеливания вычислений использована технология MPI (Message Passing Interface).

## 2. Постановка задачи

Дана система линейных алгебраических уравнений (СЛАУ) вида $Ax = b$, где:

-   $A$ – квадратная матрица коэффициентов размера $n$ × $n$,
-   $x$ – вектор неизвестных размера $n$,
-   $b$ – вектор правых частей размера $n$.

Необходимо найти приближенное решение $x$ данной системы с заданной точностью ε с использованием метода простой итерации.
Матрица $A$ и векторы $b$, $x$ хранятся в памяти в виде одномерных массивов (std::vector<double>).  
Матрица хранится **по строкам**, таким образом индекс элемента _(i, j)_ можно вычислить по формуле `index = i * n + j`.

### Входные данные:

`InType = std::tuple<size_t, std::vector<double>, std::vector<double>>`

-   Размерность системы $n$ (`size_t`)
-   Матрица $A$ (`std::vector<double>`)
-   Вектор $b$ (`std::vector<int>`)

### Выходные данные:

`OutType = std::vector<double>`

-   Вектор $x$ - решение системы уравнений (`int`)

### Ограничения:

-   Для гарантированной сходимости требуется диагональное преобладание: $|a_{ii}| > \sum_{j \neq i} |a_{ij}|$ для всех $i$
-   Максимальное число итераций: 10000
-   Точность сходимости: $ε = 10^{-6}$

## 3. Описание последовательного алгоритма

Для применения метода простой итерации необходимо преобразовать исходную систему к виду, удобному для итерационного процесса.

**Итерационная формула:**  
$x_i^{(k+1)} = (b_i - ∑_{j≠i} a_{ij}·x_j^{(k)}) / a_{ii}$

Шаги алгоритма:

1. Проверка условий применимости алгоритма:
    - Согласованность размеров
        - Матрица A должна быть квадратной размера n×n
        - Вектор b должен иметь размер n
    - Диагональное преобладание
2. Инициализация начального приближения $x = (0, 0, ..., 0)ᵀ$
3. Основной итерационный цикл:
    - Вычисление нового приближения
    ```cpp
        for (size_t i = 0; i < n; ++i) {
            double sum = 0.0;
            for (size_t j = 0; j < n; ++j) {
                if (j != i) {
                sum += A[(i * n) + j] * x[j];
                }
            }
            x_new[i] = (b[i] - sum) / A[(i * n) + i];
        }
    ```
    - Вычисление нормы разности
    ```cpp
        double norm = 0.0;
        for (size_t i = 0; i < n; ++i) {
            double diff = x_new[i] - x[i];
            norm += diff * diff;
        }
    ```
4. Проверка критериев остановки итерационного процесса:
    - Достижение заданной точности: $||x^{k+1} - x^{k}|| < ε$
    - Превышение максимального числа итераций: k ≥ kMaxIterations
5. Формирование результата:

    После завершения итерационного процесса:

    - Последнее вычисленное приближение $x^{k+1}$ принимается за решение
    - Результат сохраняется в выходной структуре данных
    - Возвращается признак успешного выполнения

## 4. Описание схемы параллельного алгоритма

Параллельная реализация метода простой итерации для решения СЛАУ на основе MPI использует распределение строк матрицы между процессами. Такой подход позволяет каждому процессу независимо вычислять новые значения неизвестных для своей части системы.

## 4.1 Распределение данных

1. **Рассылка размерности системы**:  
   Нулевой процесс (ранг 0) передает размерность системы \( n \) всем процессам с помощью функции `MPI_Bcast`.

2. **Распределение строк матрицы**:  
   Матрица \( A \) распределяется по строкам между процессами. Для равномерного распределения используется алгоритм:

    - Каждому процессу назначается базовое количество строк `base = n / proc_num`.
    - Первые `rem = n % proc\_num` процессов получают дополнительную строку.
    - Формируются массивы `chunk_sizes` и `matrix_displs`, определяющие количество строк и смещение для каждого процесса.

3. **Распределение вектора правых частей**:  
   Вектор \( b \) распределяется по тем же правилам, что и строки матрицы.

4. **Рассылка данных**:

    - Для матрицы \( A \) используется `MPI_Scatterv` с учетом блочного распределения по строкам.
    - Для вектора \( b \) также применяется `MPI_Scatterv` с соответствующими счетчиками и смещениями.

    ```cpp
         MPI_Scatterv(proc_rank == 0 ? a.data() : nullptr, matrix_chunk_sizes.data(), matrix_displs.data(), MPI_DOUBLE,
                        local_a.data(), local_matrix_size, MPI_DOUBLE, 0, MPI_COMM_WORLD);

         MPI_Scatterv(proc_rank == 0 ? b.data() : nullptr, chunk_sizes.data(), displs.data(), MPI_DOUBLE,
                        local_b.data(), local_rows, MPI_DOUBLE, 0, MPI_COMM_WORLD);
    ```

## 4.2 Локальная обработка на каждом процессе

Каждый процесс получает:

-   Локальный блок матрицы `local_a` размером `local_rows * n`
-   Соответствующий блок вектора правых частей `local_b` размером `local_rows`

**В каждой итерации процесса:**

1. Для каждой локальной строки \( i \) вычисляется новое приближение:

    ```cpp
        for (int i = 0; i < count; ++i) {
            int global_i = start + i;
            double sum = 0.0;
            for (size_t j = 0; j < n; ++j) {
                 if (std::cmp_not_equal(j, global_i)) {
                 sum += local_a[(i * n) + j] * x[j];
            }
        }
        local_x_new[i] = (local_b[i] - sum) / local_a[(i * n) + global_i];
     }
    ```

2. Вычисленные локальные значения сохраняются в массиве `local_x_new`.

## 4.3 Глобальная синхронизация

1. **Сбор обновленных значений**:  
   После вычисления локальных значений все процессы обмениваются результатами через `MPI_Allgatherv`. Это позволяет каждому процессу получить полный обновленный вектор `x_new`.

2. **Вычисление нормы**:  
   Каждый процесс вычисляет локальную сумму квадратов разностей между старым и новым приближениями для своих строк:

    ```cpp
     double local_norm = 0.0;
     for (int i = 0; i < count; ++i) {
       int gi = start + i;
       double diff = x_new[gi] - x[gi];
       local_norm += diff * diff;
     }
    ```

    Затем с помощью `MPI_Allreduce` с операцией `MPI_SUM` вычисляется глобальная норма:

    ```cpp
        double global_norm = 0.0;
        MPI_Allreduce(&local_norm, &global_norm, 1, MPI_DOUBLE, MPI_SUM, MPI_COMM_WORLD);
    ```

3. **Проверка критериев остановки итерационного процесса**:

    - Достижение заданной точности: $||x^{k+1} - x^{k}|| < ε$
    - Превышение максимального числа итераций: $k ≥ kMaxIterations$

4. **Обновление приближения**:  
   Вектор x обновляется значениями x_new для следующей итерации.

## 5. Детали реализации

**Файловая структура:**

pylaeva_s_simple_iteration_method/  
├── common/include/common.hpp  
├── data/  
├── mpi/include/ops_mpi.hpp  
├── mpi/src/ops_mpi.cpp  
├── seq/include/ops_seq.hpp  
├── seq/src/ops_seq.cpp  
├── tests/functional/main.cpp  
└── tests/performance/main.cpp

**Ключевые файлы:**

1.  Последовательная реализация (`seq`):

    -   `ops_seq.hpp` - объявление класса `PylaevaSSimpleIterationMethodSEQ`
    -   `ops_seq.cpp` - реализация методов

2.  MPI реализация (`mpi`):

    -   `ops_mpi.hpp` - объявление класса `PylaevaSSimpleIterationMethodMPI`
    -   `ops_mpi.cpp` - реализация методов

3.  Общие компоненты (`common`):

    -   `common.hpp` - общие типы данных и константы

4.  Тестовые данные (`data`)
    -   `Simple_1x1.txt` - тривиальный случай (одно уравнение)
    -   `Identity_matrix_2x2.txt` - единичная матрица, единичный вектор решений
    -   `Identity_matrix_3x3.txt` - единичная матрица, единичный вектор решений
    -   `Identity_matrix_5x5.txt` - единичная матрица, вектор решений $(1.0, 2.0, 3.0, 4.0, 5.0)$
    -   `Negative_1x1.txt` - тривиальный случай (одно уравнение, содержащее отрицательные элементы)
    -   `Negative_2x2.txt` - матрица содержащая отрицательные значения, единичный вектор решений
    -   `Negative_3x3.txt` - матрица содержащая отрицательные значения, единичный вектор решений
    -   `Negative_5x5.txt` - матрица содержащая отрицательные значения, единичный вектор решений
    -   `DiagDominanceRandom_2x2.txt` - матрица заполненная случайными числами, единичный вектор решений
    -   `DiagDominanceRandom_3x3.txt` - матрица заполненная случайными числами, единичный вектор решений
    -   `DiagDominanceRandom_5x5.txt` - матрица заполненная случайными числами, единичный вектор решений

**Ключевые классы и методы:**

Ключевые классы:

-   `PylaevaSSimpleIterationMethodSEQ` - последовательная реализация.
-   `PylaevaSSimpleIterationMethodMPI` - параллельная реализация.

Вспомогательные методы SEQ реализации:

-   `DiagonalDominance()` — проверка диагонального преобладания матрицы коэффициентов.

Вспомогательные методы MPI реализации:

-   `DiagonalDominance()` — проверка диагонального преобладания матрицы коэффициентов.
-   `CalculateLocalXNew()` — вычисление новых приближений решения для строк, распределенных на текущем MPI-процессе.
-   `CalculateLocalNorm()` — вычисление локальной нормы разности между новым и текущим приближениями решения.
-   `CalculateChunkSizesAndDispls()` — вычисление размеров блоков данных (строк матрицы/вектора) и смещений для каждого MPI-процесса.
-   `CalculateMatrixChunkSizesAndDispls()` — вычисление размеров блоков матрицы коэффициентов и смещений для каждого MPI-процесса при распределении матрицы по строкам.

## 6. Экспериментальная установка

-   CPU: Intel Core i5-10210U @ 1.60GHz (4 cores, 8 threads)
-   RAM: 16 GB
-   OS: Windows 10 version 22H2
-   Компилятор: MinGW-w64 GCC, C++20, тип сборки Release
-   MPI: Microsoft MPI 10.1.12498.52
-   CMake: 3.30.3
-   Данные:

    1. **Func_tests**  
       Для тестирования метода простой итерации были подготовлены 11 тестовых файлов с системами линейных уравнений, обладающими диагональным преобладанием. Название файлов содержат в себе информацию о типе матрицы и размерности системы.

        Все данные представлены в формате:

        - Целое число n — размерность системы
        - Матрица коэффициентов A размера n×n (хранится по строкам)
        - Вектор правых частей b длины n
        - Вектор точного решения x длины n

    2. **Perf_tests**  
       `N = 300` - размерность системы  
       Для проверки производительности генерируется матрица с диагональным преобладанием размерности NxN, единичный ожидаемый вектор решений, вектор правых частей системы вычисляется исходя из матрицы и ожидаемого вектора решений.

### Управление процессами

PPC_NUM_PROC: устанавливается через параметр -np в mpiexec

```cpp
//Запуск с различным количеством процессов MPI
mpiexec -np 1 ./build/bin/ppc_perf_tests --gtest_filter="*PylaevaSSimpleIterationMethodPerfTests*"
mpiexec -np 2 ./build/bin/ppc_perf_tests --gtest_filter="*PylaevaSSimpleIterationMethodPerfTests*"
mpiexec -np 4 ./build/bin/ppc_perf_tests --gtest_filter="*PylaevaSSimpleIterationMethodPerfTests*"
```

## 7. Результаты экспериментов и выводы

Ниже приведена таблица с результатами Perf тестов для системы, размерность которой равна 300.

_результаты были получены при запуске build/bin/ppc_perf_tests на 1, 2, 4 и 8 процессах_

| Mode | Count | Time, s | Speedup | Efficiency |
| ---- | ----- | ------- | ------- | ---------- |
| seq  | 1     | 0.698   | 1.00    | N/A        |
| mpi  | 2     | 0.235   | 2.97    | 148%       |
| mpi  | 4     | 0.207   | 3.37    | 84%        |
| mpi  | 8     | 0.218   | 3.20    | 40%        |

**Анализ результатов:**

-   Параллельная MPI-реализация демонстрирует ускорение по сравнению с последовательной версией.
-   Эксперименты на задаче размерности N=300 показали, что оптимальное число процессов для данной конфигурации — 4. На этом уровне достигнуто ускорение в 3.37 раз по сравнению с последовательной версией.

## 8. Заключение

В ходе работы были реализованы и протестированы последовательная и параллельная версии метода простой итерации для решения систем линейных уравнений с использованием технологии MPI.

-   Применена декомпозиция данных по строкам матрицы, что позволило независимо вычислять блоки нового приближения на каждом процессе.

-   Для глобальной синхронизации состояния вектора решения на каждой итерации использована операция MPI_Allgatherv, обеспечивающая всех процессов полными актуальными данными.

## Источники

1. [Документация OpenMPI](https://www.open-mpi.org/doc/)
2. Сысоев А.В. Курс лекций по параллельному программированию
