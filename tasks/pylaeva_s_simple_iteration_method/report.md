# Метод простой итерации

-   Студент: Пылаева Светлана Алексеевна, группа 3823Б1ФИ3
-   Технология: SEQ, MPI
-   Вариант: 20

## 1. Введение

Итерационные методы являются фундаментальным инструментом в вычислительной математике для решения систем линейных алгебраических уравнений. В данной работе реализованы последовательная и параллельная версии метода простой итерации — одного из базовых алгоритмов для приближенного решения СЛАУ. Для распараллеливания вычислений использована технология MPI (Message Passing Interface).

## 2. Постановка задачи

Дана система линейных алгебраических уравнений (СЛАУ) вида $Ax = b$, где:

-   $A$ – квадратная матрица коэффициентов размера $n$ × $n$,
-   $x$ – вектор неизвестных размера $n$,
-   $b$ – вектор правых частей размера $n$.

Необходимо найти приближенное решение $x$ данной системы с заданной точностью ε с использованием метода простой итерации.
Матрица $A$ и векторы $b$, $x$ хранятся в памяти в виде одномерных массивов (std::vector<double>).  
Матрица хранится **по строкам**, таким образом индекс элемента _(i, j)_ можно вычислить по формуле `index = i * n + j`.

### Входные данные:

`InType = std::tuple<size_t, std::vector<double>, std::vector<double>>`

-   Размерность системы $n$ (`size_t`)
-   Матрица $A$ (`std::vector<double>`)
-   Вектор $b$ (`std::vector<int>`)

### Выходные данные:

`OutType = std::vector<double>`

-   Вектор $x$ - решение системы уравнений (`int`)

### Ограничения:

-   Определитель матрицы $A$ не равен нулю
-   Для гарантированной сходимости требуется диагональное преобладание: $|a_{ii}| > \sum_{j \neq i} |a_{ij}|$ для всех $i$
-   Максимальное число итераций: 10000
-   Точность сходимости: $ε = 10^{-5}$

## 3. Описание последовательного алгоритма

Для применения метода простой итерации необходимо преобразовать исходную систему к виду, удобному для итерационного процесса.

**Итерационная формула:**  
$x_i^{(k+1)} = (b_i - ∑_{j≠i} a_{ij}·x_j^{(k)}) / a_{ii}$

Шаги алгоритма:

1. Проверка условий применимости алгоритма:
    - Согласованность размеров
        - Матрица A должна быть квадратной размера n×n
        - Вектор b должен иметь размер n
    - Невырожденность матрицы
    - Диагональное преобладание
2. Инициализация начального приближения $x = (0, 0, ..., 0)ᵀ$
3. Основной итерационный цикл:
    - Вычисление нового приближения
    ```cpp
        for (size_t i = 0; i < n; ++i) {
            double sum = 0.0;
            for (size_t j = 0; j < n; ++j) {
                if (j != i) {
                sum += A[(i * n) + j] * x[j];
                }
            }
            x_new[i] = (b[i] - sum) / A[(i * n) + i];
        }
    ```
    - Вычисление нормы разности
    ```cpp
        double norm = 0.0;
        for (size_t i = 0; i < n; ++i) {
            double diff = x_new[i] - x[i];
            norm += diff * diff;
        }
    ```
4. Проверка критериев остановки итерационного процесса:
    - Достижение заданной точности: $||x^{k+1} - x^{k}|| < ε$
    - Превышение максимального числа итераций: k ≥ MaxIterations
5. Формирование результата:

    После завершения итерационного процесса:

    - Последнее вычисленное приближение $x^{k+1}$ принимается за решение
    - Результат сохраняется в выходной структуре данных
    - Возвращается признак успешного выполнения

## 4. Описание схемы параллельного алгоритма

Параллельная версия использует блочное распределение элементов матрицы между MPI-процессами с помощью функции `MPI_Scatterv`.

1. **Распределение данных**:

-   Корневой процесс (ранг 0) передает общий размер матрицы `matrix_size` всем процессам через `MPI_Bcast`
-   Для каждого MPI-процесса рассчитывается количество элементов `sendcounts[i]` и смещение `displs[i]` в исходном массиве
-   Распределение учитывает возможный остаток при неравномерном делении: первые `remainder` процессов получают на один элемент больше

2. **Рассылка данных**

-   Используется `MPI_Scatterv` для распределения блоков матрицы по процессам
-   Каждый процесс получает свой блок данных `local_data` размером `local_elements`
-   Нулевой процесс передает данные, остальные процессы получают свои блоки

3. **Локальная обработка**:  
   Каждый процесс независимо находит максимальный элемент в своем блоке. В результате каждый процесс формирует локальное значение максимума `local_max`

```cpp
   int local_max = std::numeric_limits<int>::min();
   for (int i = 0; i < local_elements; i++) {
      local_max = std::max(local_max, local_data[i]);
  }
```

4. **Глобальная редукция**: Все процессы участвуют в операции `MPI_Allreduce`, которая одновременно выполняет редукцию по всем процессам с использованием операции `MPI_MAX` и распространяет результат глобального максимума на все процессы

## 5. Детали реализации

**Файловая структура:**

pylaeva_s_simple_iteration_method/  
├── common/include/common.hpp  
├── data/  
├── mpi/include/ops_mpi.hpp  
├── mpi/src/ops_mpi.cpp  
├── seq/include/ops_seq.hpp  
├── seq/src/ops_seq.cpp  
├── tests/functional/main.cpp  
└── tests/performance/main.cpp

**Ключевые классы и файлы:**

1. Последовательная реализация (`seq`):

    - `ops_seq.hpp` - объявление класса `PylaevaSSimpleIterationMethodSEQ`
    - `ops_seq.cpp` - реализация методов

2. MPI реализация (`mpi`):

    - `ops_mpi.hpp` - объявление класса `PylaevaSSimpleIterationMethodMPI`
    - `ops_mpi.cpp` - реализация методов

3. Вспомогательные методы:

    - `NotNullDeterm()` — проверка определителя на ненулевое значение
    - `DiagonalDominance()` — проверка диагонального преобладания

4. Общие компоненты (`common`):

    - `common.hpp` - общие типы данных и константы

5. Тестовые данные (`data`)
    - `matrix_3x3.txt` - квадратная матрица 3x3 содержит 1 максимальный элемент (max = 100) в начале
    - `matrix_5x5.txt` - квадратная матрица 5x5 содержит 1 максимальный элемент (max = 500) в середине
    - `matrix_11x11.txt` - квадратная матрица 11x11 содержит 1 максимальный элемент (max = 21) в конце
    - `matrix_50x50.txt` - квадратная матрица 50x50 содержит несколько максимальных элементов (max = -10)
    - `matrix_100x100.txt` - квадратная матрица 100x100 содержит несколько максимальных элементов (max = 2048)
    - `matrix_150x100.txt` - прямоугольная матрица 150x100 содержит 1 максимальный элемент (max = -100) в конце
    - `matrix_200x200.txt` - квадратная матрица 200x200 содержит 1 максимальный элемент (max = 0) в конце
    - `matrix_500x1000.txt` - прямоугольная матрица 500x1000 содержит несколько максимальных элементов (max = 500)
    - `matrix_1000x1000.txt` - квадратная матрица 1000x1000 содержит несколько максимальных элементов (max = 2048)
    - `matrix_1500x1000.txt` - прямоугольная матрица 1500x1000 содержит 1 максимальный элемент (max = 1000) в начале

## 6. Экспериментальная установка

-   CPU: Intel Core i5-10210U @ 1.60GHz (4 cores, 8 threads)
-   RAM: 16 GB
-   OS: Windows 10 version 22H2
-   Компилятор: MinGW-w64 GCC, C++20, тип сборки Release
-   MPI: Microsoft MPI 10.1.12498.52
-   CMake: 3.30.3
-   Данные:
    1. **Func_tests** данные сгенерированы в текстовых файлах.
    -   Название: `matrix_NxM.txt`
    -   Формат хранения: `N` (число строк) `M` (число столбцов) `expected_max` (ожидаемое максимальное значение) далее `elem` элементы матрицы в количестве N\*M
    2. **Perf_tests** `kMatrixRows_` = 10*000, `kMatrixColumns*`= 10_000,`matrix_data[i]`= i,`max` = 99_999_999

### Управление процессами

PPC_NUM_PROC: устанавливается через параметр -np в mpiexec

```cpp
//Запуск с различным количеством процессов MPI
mpiexec -np 1 ./build/bin/ppc_perf_tests --gtest_filter="*MaxElemMatrix*"
mpiexec -np 2 ./build/bin/ppc_perf_tests --gtest_filter="*MaxElemMatrix*"
mpiexec -np 4 ./build/bin/ppc_perf_tests --gtest_filter="*MaxElemMatrix*"
```

## 7. Результаты экспериментов и выводы

Ниже приведены таблицы с результатами Perf тестов для матрицы на 100_000_000 (10_000\*10_000) элементов.

_результаты были получены при запуске build/bin/ppc_perf_tests на 1, 2 и 4 процессах_

| Mode | Count | Time, s | Speedup |
| ---- | ----- | ------- | ------- |
| seq  | 1     | 0.078   | 1.00    |
| mpi  | 2     | 0.258   | 0.30    |
| mpi  | 4     | 0.216   | 0.36    |

**Анализ результатов:**

-   Полученные результаты показывают замедление MPI-версии относительно последовательной реализации.

**Возможные причины замедления:**

-   Время коммуникации между процессами превышает время вычислений
-   Операция сравнения имеет очень низкую вычислительную сложность
-   Накладные расходы MPI не компенсируются распараллеливанием

## 8. Заключение

В ходе работы были реализованы последовательная и параллельная MPI-версии алгоритма поиска максимального элемента в матрице.

**Основные выводы:**

1. **Эффективность параллелизации**: Для задачи поиска максимума параллельная реализация на MPI не показала ожидаемого ускорения.

2. **Возможные причины низкой производительности MPI**:
    - Высокие накладные расходы, т.к. при использовании MPI_Allreduce все процессы должны синхронизироваться
    - Маленький объем вычислений на процесс: Для операции поиска максимума слишком мало вычислений на один элемент, чтобы компенсировать затраты на коммуникацию.

## Источники

1. [Документация OpenMPI](https://www.open-mpi.org/doc/)
2. Сысоев А.В. Курс лекций по параллельному программированию
