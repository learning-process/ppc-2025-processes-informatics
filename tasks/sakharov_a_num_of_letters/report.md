# Подсчет числа буквенных символов в строке

- Student: Сахаров Александр Владимирович, group 3823Б1ФИ3
- Technology: SEQ | MPI
- Variant: 22

## 1. Введение
Задача подсчета буквенных символов в строке относится к базовым операциям обработки текстовой информации. Несмотря на простоту постановки, она хорошо подходит для изучения принципов параллельного программирования: разбиения данных, организации взаимодействия процессов и измерения производительности.

В данной работе реализованы последовательная (SEQ) и параллельная (MPI) версии задачи подсчета числа буквенных символов в строке. Целью является не только корректное решение задачи, но и демонстрация ускорения за счёт использования MPI при обработке длинных строк.

## 2. Постановка задачи
**Формальная постановка:** для заданной строки `S` длиной `N` подсчитать количество символов, которые являются буквами латинского алфавита (`A–Z`, `a–z`).

**Входные данные:**
- строка `S` произвольной длины `N` (в тестах используются строки из латинских букв и дополнительных символов).

**Выходные данные:**
- целое число — количество буквенных символов в строке.

**Ограничения:**
- реализация должна работать корректно для `N ≥ 0`;
- параллельная версия должна поддерживать произвольное число MPI-процессов ≥ 1;
- валидация проверяет согласованность длины строки и переданного значения `N`.

## 3. Последовательная версия
Последовательный алгоритм последовательно просматривает строку и для каждого символа проверяет, является ли он буквой латинского алфавита. Для проверки используется стандартная функция `isalpha`, к которой символ передаётся как `unsigned char`.

В учебной постановке алгоритм можно описать следующим образом:

```cpp
int CountLetters(const std::string &s) {
  int result = 0;
  for (char c : s) {
    if (std::isalpha(static_cast<unsigned char>(c)) != 0) {
      result++;
    }
  }
  return result;
}
```

## 4. Параллельная версия

### 4.1. Схема распределения данных
Для MPI-версии используется блочная схема распределения исходной строки по процессам с равномерным распределением остатка:

- `world_size` — число процессов в коммуникаторе `MPI_COMM_WORLD`;
- `world_rank` — номер текущего процесса;
- `size_of_string = N` — длина строки (определяется на процессе 0 и рассылается остальным);
- базовый размер блока: `base_chunk = N / world_size`;
- остаток: `remainder = N % world_size`;
- процесс `r` получает `base_chunk + (r < remainder ? 1 : 0)` символов.

Для распределения используется коллективная операция `MPI_Scatterv`, что позволяет сразу разослать каждому процессу соответствующий непрерывный фрагмент строки.

### 4.2. Взаимодействие процессов

**Процесс 0 (координатор):**
- получает входные данные через `GetInput()` (кортеж `(N, std::string)`);
- инициализирует указатель на внутренний буфер строки без лишнего копирования;
- вычисляет размеры блоков и смещения (`send_counts`, `displs`);
- вызывает `MPI_Scatterv`, чтобы отправить подстроки всем процессам, включая себя;
- участвует в локальном подсчёте и в глобальном редуцировании результата.

**Процессы 1..(world_size-1) (рабочие):**
- получают длину строки `N` через `MPI_Bcast`;
- по тем же формулам восстанавливают свои `send_counts[rank]` и смещения;
- выделяют локальный буфер нужного размера;
- получают свою часть строки через `MPI_Scatterv`;
- выполняют подсчёт буквенных символов на своей части;
- участвуют в коллективной операции `MPI_Allreduce` для суммирования частичных результатов.

В качестве финального шага вызывается `MPI_Barrier` для синхронизации всех процессов перед завершением `RunImpl()`.

### 4.3. Псевдокод MPI-алгоритма

```pseudocode
MPI_Init
Get world_size, world_rank

if world_rank == 0:
    (N, S) = GetInput()
    set send_buffer = pointer to S.data()
else:
    N = 0

MPI_Bcast(N)
if N < 0: return false

base_chunk = N / world_size
remainder  = N % world_size

for r in 0..world_size-1:
    send_counts[r] = base_chunk + (r < remainder ? 1 : 0)
    displs[r]      = r * base_chunk + min(r, remainder)

local_count = send_counts[world_rank]
allocate local_string[local_count]

MPI_Scatterv(send_buffer, send_counts, displs,
             local_string, local_count)

local_letters = 0
for each c in local_string:
    if isalpha((unsigned char)c):
        local_letters++

MPI_Allreduce(local_letters → global_letters, SUM)
GetOutput() = global_letters
MPI_Barrier
return true
```

## 5. Детали реализации

### 5.1. Файловая структура проекта
`sakharov_a_num_of_letters/`:
- `common/include/common.hpp` — общие типы (`InType`, `OutType`, `TestType`, `BaseTask`);
- `seq/include/ops_seq.hpp` — объявление последовательной задачи `SakharovANumberOfLettersSEQ`;
- `seq/src/ops_seq.cpp` — реализация последовательной версии (`ValidationImpl`, `RunImpl` и др.);
- `mpi/include/ops_mpi.hpp` — объявление MPI-задачи `SakharovANumberOfLettersMPI`;
- `mpi/src/ops_mpi.cpp` — реализация MPI-версии с использованием `MPI_Scatterv` и `MPI_Allreduce`;
- `tests/functional/main.cpp` — параметрические функциональные тесты (SEQ + MPI);
- `tests/performance/main.cpp` — параметрические тесты производительности.

### 5.2. Ключевые классы и функции

- `SakharovANumberOfLettersSEQ` — последовательная реализация задачи;
- `SakharovANumberOfLettersMPI` — параллельная реализация на основе MPI;
- `ValidationImpl()` — проверяет, что длина строки, указанная во входном кортеже, совпадает с `string.size()`;
- `RunImpl()` (SEQ) — линейный проход по строке и подсчёт буквенных символов;
- `RunImpl()` (MPI) — распределение строки по процессам и параллельный подсчёт с последующим суммированием через `MPI_Allreduce`;
- `SakharovARunFuncTestsProcesses` — функциональные тесты (`BaseRunFuncTests`);
- `SakharovARunPerfTestProcesses` — тесты производительности (`BaseRunPerfTests`).

### 5.3. Использование памяти

**SEQ версия:**
- хранит целиком входную строку в памяти;
- сложность по памяти `O(N)`;
- нет дополнительных накладных расходов на коммуникации.

**MPI версия:**
- процесс 0 хранит всю строку;
- каждый процесс хранит только свой локальный фрагмент строки (`local_string`);
- используются небольшие векторы `send_counts` и `displs` длиной `world_size`;
- дополнительная память на коммуникационные буферы минимальна и не зависит от `N` линейно (кроме локального участка).

## 6. Экспериментальное окружение

**Аппаратная платформа / ОС:**
- CPU: 6 ядер / 12 потоков (ноутбук, архитектура x86-64);
- RAM: 16 ГБ;
- Накопитель: SSD;
- ОС: Windows 10 + WSL (Ubuntu) для запуска MPI-тестов.

**Инструменты разработки:**
- Компилятор: `g++` (через CMake, режим `Release`);
- MPI-реализация: OpenMPI (запуск через `mpirun`);
- Сборка: CMake + Make, профиль `Release`.

**Параметры окружения:**
- `PPC_NUM_PROC` задаётся через конфигурацию тестов (используется 4 MPI-процесса);
- тесты запускаются командами вида:
  - `mpirun -n 4 ./ppc_func_tests --gtest_filter=*SakharovARunFuncTestsProcesses*`;
  - `mpirun -n 4 ./ppc_perf_tests --gtest_filter=*SakharovARunPerfTestProcesses*`.

**Данные для производительности:**
- строка формируется в тесте производительности путём повторения шаблона `"abcdABCD"` `kRepeat_` раз;
- итоговая длина строки велика (десятки миллионов символов), что позволяет нагрузить память и показать выигрыш MPI.

## 7. Результаты и обсуждение

### 7.1. Корректность

Корректность реализованных алгоритмов проверялась с помощью:
- набора функциональных тестов для SEQ и MPI-версий с различными строками:
  - пустая строка,
  - строки, состоящие только из букв,
  - смешанные строки (буквы + цифры + знаки препинания),
  - строки без буквенных символов;
- сравнением результатов SEQ и MPI-версий на одних и тех же входных данных;
- встроенной валидацией входных данных (`ValidationImpl`).

Все функциональные тесты успешно проходят как для последовательной, так и для MPI-реализации.

### 7.2. Производительность

Тесты производительности (`SakharovARunPerfTestProcesses`) запускаются для больших строк и двух режимов:
- `pipeline` — измеряется время полной обработки (Validation + PreProcessing + Run + PostProcessing);
- `task_run` — измеряется время многократного вызова только `Run()` с фиксированными входными данными.

Пример результатов (4 MPI-процесса, крупный размер строки):

| Mode                                   | Time, s | Speedup | Efficiency |
|----------------------------------------|---------|---------|------------|
| seq, pipeline_sakharov_a_num_of_letters_seq_enabled   | 0.0817  | 1.00    | N/A        |
| mpi, pipeline_sakharov_a_num_of_letters_mpi_enabled   | 0.0478  | 1.71    | 42.8%      |
| seq, task_run_sakharov_a_num_of_letters_seq_enabled   | 0.0826  | 1.00    | N/A        |
| mpi, task_run_sakharov_a_num_of_letters_mpi_enabled   | 0.0318  | 2.60    | 65.0%      |

**Анализ результатов:**
- во всех режимах MPI-версия показывает ускорение по сравнению с последовательной;
- наиболее показательным является режим `task_run`, в котором измеряется именно производительность функции `RunImpl()` без накладных расходов других стадий конвейера;
- эффективность ниже 100% объясняется накладными расходами на создание нескольких процессов, коллективные операции (`MPI_Bcast`, `MPI_Scatterv`, `MPI_Allreduce`, `MPI_Barrier`) и конкуренцией за ресурсы на одной машине;
- основными узкими местами являются пропускная способность памяти и стоимость коммуникаций, что типично для задач с простой арифметикой и большим объёмом данных.

## 8. Заключение

В ходе работы были реализованы и исследованы последовательная и MPI-версии задачи подсчёта числа буквенных символов в строке. На основе реализации в инфраструктуре PPC были разработаны функциональные и производительные тесты, позволяющие автоматически проверять корректность и измерять время выполнения.

Благодаря эффективной схеме распределения данных (блочная декомпозиция с `MPI_Scatterv`) и использованию коллективной операции `MPI_Allreduce` удалось получить заметное ускорение MPI-версии по сравнению с последовательной как в режиме `pipeline`, так и в режиме `task_run`. При этом работа наглядно демонстрирует влияние коммуникационных накладных расходов и ограничений пропускной способности памяти на масштабируемость параллельных программ.

## 9. Источники

1. Документация по MPI (MPI-3.1 Standard) [Электронный ресурс].
2. OpenMPI: официальная документация [Электронный ресурс].
3. Материалы курса по параллельному программированию (лекции кафедры).
