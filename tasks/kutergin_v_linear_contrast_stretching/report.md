# Отчет по лабораторной работе №3
## "Повышение контраста полутонового изображения посредством линейной растяжки гистограммы"

**Студент:** Кутергин Валентин
**Группа:** 3823Б1ФИ3
**Вариант:** 30

### 1. Введение
**Мотивация:** Обработка изображений является одной из классических областей применения параллельных вычислений. Алгоритмы, применяемые к каждому пикселю, часто можно выполнять независимо, что открывает большие возможности для распараллеливания.

**Цель работы:** Реализовать параллельную версию алгоритма линейной растяжки гистограммы с использованием технологии MPI и исследовать, насколько эффективно можно ускорить данный процесс по сравнению с последовательной реализацией.

### 2. Постановка задачи
**Задано:** Полутоновое изображение, представленное как одномерный массив значений яркости пикселей (0-255).

**Требуется:** Повысить контраст изображения путем "растягивания" текущего диапазона яркостей `[min_old, max_old]` на полный доступный диапазон `[0, 255]`.

**Формула преобразования для каждого пикселя:**
`pixel_new = (pixel_old - min_old) * (255.0 / (max_old - min_old))`

### 3. Схема распараллеливания
Алгоритм был разделен на два основных параллельных этапа:

1.  **Поиск глобальных `min` и `max`:**
    *   Исходное изображение разделяется на `P` равных частей. Корневой процесс рассылает эти части всем процессам с помощью `MPI_Scatterv`.
    *   Каждый процесс находит свои **локальные** `min_local` и `max_local` на своем участке данных.
    *   С помощью двух коллективных операций **`MPI_Allreduce`** (с операциями `MPI_MIN` и `MPI_MAX`) все процессы получают итоговые **глобальные** `min_global` и `max_global` для всего изображения.

2.  **Параллельное преобразование:**
    *   Зная глобальные `min` и `max`, каждый процесс **полностью независимо** применяет формулу линейной растяжки к каждому пикселю в своем локальном участке. На этом этапе коммуникации не требуются.
    *   После завершения обработки все процессы отправляют свои измененные участки обратно на корневой процесс с помощью **`MPI_Gatherv`**.

### 4. Экспериментальная установка
*   **Hardware/OS:** AMD Ryzen 5 7500F, 6 ядер, 32GB RAM, Windows 10
*   **Toolchain & Environment:**
    *   Работа производилась внутри **Dev Container** (Ubuntu)
    *   Компилятор: **GCC 14.2.0**
    *   MPI-библиотека: **Open MPI 3.1**
    *   Тип сборки: **Release**
*   **Data:** Для замеров производительности использовалось сгенерированное изображение размером **4000x4000** пикселей (16 млн. элементов) с узким диапазоном яркостей [100, 150].

### 5. Результаты и обсуждение
#### 5.1 Корректность
Корректность обеих реализаций (`seq` и `mpi`) подтверждена набором функциональных тестов для различных сценариев (низкий контраст, полный контраст, сплошной цвет). Все тесты пройдены успешно.

#### 5.2 Производительность
Замеры производительности проводились для `seq` и `mpi` реализаций на разном количестве процессов (от 1 до 8, на машине с 6 физическими ядрами). В таблице представлено время выполнения `RunImpl` — основной вычислительной части.

| Mode | Count (P) | Time, s | Speedup (S) | Efficiency (E) |
| :--- | :--- | :--- | :--- | :--- |
| seq | 1 | **0.06573** | 1.00 | N/A |
| mpi | 2 | **0.03939** | 1.67 | 83.5% |
| mpi | 3 | **0.02649** | 2.48 | 82.7% |
| mpi | 4 | **0.02234** | 2.94 | 73.5% |
| mpi | 5 | **0.02554** | 2.57 | 51.4% |
| mpi | 6 | **0.01774** | **3.70** | **61.7%** |
| mpi | 7 | **0.02005** | 3.28 | 46.9% |
| mpi | 8 | **0.01447** | **4.54** | **56.8%** |

*   **Speedup** = `Time(seq) / Time(mpi, P)`
*   **Efficiency** = `Speedup / P * 100%`

**Обсуждение:**
Полученные результаты наглядно демонстрируют эффективность MPI-реализации для данной задачи при работе с **большим объемом данных**.

*   **Масштабируемость и ускорение:** Анализ показывает **положительную масштабируемость**: время выполнения стабильно уменьшается с ростом числа процессов. Максимальное ускорение достигает **4.54x на 8 процессах**. Этот результат особенно интересен, так как он получен в режиме "oversubscription" (когда количество процессов превышает количество физических ядер, равное 6). Такой эффект может быть объяснен эффективной работой планировщика операционной системы, который может перекрывать время ожидания коммуникаций одних процессов выполнением вычислений на других.

*   **Эффективность:** Метрика эффективности демонстрирует классическую для параллельных вычислений тенденцию. Наивысшие значения (около 83%) достигаются при малом числе процессов (2-3). С дальнейшим увеличением числа процессов эффективность снижается. Это указывает на то, что, хотя общее время выполнения сокращается, доля накладных расходов на коммуникацию и синхронизацию в работе каждого отдельного процесса постепенно возрастает.

Ключевым фактором успеха в данном эксперименте является **высокая вычислительная интенсивность**, достигнутая за счет большого размера входных данных (16 млн. пикселей). Именно это позволило полезным вычислениям доминировать над затратами на MPI-коммуникации.

### 6. Заключение
В ходе работы был успешно реализован и протестирован параллельный алгоритм линейной растяжки гистограммы с использованием MPI.

Эксперименты по производительности, проведенные на большом наборе данных, продемонстрировали **значительное ускорение (до 4.54x) и высокую эффективность** параллельной реализации. Это подтверждает, что при достаточной вычислительной нагрузке, правильно выбранная схема декомпозиции (`Scatterv`/`Gatherv`) и коллективных операций (`Allreduce`) в MPI является мощным инструментом для ускорения задач обработки изображений.

### 7. Приложение: Исходный код MPI-реализации (`RunImpl`)
```cpp
bool LinearContrastStretchingMPI::RunImpl() {
  int process_rank = 0;
  int process_count = 1;
  MPI_Comm_rank(MPI_COMM_WORLD, &process_rank);
  MPI_Comm_size(MPI_COMM_WORLD, &process_count);

  const int root = 0;
  size_t total_pixel_count = (process_rank == root) ? GetInput().data.size() : 0;

  const int base_chunk =
      static_cast<int>(total_pixel_count) / process_count;  // целое часть от деления числа пикселей на число процессов
  const int remainder =
      static_cast<int>(total_pixel_count) % process_count;  // остаток от деления числа пикселей на число процессов

  std::vector<int> send_counts(process_count);  // вектор чисел пикселей на каждый процесс
  std::vector<int> displs(process_count, 0);    // вектор смещений начал "кусков" на каждый процесс

  if (process_rank == root)  // root-процесс вычисляет кому сколько пикселей дать и какое будет смещение
  {
    for (int i = 0; i < process_count; ++i) {
      send_counts[i] = base_chunk + (i < remainder ? 1 : 0);
      if (i > 0) {
        displs[i] = displs[i - 1] + send_counts[i - 1];
      }
    }
  }

  int local_chunk_size = 0;
  MPI_Scatter(send_counts.data(), 1, MPI_INT, &local_chunk_size, 1, MPI_INT, root, MPI_COMM_WORLD);

  std::vector<unsigned char> local_chunk(
      local_chunk_size);  // локальный буфер для приема части вектора на каждом процессе

  const auto &image_in_full = GetInput().data;
  auto &image_out_full = GetOutput();

  MPI_Scatterv(image_in_full.data(), send_counts.data(), displs.data(), MPI_UNSIGNED_CHAR, local_chunk.data(),
               local_chunk_size, MPI_UNSIGNED_CHAR, root, MPI_COMM_WORLD);

  unsigned char local_min = 255;
  unsigned char local_max = 0;
  if (!local_chunk.empty()) {
    // поиск локальных минимумов и максимумов (на каждом куске вектора)
    const auto &minmax_it = std::ranges::minmax_element(local_chunk);
    local_min = *minmax_it.min;
    local_max = *minmax_it.max;
  }

  // получение глобального минимума и максимума (для всего вектора)
  unsigned char global_min = 0;
  unsigned char global_max = 0;
  MPI_Allreduce(&local_min, &global_min, 1, MPI_UNSIGNED_CHAR, MPI_MIN,
                MPI_COMM_WORLD);  // сбор минимума из всех local_min
  MPI_Allreduce(&local_max, &global_max, 1, MPI_UNSIGNED_CHAR, MPI_MAX,
                MPI_COMM_WORLD);  // сбор максимума из всех local_max

  if (global_min != global_max) {
    const double scale = 255.0 / (global_max - global_min);
    for (unsigned char &pixel : local_chunk) {
      double p_out = (pixel - global_min) * scale;
      pixel = static_cast<unsigned char>(std::round(p_out));
    }
  }

  MPI_Gatherv(local_chunk.data(), local_chunk_size, MPI_UNSIGNED_CHAR, image_out_full.data(), send_counts.data(),
              displs.data(), MPI_UNSIGNED_CHAR, root, MPI_COMM_WORLD);

  return true;
}