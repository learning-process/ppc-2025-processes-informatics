# Умножение разреженных матриц. Элементы типа double. Формат хранения матрицы – строковый (CRS).

- Студент: Николаев Денис Андреевич, группа 3823Б1ФИ1
- Технология: SEQ | MPI 
- Вариант: 4

## 1. Введение
Разработка последовательного (SEQ) и параллельного (MPI) алгоритмов умножения разреженных матриц в формате CRS (Compressed Row Storage). Цель — ускорить работу с большими матрицами за счет распараллеливания вычислений и минимизации накладных расходов на передачу данных.

## 2. Постановка задачи
Для заданных двух разреженных матриц A (M×K) и B (K×N) в формате CRS вычислить произведение C = A × B, затем найти сумму всех ненулевых элементов результирующей матрицы C.

Вход: два текстовых файла с разреженными матрицами в формате CRS (целые размеры, затем построчно: количество ненулевых элементов, индексы столбцов, значения)

Выход: сумма всех ненулевых элементов результирующей матрицы

## 3. Базовый алгоритм (последовательный)

```cpp
for (int i = 0; i < A.rows; ++i) {
    std::fill(row_values.begin(), row_values.end(), 0.0);
    
    int row_start_a = A.row_pointers[i];
    int row_end_A = A.row_pointers[i + 1];
    
    for (int k = row_start_a; k < row_end_A; ++k) {
        int col_A = A.col_indices[k];
        double val_A = A.values[k];
        
        int row_start_B = B.row_pointers[col_A];
        int row_end_B = B.row_pointers[col_A + 1];
        
        for (int l = row_start_B; l < row_end_B; ++l) {
            int col_B = B.col_indices[l];
            double val_B = B.values[l];
            row_values[col_B] += val_A * val_B;
        }
    }
    
    // Сохранение ненулевых элементов строки i в матрицу C
}
```


## 4. Схема параллелизации

### Распределение данных: 

Блочное распределение строк матрицы A между процессами. Каждый процесс получает свой набор строк матрицы A для умножения на всю матрицу B.

### Коммуникация:

Процесс 0 читает обе матрицы из файлов
Матрица A распределяется по строкам между всеми процессами
Матрица B широковещательно рассылается всем процессам (так как нужна полностью каждому процессу)
Каждый процесс вычисляет произведение своих строк A на матрицу B
Результаты (вычисленные строки матрицы C) собираются на процесс 0

### Роли рангов:

Rank 0: мастер-процесс (чтение, распределение, сбор результатов)
Rank 1..N-1: рабочие процессы (обработка своих блоков строк)

## 5. Особенности реализации
### Структура кода:
Используем команду "tree tasks/nikolaev_d_sparse_matrix_mult_crs_double/" для того чтобы узнать структуру проекта:

tasks/nikolaev_d_sparse_matrix_mult_crs_double/
```
├── common
│   └── include
│       └── common.hpp
├── data
│   ├── A_1.txt
│   ├── A_2.txt
│   ├── A_27.txt
│   ├── A_28.txt
│   ├── A_29.txt
│   ├── A_3.txt
│   ├── A_30.txt
│   ├── A_31.txt
│   ├── A_32.txt
│   ├── A_33.txt
│   ├── A_34.txt
│   ├── A_35.txt
│   ├── A_36.txt
│   ├── A_4.txt
│   ├── A_5.txt
│   ├── B_1.txt
│   ├── B_2.txt
│   ├── B_27.txt
│   ├── B_28.txt
│   ├── B_29.txt
│   ├── B_3.txt
│   ├── B_30.txt
│   ├── B_31.txt
│   ├── B_32.txt
│   ├── B_33.txt
│   ├── B_34.txt
│   ├── B_35.txt
│   ├── B_36.txt
│   ├── B_4.txt
│   └── B_5.txt
├── generate_sparse_matrices.py
├── info.json
├── mpi
│   ├── include
│   │   └── ops_mpi.hpp
│   └── src
│       └── ops_mpi.cpp
├── report.md
├── seq
│   ├── include
│   │   └── ops_seq.hpp
│   └── src
│       └── ops_seq.cpp
├── settings.json
└── tests
    ├── functional
    │   └── main.cpp
    └── performance
        └── main.cpp
```

13 директорий, 41 файл

### Ключевые методы (вспомогательные для RunImpl(), с целью декомпозиции функции и избежания ее перегрузки):
- BroadcastMatrixInfo() - передача размеров матриц всем процессам
- DistributeMatrixA() - распределение строк матрицы A по процессам
- DistributeMatrixB() - широковещательная рассылка матрицы B
- MultiplyLocalRows() - умножение локальных строк на матрицу B
- GatherResults() - сбор результатов на процесс 0

### Особенности:
- Использование формата CRS для эффективного хранения разреженных матриц
- Минимизация коммуникаций: матрица B отправляется один раз всем процессам
- Обработка неравномерного распределения строк при делении M на N процессов
- Эффективный алгоритм умножения с учетом разреженности
- Проверка совместимости размеров матриц (cols_A == rows_B)

## 6. Экспериментальная установка
- CPU: AMD Ryzen 5 2600 (6 cores, 12 threads, 3.4 GHz)
- RAM: 16 GB
- OS: Linux, CachyOS 6.18.2-2-cachyos
- Toolchain: GCC 15.2.1, Release build
- Data: разреженные матрицы размером от 100×100 до 5000×5000 с плотностью ~1-5% 

## 7. Результаты и обсуждение

### 7.1 Корректность
- Проверка против последовательной реализации (SEQ vs MPI)
- Unit-тесты с эталонными значениями для краевых случаев
- Тестирование на матрицах разного размера и плотности
- Валидация результатов умножения (C = A × B)
- Проверка обработки пустых строк и полностью нулевых матриц

### 7.2 Производительность
Результаты в столбце "Время" представлены как среднее между запусками performance тестов

Ускорение = T_seq / T_parallel \
Эффективность = Ускорение / Количество процессов * 100%

#### Измерения "чистого" времени вычислений максимальных элементов по строкам матрицы - task_run

| Режим       | Процессов |    Время, с    | Ускорение | Эффективность |
|-------------|---------: |--------------: |---------: |-------------: |
| seq         | 1         | 0.0413610458   | 1.00      | N/A           |
| mpi         | 2         | 0.0304679200   | 1.36      | 68.0%         |
| mpi         | 4         | 0.0229556508   | 1.80      | 45.0%         |
| mpi         | 6         | 0.0245468440   | 1.68      | 28.0%         |
| mpi         | 8         | 0.0287364084   | 1.44      | 18.0%         |

#### Измерения полного времени вычислений ("чистое" + затраты на открытие файла, считывание и коммуникацию процессов) - pipeline

| Режим       | Процессов |    Время, с    | Ускорение | Эффективность |
|-------------|---------: |--------------: |---------: |-------------: |
| seq         | 1         | 0.0799592018   | 1.00      | N/A           |
| mpi         | 2         | 0.0887974966   | 0.90      | 45.0%         |
| mpi         | 4         | 0.0849226260   | 0.94      | 23.5%         |
| mpi         | 6         | 0.0896824092   | 0.89      | 14.8%         |
| mpi         | 8         | 0.0956344734   | 0.83      | 10.4%         |

Анализ результатов:

Вычислительная эффективность (task_run):

- На 2 процессах: Ускорение = 1.36, Эффективность = 68.0%
- На 4 процессах: Ускорение = 1.80, Эффективность = 45.0%
- На 6 процессах: Ускорение = 1.68, Эффективность = 28.0%
- На 8 процессах: Ускорение = 1.44, Эффективность = 18.0%

Эффективность полного процесса (pipeline):

- На 2 процессах: Замедление (Ускорение = 0.90), Эффективность = 45.0%
- На 4 процессах: Замедление (Ускорение = 0.94), Эффективность = 23.5%
- На 6 процессах: Замедление (Ускорение = 0.89), Эффективность = 14.8%
- На 8 процессах: Замедление (Ускорение = 0.83), Эффективность = 10.4%

Анализ:

- Сверхлинейное ускорение отсутствует даже для "чистых" вычислений
- Эффективность падает значительно быстрее с ростом числа процессов
- Оптимальное число процессов: 2 (максимальная эффективность 68.0% для вычислений)
- Сверх 8 процессов нецелесообразно - эффективность ниже 25%

Ограничения масштабируемости:

- Недостаточный объем вычислений: Для матриц размером, дающим время выполнения ~0.04 c, накладные расходы MPI превосходят вычислительную работу
- Доминирование коммуникаций: Время передачи данных превышает время вычислений уже при 8 процессах
- Синхронизационные издержки: Барьеры и коллективные операции съедают преимущество параллелизма

"Бутылочные горлышки" (уточненные):

- Стартовые затраты MPI: Инициализация MPI среды для коротких задач
- Микросекундные вычисления: Алгоритм слишком быстрый для параллелизации на MPI
- Сбор мелких фрагментов: Сбор результатов с многих процессов для малого объема данных

Порог эффективности:
- Алгоритм эффективен для матриц размером от 1000×1000 элементов. Для меньших матриц последовательная версия может оказаться быстрее из-за накладных расходов параллелизации.

## 8. Выводы
- MPI реализация обеспечивает значительное ускорение
- Наилучшая эффективность достигается при умеренном числе процессов (от 2 до 6 процессов, так как дальше идут неоправданные затраты времени на коммуникацию рабочих процессов с мастер процессом)
- Алгоритм хорошо масштабируется для больших матриц
- Основное ограничение - коммуникационные затраты

## 9. Список литературы 
1. MPI Forum. **MPI: A Message-Passing Interface Standard. Version 3.1** [Электронный ресурс]. — Режим доступа: https://www.mpi-forum.org/docs/
2. Лекции по параллельному программированию
3. Практические занятия по параллельному программированию