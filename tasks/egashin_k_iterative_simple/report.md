# Метод простой итерации

* **Студент:** Егашин Кирилл Олегович, группа 3823Б1ФИ2
* **Технологии:** SEQ | MPI
* **Вариант:** 20

## 1. Введение

**Цель:** Решить систему линейных алгебраических уравнений (СЛАУ) методом простой итерации и проверить, даст ли использование MPI ускорение.

**Проблема:** Метод простой итерации требует на каждой итерации умножения матрицы на вектор и синхронизации результатов между процессами. При параллелизации добавляются накладные расходы на обмен данными (MPI_Allgatherv на каждой итерации).

**Ожидание:** При достаточно большом размере матрицы параллельная версия должна показать ускорение, так как вычислительная нагрузка (умножение матрицы на вектор) значительно выше, чем в задаче сравнения строк.

## 2. Постановка задачи

**Дано:** Система уравнений `Ax = b`, где:
* `A` - квадратная матрица коэффициентов (n x n)
* `b` - вектор правой части
* `x0` - начальное приближение
* `tolerance` - требуемая точность
* `max_iterations` - максимальное число итераций

**Надо найти:** Вектор `x`, являющийся решением системы.

**Метод:** Простая итерация (метод Ричардсона):
```
x^(k+1) = x^k + tau * (b - Ax^k)
```
где tau - итерационный параметр.

**Вход:** Матрица A, вектор b, начальное приближение x0, точность, макс. итераций.
**Выход:** Вектор x - приближённое решение.

## 3. Описание алгоритма (SEQ)

1. Вычисляем параметр tau = 1 / (||A|| + 1), где ||A|| - максимальная сумма элементов строки.
2. Инициализируем x = x0.
3. На каждой итерации:
   * Вычисляем произведение Ax.
   * Вычисляем невязку r = b - Ax.
   * Обновляем решение: x_new = x + tau * r.
   * Проверяем сходимость: ||x_new - x|| < tolerance.
4. Возвращаем найденное решение.

Сложность одной итерации: `O(N^2)` для матрицы NxN.

## 4. Схема распараллеливания (MPI)

Строки матрицы распределяются между процессами.

1. **Главный процесс (Rank 0)** рассылает размерность задачи всем процессам (`MPI_Bcast`).
2. **Распределение:** Каждый процесс получает n/size строк матрицы (с учётом остатка).
3. **Рассылка данных:** Матрица A, вектор b и начальное приближение x0 рассылаются всем процессам.
4. **Вычисление tau:** Каждый процесс считает максимальную сумму своих строк, затем `MPI_Allreduce` находит глобальный максимум.
5. **На каждой итерации:**
   * Каждый процесс вычисляет свою часть вектора x_new.
   * Результаты собираются через `MPI_Allgatherv`.
   * Проверка сходимости на процессе 0, результат рассылается всем.
6. **Итог:** Финальное решение сохраняется на процессе 0.

## 5. Экспериментальное окружение

### ПК

* **ОС:** Microsoft Windows 11 Pro, 64-разрядная
* **Процессор:** Intel Core i5-9600K @ 3.70GHz (6 ядер, 6 потоков)
* **Оперативная память:** 32 ГБ

### Docker-контейнер

* **Базовый образ:** Ubuntu 24.04
* **Компилятор:** GCC 14 (g++-14)
* **MPI:** OpenMPI + MPICH
* **Система сборки:** CMake, Ninja
* **Дополнительно:** LLVM/Clang 21, OpenMP, Valgrind, ccache

### Тестовые данные

* Трёхдиагональная матрица размером **5000 x 5000**
* Диагональное преобладание: A[i][i] = 4, A[i][i-1] = A[i][i+1] = -1
* Правая часть подобрана так, чтобы точное решение было x = (1, 1, ..., 1)
* Точность: 1e-6, максимум итераций: 10000

## 6. Результаты

### 6.1 Корректность

5 тестов на матрицах разного размера (2x2, 3x3) с различными конфигурациями:
* Диагонально преобладающие матрицы
* Разные начальные приближения

### 6.2 Производительность

Время работы на матрице 5000x5000:

| Режим | Кол-во процессов | Время (сек) | Ускорение | Эффективность |
|:-----:|:----------------:|:-----------:|:---------:|:-------------:|
| **seq** | **1** | **3.49** | **1.00** | **100%** |
| mpi | 2 | 1.99 | 1.75 | 87.5% |
| mpi | 4 | 2.51 | 1.39 | 34.75% |

**Вывод по таблице:**
Параллельная версия показывает ускорение при 2 процессах (Speedup = 1.75). При 4 процессах эффективность падает из-за накладных расходов на коммуникации и ограниченного числа физических ядер в тестовой среде.

## 7. Заключение

Эксперименты показали, что в отличие от задачи сравнения строк, здесь параллелизация даёт реальное ускорение (1.75x на 2 процессах). Это объясняется тем, что вычислительная нагрузка (умножение матрицы на вектор - O(N^2)) значительно превышает накладные расходы на коммуникации.

При увеличении числа процессов до 4 эффективность снижается из-за:
* Накладных расходов на `MPI_Allgatherv` на каждой итерации
* Ограниченного числа физических ядер в Docker-контейнере

Для лучшего масштабирования нужно увеличить размер задачи или оптимизировать схему коммуникаций.

## 8. Источники

1. MPI Standard Documentation - https://www.mpi-forum.org/docs/
2. Лекции и практики по параллельному программированию

## Приложение

**Формула итерации:**

```cpp
for (size_t i = 0; i < n; ++i) {
  double Ax_i = 0.0;
  for (size_t j = 0; j < n; ++j) {
    Ax_i += A[i][j] * x[j];
  }
  x_new[i] = x[i] + tau * (b[i] - Ax_i);
}
```
