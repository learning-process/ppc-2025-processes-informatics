# Сумма значений по столбцам матрицы

- Студент: Гутянский Алексей Сергеевич, группа 3823Б1ФИ3
- Технология: SEQ | MPI
- Вариант: 12

## 1. Введение

Суммирование элементов матрицы по столбцам относится к числу базовых операций, которые часто становятся частью более сложных вычислительных цепочек: статистической обработки данных, численных методов, разложения больших наборов измерений. Несмотря на простоту формулировки, практическая реализация такой операции для крупных матриц требует продуманной организации данных и эффективного распределения нагрузки между вычислительными узлами.

Работа посвящена разработке решения для суммирования матрицы по столбцам, а также анализу возможностей его параллельного исполнения. Рассматриваются ключевые аспекты реализации, потенциальные источники узких мест и факторы, влияющие на производительность при различных способах разбиения данных.

## 2. Постановка задачи

Дана прямоугольная матрица \(A\) размера \(n \times m\), содержащая целые числа. Для каждого столбца требуется вычислить сумму элементов в нём.

**Входные данные:** два целых числа \(n\) и \(m\), задающих соответственно кол-во строк и столбцов матрицы, \(n \times m\) чисел задающих элементы матрицы. Гарантируется, что элементы матрицы и ответ помещаются в 32-битный знаковый тип.

**Выходные данные:** \(m\) целых чисел - сумм по каждому из столбцов матрицы.

## 3. Базовый алгоритм (Последовательный)

Будем рассматривать решение как массив длины \(m\). Изначально все его элементы инициализированы нулями.

```cpp
std::fill(GetOutput().begin(), GetOutput().end(), 0);

for (size_t i = 0; i < GetInput().rows; i++) {
    for (size_t j = 0; j < GetInput().cols; j++) {
        GetOutput()[j] += GetInput().data[(i * GetInput().cols) + j];
    }
}
```

Для каждой строки матрицы от \(1\) до \(n\) столбцы обходятся в порядке от \(1\) до \(m\). Ответ накапливается в соответствующих ячейках массива.

При описанной схеме обхода достигается почти последовательный доступ к памяти. При этом внутренний цикл по \(j\) может быть легко векторизован.

## 4. Параллелизация

Отдадим каждому процессу на обработку одну или более последовательных строк матрицы. Количество строк вычисляется по формуле:

```cpp
size_t chunk_size = row_count / static_cast<size_t>(p_count);
size_t remainder_size = row_count % static_cast<size_t>(p_count);

size_t rows_count = chunk_size + (std::cmp_less(rank, remainder_size) ? 1 : 0);
size_t elements_count = col_count * rows_count;
```

В формуле _p_count_ обозначает число запущенных процессов, а _rank_ - ранг процесса.

Если \(n\) не делится на _proc_count_ нацело, то остаток распределяется между процессами. Разница в количестве обрабатываемых элементов не превышает количество столбцов матрицы.

Полную копию данных имеет только процесс с нулевым рангом. Из него в другие процессы через _MPI_Bcast_ рассылаются сначала размеры матрицы, а потом через _MPI_Scatterv_ блоки для обработки.

Для вычисления частичного ответа используется код аналогичный последовательной версии за тем исключением, что суммирование выполняется не по всей матрице, а по ее части.

Полное решение задачи вычисляется с использованием функции _MPI_Reduce_. Ответ становится доступен в памяти процесса с нулевым рангом.

## 5. Детали реализации

Для функциональных тестов реализована загрузка из файлов, хранящихся в директории data/. Матрица большого размера, сохраненная в текстовом файле, может занимать большой объем места на диске, но для функциональных тестов возможность использования больших матриц является избыточной.

Для теста производительности используется квадратная матрица размера 16000 на 16000 элементов. Матрица состоит из единиц, что никак не влияет на скорость исполнения, но упрощает генерацию теста.

Как уже отмечалось ранее, для параллельной версии загрузка тестовых данных и проверка ответа происходит только на процессе с нулевым рангом.

## 6. Тестовое окружение

- Аппаратное обеспечение/Операционная система: Intel Core i5 14600KF, 6P+8E ядер, 64Gb Ddr5 5600Mhz, Windows 10, MS-MPI.
- Инструменты сборки: Cmake 4.2.0-rc4, Visual Studio 2022, MSVC, x64 Release.
- Переменные окружения: PPC_NUM_THREADS=PPC_NUM_PROC=1/2/4/6/8/10/12/14/16/20/24/32/40/60/80, PPC_PERF_MAX_TIME=10000.
- Данные: вручную созданные тесты небольшого размера, случайным образом сгенерированный тест 1000x1000 элементов, матрица из единиц размера 16000x16000.
- Дополнительно: в scripts/run_tests.py были отключены perf тесты для всех технологий, кроме seq и mpi.

## 7. Результаты

### 7.1 Корректность

Решение, как в последовательной, так и в параллельной версии было протестировано на входных данных разного размера и содержания. В числе тестов: матрица с одним столбцом и одной строкой, матрица, состоящая из нулей, неквадратная матрица и другие.

Скорость исполнения проверена на матрице из единиц размера 16000х16000 элементов. Время исполнения стабильно и равно 110мс.

### 7.2 Производительность

Время, ускорение, эффективность:

| Режим        | Кол-во процессов | Время, сек | Ускорение | Эффективность параллелизма |
|-------------|-------|---------|---------|------------|
| seq         | 1     | 0.106  | 1.00    | N/A        |
| mpi         | 2     | 0.230  | 0.48   | 24.7%      |
| mpi         | 4     | 0.175  | 0.61   | 15.2%      |
| mpi         | 6     | 0.162  | 0.65   | 10.9%      |
| mpi         | 8     | 0.154  | 0.70   | 8.7%       |
| mpi         | 10    | 0.144  | 0.75    | 7.4%       |
| mpi         | 12    | 0.139  | 0.79    | 6.6%       |
| mpi         | 14    | 0.141  | 0.77    | 5.5%       |
| mpi         | 16    | 0.142  | 0.77    | 4.8%       |
| mpi         | 20    | 0.139  | 0.77    | 3.8%       |
| mpi         | 24    | 0.138  | 0.81    | 3.3%       |
| mpi         | 32    | 0.139  | 0.77    | 2.3%       |
| mpi         | 40    | 0.135  | 0.81    | 2.0%       |
| mpi         | 60    | 0.139  | 0.82    | 1.3%       |
| mpi         | 80    | 0.147  | 0.73    | 0.9%       |

![Ускорение от количества процессов](./report_assets/pic1.png)
![Эффективность параллелизма](./report_assets/pic2.png)

Первое, что стоит отметить, это замедление MPI версии относительно последовательной. Данных для обработки много, в то время как операций над ними мало. Большая часть времени тратится на пересылку данных между процессами.

При отсутствии рассылки входных данных удавалось достичь ускорения относительно последовательной версии в 5.2 раза, что не являлось пределом, так как то решение не было оптимизировано и хранило полную копию входных данных на каждом из процессов.

Видно, что производительность растет до достижения 12 процессов, что равно количеству производительных ядер процессора с учетом Hyper-Threading. Ускорение в максимуме достигло 2.54 раз относительно запуска с одним MPI процессом.

Далее до 20 процессов наблюдается плато. После 20 процессов размер работы, проделанной каждым процессом продолжает уменьшаться, но накладные расходы на пересылки растут медленнее, что позволяет выжать еще немного ускорения.

## 8. Заключение

Была разработана программа, решающая задачу суммирования матрицы по столбцам и проанализированы возможности её параллелизации с использованием MPI. Были найдены и описаны узкие места и возможные причины их возникновения.

## 9. Источники

1. Microsoft MPI : документация [Электронный ресурс] // Microsoft Learn. – URL: https://learn.microsoft.com/ru-ru/message-passing-interface/microsoft-mpi (дата обращения: 18.11.2025).
2. Курс лекций "Параллельная обработка данных" : Лаборатория Параллельных Информационных Технологий, НИВЦ МГУ [Электронный ресурс] // PARALLEL.RU. - URL: https://parallel.ru/vvv/mpi.html (дата обращения: 03.11.2025)
3. Сысоев А. В. Курс лекций по параллельному программированию.